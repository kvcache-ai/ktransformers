<!DOCTYPE HTML>
<html lang="zh-CN" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Ktransformers</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon-de23e50b.svg">
        <link rel="shortcut icon" href="favicon-8114d1fc.png">
        <link rel="stylesheet" href="css/variables-8adf115d.css">
        <link rel="stylesheet" href="css/general-2459343d.css">
        <link rel="stylesheet" href="css/chrome-ae938929.css">
        <link rel="stylesheet" href="css/print-9e4910d8.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="fonts/fonts-9644e21d.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="mdbook-highlight-css" href="highlight-493f70e1.css">
        <link rel="stylesheet" id="mdbook-tomorrow-night-css" href="tomorrow-night-4c0ae647.css">
        <link rel="stylesheet" id="mdbook-ayu-highlight-css" href="ayu-highlight-3fdfc3ac.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex-947113ac.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc-8ca7a795.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>‚Üê</kbd> or <kbd>‚Üí</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="mdbook-body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="mdbook-sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("mdbook-sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="mdbook-sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="mdbook-sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="mdbook-page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="mdbook-menu-bar-hover-placeholder"></div>
                <div id="mdbook-menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="mdbook-sidebar-toggle" class="icon-button" for="mdbook-sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="mdbook-sidebar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M0 96C0 78.3 14.3 64 32 64H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32C14.3 128 0 113.7 0 96zM0 256c0-17.7 14.3-32 32-32H416c17.7 0 32 14.3 32 32s-14.3 32-32 32H32c-17.7 0-32-14.3-32-32zM448 416c0 17.7-14.3 32-32 32H32c-17.7 0-32-14.3-32-32s14.3-32 32-32H416c17.7 0 32 14.3 32 32z"/></svg></span>
                        </label>
                        <button id="mdbook-theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="mdbook-theme-list">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M371.3 367.1c27.3-3.9 51.9-19.4 67.2-42.9L600.2 74.1c12.6-19.5 9.4-45.3-7.6-61.2S549.7-4.4 531.1 9.6L294.4 187.2c-24 18-38.2 46.1-38.4 76.1L371.3 367.1zm-19.6 25.4l-116-104.4C175.9 290.3 128 339.6 128 400c0 3.9 .2 7.8 .6 11.6c1.8 17.5-10.2 36.4-27.8 36.4H96c-17.7 0-32 14.3-32 32s14.3 32 32 32H240c61.9 0 112-50.1 112-112c0-2.5-.1-5-.2-7.5z"/></svg></span>
                        </button>
                        <ul id="mdbook-theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="mdbook-theme-ayu">Ayu</button></li>
                        </ul>
                        <button id="mdbook-search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="mdbook-searchbar">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M416 208c0 45.9-14.9 88.3-40 122.7L502.6 457.4c12.5 12.5 12.5 32.8 0 45.3s-32.8 12.5-45.3 0L330.7 376c-34.4 25.2-76.8 40-122.7 40C93.1 416 0 322.9 0 208S93.1 0 208 0S416 93.1 416 208zM208 352c79.5 0 144-64.5 144-144s-64.5-144-144-144S64 128.5 64 208s64.5 144 144 144z"/></svg></span>
                        </button>
                    </div>

                    <h1 class="menu-title">Ktransformers</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <span class=fa-svg id="print-button"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M128 0C92.7 0 64 28.7 64 64v96h64V64H354.7L384 93.3V160h64V93.3c0-17-6.7-33.3-18.7-45.3L400 18.7C388 6.7 371.7 0 354.7 0H128zM384 352v32 64H128V384 368 352H384zm64 32h32c17.7 0 32-14.3 32-32V256c0-35.3-28.7-64-64-64H64c-35.3 0-64 28.7-64 64v96c0 17.7 14.3 32 32 32H64v64c0 35.3 28.7 64 64 64H384c35.3 0 64-28.7 64-64V384zm-16-88c-13.3 0-24-10.7-24-24s10.7-24 24-24s24 10.7 24 24s-10.7 24-24 24z"/></svg></span>
                        </a>
                        <a href="https://github.com/kvcache-ai/ktransformers" title="Git repository" aria-label="Git repository">
                            <span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span>
                        </a>

                    </div>
                </div>

                <div id="mdbook-search-wrapper" class="hidden">
                    <form id="mdbook-searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="mdbook-searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="mdbook-searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <span class=fa-svg id="fa-spin"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M304 48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zm0 416c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM48 304c26.5 0 48-21.5 48-48s-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48zm464-48c0-26.5-21.5-48-48-48s-48 21.5-48 48s21.5 48 48 48s48-21.5 48-48zM142.9 437c18.7-18.7 18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zm0-294.2c18.7-18.7 18.7-49.1 0-67.9S93.7 56.2 75 75s-18.7 49.1 0 67.9s49.1 18.7 67.9 0zM369.1 437c18.7 18.7 49.1 18.7 67.9 0s18.7-49.1 0-67.9s-49.1-18.7-67.9 0s-18.7 49.1 0 67.9z"/></svg></span>
                            </div>
                        </div>
                    </form>
                    <div id="mdbook-searchresults-outer" class="searchresults-outer hidden">
                        <div id="mdbook-searchresults-header" class="searchresults-header"></div>
                        <ul id="mdbook-searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('mdbook-sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('mdbook-sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#mdbook-sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="mdbook-content" class="content">
                    <main>
                        <h1 id="introduction-2"><a href="#introduction-2" class="header">Introduction</a></h1>
<div align="center">
  
<p align="center">
<picture>
    <img alt="KTransformers" src="https://github.com/user-attachments/assets/d5a2492f-a415-4456-af99-4ab102f13f8b" width="50%">
</picture>
</p>

  
<h3>A Flexible Framework for Experiencing Cutting-edge LLM Inference/Fine-tune Optimizations</h3>

  <strong><a href="#-overview">üéØ Overview</a> | <a href="#-kt-kernel---high-performance-inference-kernels">üöÄ kt-kernel</a> | <a href="#-kt-sft---fine-tuning-framework">üéì kt-sft</a> | <a href="#-citation">üî• Citation</a> | <a href="https://github.com/kvcache-ai/ktransformers/issues/1582">üöÄ Roadmap(2025Q4)</a>  </strong>
</div>

<h2 id="-overview"><a class="header" href="#-overview">üéØ Overview</a></h2>
<p>KTransformers is a research project focused on efficient inference and fine-tuning of large language models through CPU-GPU heterogeneous computing. The project has evolved into <strong>two core modules</strong>: <a href="kt-kernel">kt-kernel</a> and <a href="kt-sft">kt-sft</a>.</p>
<h2 id="-updates"><a class="header" href="#-updates">üî• Updates</a></h2>
<ul>
<li><strong>Nov 6, 2025</strong>: Support Kimi-K2-Thinking inference (<a href="doc/en/Kimi-K2-Thinking.html">Tutorial</a>) and fine-tune (<a href="doc/en/SFT_Installation_Guide_KimiK2.html">Tutorial</a>)</li>
<li><strong>Nov 4, 2025</strong>: KTransformers Fine-Tuning √ó LLaMA-Factory Integration. (<a href="doc/en/KTransformers-Fine-Tuning_User-Guide.html">Tutorial</a>)</li>
<li><strong>Oct 27, 2025</strong>: Support Ascend NPU. (<a href="doc/zh/DeepseekR1_V3_tutorial_zh_for_Ascend_NPU.html">Tutorial</a>)</li>
<li><strong>Oct 10, 2025</strong>: Integrating into SGLang. (<a href="https://github.com/sgl-project/sglang/issues/11425">Roadmap</a>, <a href="https://lmsys.org/blog/2025-10-22-KTransformers/">Blog</a>)</li>
<li><strong>Sept 11, 2025</strong>: Support Qwen3-Next. (<a href="doc/en/Qwen3-Next.html">Tutorial</a>)</li>
<li><strong>Sept 05, 2025</strong>: Support Kimi-K2-0905. (<a href="doc/en/Kimi-K2.html">Tutorial</a>)</li>
<li><strong>July 26, 2025</strong>: Support SmallThinker and GLM4-MoE. (<a href="doc/en/SmallThinker_and_Glm4moe.html">Tutorial</a>)</li>
<li><strong>July 11, 2025</strong>: Support Kimi-K2. (<a href="doc/en/Kimi-K2.html">Tutorial</a>)</li>
<li><strong>June 30, 2025</strong>: Support 3-layer (GPU-CPU-Disk) <a href="doc/en/prefix_cache.html">prefix cache</a> reuse.</li>
<li><strong>May 14, 2025</strong>: Support Intel Arc GPU (<a href="doc/en/xpu.html">Tutorial</a>).</li>
<li><strong>Apr 29, 2025</strong>: Support AMX-Int8„ÄÅ AMX-BF16 and Qwen3MoE (<a href="doc/en/AMX.html">Tutorial</a>)</li>
<li><strong>Apr 9, 2025</strong>: Experimental support for LLaMA 4 models (<a href="doc/en/llama4.html">Tutorial</a>).</li>
<li><strong>Apr 2, 2025</strong>: Support Multi-concurrency. (<a href="doc/en/balance-serve.html">Tutorial</a>).</li>
<li><strong>Mar 15, 2025</strong>: Support ROCm on AMD GPU (<a href="doc/en/ROCm.html">Tutorial</a>).</li>
<li><strong>Mar 5, 2025</strong>: Support unsloth 1.58/2.51 bits weights and <a href="doc/en/fp8_kernel.html">IQ1_S/FP8 hybrid</a> weights. Support 139K <a href="doc/en/DeepseekR1_V3_tutorial.html#v022--v023-longer-context--fp8-kernel">Longer Context</a> for DeepSeek-V3 and R1 in 24GB VRAM.</li>
<li><strong>Feb 25, 2025</strong>: Support <a href="doc/en/fp8_kernel.html">FP8 GPU kernel</a> for DeepSeek-V3 and R1; <a href="doc/en/DeepseekR1_V3_tutorial.html#v022-longer-context">Longer Context</a>.</li>
<li><strong>Feb 15, 2025</strong>: Longer Context (from 4K to 8K for 24GB VRAM) &amp; Slightly Faster Speed Ôºà+15%, up to 16 Tokens/s), update <a href="doc/en/DeepseekR1_V3_tutorial.html">docs</a> and <a href="https://kvcache-ai.github.io/ktransformers/">online books</a>.</li>
<li><strong>Feb 10, 2025</strong>: Support Deepseek-R1 and V3 on single (24GB VRAM)/multi gpu and 382G DRAM, up to 3~28x speedup. For detailed show case and reproduction tutorial, see <a href="doc/en/DeepseekR1_V3_tutorial.html">here</a>.</li>
<li><strong>Aug 28, 2024</strong>: Decrease DeepseekV2‚Äôs required VRAM from 21G to 11G.</li>
<li><strong>Aug 15, 2024</strong>: Update detailed <a href="doc/en/injection_tutorial.html">tutorial</a> for injection and multi-GPU.</li>
<li><strong>Aug 14, 2024</strong>: Support llamfile as linear backend.</li>
<li><strong>Aug 12, 2024</strong>: Support multiple GPU; Support new model: mixtral 8*7B  and 8*22B; Support q2k, q3k, q5k dequant on gpu.</li>
<li><strong>Aug 9, 2024</strong>: Support windows native.</li>
</ul>
<hr>
<h2 id="-core-modules"><a class="header" href="#-core-modules">üì¶ Core Modules</a></h2>
<h3 id="-kt-kernel---high-performance-inference-kernels"><a class="header" href="#-kt-kernel---high-performance-inference-kernels">üöÄ <a href="kt-kernel">kt-kernel</a> - High-Performance Inference Kernels</a></h3>
<p>CPU-optimized kernel operations for heterogeneous LLM inference.</p>
<img width="1049" height="593" alt="image" src="https://github.com/user-attachments/assets/68f423da-3f55-4025-bdc9-9ceaa554f00b" />
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>AMX/AVX Acceleration</strong>: Intel AMX and AVX512/AVX2 optimized kernels for INT4/INT8 quantized inference</li>
<li><strong>MoE Optimization</strong>: Efficient Mixture-of-Experts inference with NUMA-aware memory management</li>
<li><strong>Quantization Support</strong>: CPU-side INT4/INT8 quantized weights, GPU-side GPTQ support</li>
<li><strong>Easy Integration</strong>: Clean Python API for SGLang and other frameworks</li>
</ul>
<p><strong>Quick Start:</strong></p>
<pre><code class="language-bash">cd kt-kernel
pip install .
</code></pre>
<p><strong>Use Cases:</strong></p>
<ul>
<li>CPU-GPU hybrid inference for large MoE models</li>
<li>Integration with SGLang for production serving</li>
<li>Heterogeneous expert placement (hot experts on GPU, cold experts on CPU)</li>
</ul>
<p><strong>Performance Examples:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>Hardware Configuration</th><th>Total Throughput</th><th>Output Throughput</th></tr>
</thead>
<tbody>
<tr><td>DeepSeek-R1-0528 (FP8)</td><td>8√óL20 GPU + Xeon Gold 6454S</td><td>227.85 tokens/s</td><td>87.58 tokens/s (8-way concurrency)</td></tr>
</tbody>
</table>
</div>
<p>üëâ <strong><a href="kt-kernel/README.html">Full Documentation ‚Üí</a></strong></p>
<hr>
<h3 id="-kt-sft---fine-tuning-framework"><a class="header" href="#-kt-sft---fine-tuning-framework">üéì <a href="kt-sft">kt-sft</a> - Fine-Tuning Framework</a></h3>
<p>KTransformers √ó LLaMA-Factory integration for ultra-large MoE model fine-tuning.</p>
<p><img src="doc/assets/image-20251011010558909.png" alt="image-20251011010558909"></p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Resource Efficient</strong>: Fine-tune 671B DeepSeek-V3 with just <strong>70GB GPU memory</strong> + 1.3TB RAM</li>
<li><strong>LoRA Support</strong>: Full LoRA fine-tuning with heterogeneous acceleration</li>
<li><strong>LLaMA-Factory Integration</strong>: Seamless integration with popular fine-tuning framework</li>
<li><strong>Production Ready</strong>: Chat, batch inference, and metrics evaluation</li>
</ul>
<p><strong>Performance Examples:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>Configuration</th><th>Throughput</th><th>GPU Memory</th></tr>
</thead>
<tbody>
<tr><td>DeepSeek-V3 (671B)</td><td>LoRA + AMX</td><td>~40 tokens/s</td><td>70GB (multi-GPU)</td></tr>
<tr><td>DeepSeek-V2-Lite (14B)</td><td>LoRA + AMX</td><td>~530 tokens/s</td><td>6GB</td></tr>
</tbody>
</table>
</div>
<p><strong>Quick Start:</strong></p>
<pre><code class="language-bash">cd kt-sft
# Install environment following kt-sft/README.md
USE_KT=1 llamafactory-cli train examples/train_lora/deepseek3_lora_sft_kt.yaml
</code></pre>
<p>üëâ <strong><a href="kt-sft/README.html">Full Documentation ‚Üí</a></strong></p>
<hr>
<h2 id="-citation"><a class="header" href="#-citation">üî• Citation</a></h2>
<p>If you use KTransformers in your research, please cite our paper:</p>
<pre><code class="language-bibtex">@inproceedings{10.1145/3731569.3764843,
  title = {KTransformers: Unleashing the Full Potential of CPU/GPU Hybrid Inference for MoE Models},
  author = {Chen, Hongtao and Xie, Weiyu and Zhang, Boxin and Tang, Jingqi and Wang, Jiahao and Dong, Jianwei and Chen, Shaoyuan and Yuan, Ziwei and Lin, Chen and Qiu, Chengyu and Zhu, Yuening and Ou, Qingliang and Liao, Jiaqi and Chen, Xianglin and Ai, Zhiyuan and Wu, Yongwei and Zhang, Mingxing},
  booktitle = {Proceedings of the ACM SIGOPS 31st Symposium on Operating Systems Principles},
  year = {2025}
}
</code></pre>
<h2 id="-contributors--team"><a class="header" href="#-contributors--team">üë• Contributors &amp; Team</a></h2>
<p>Developed and maintained by:</p>
<ul>
<li><a href="https://madsys.cs.tsinghua.edu.cn/">MADSys Lab</a> @ Tsinghua University</li>
<li><a href="http://approaching.ai/">Approaching.AI</a></li>
<li>Community contributors</li>
</ul>
<p>We welcome contributions! Please feel free to submit issues and pull requests.</p>
<h2 id="-community--support"><a class="header" href="#-community--support">üí¨ Community &amp; Support</a></h2>
<ul>
<li><strong>GitHub Issues</strong>: <a href="https://github.com/kvcache-ai/ktransformers/issues">Report bugs or request features</a></li>
<li><strong>WeChat Group</strong>: See <a href="archive/WeChatGroup.png">archive/WeChatGroup.png</a></li>
</ul>
<h2 id="-kt-original-code"><a class="header" href="#-kt-original-code">üì¶ KT original Code</a></h2>
<p>The original integrated KTransformers framework has been archived to the <a href="archive"><code>archive/</code></a> directory for reference. The project now focuses on the two core modules above for better modularity and maintainability.</p>
<p>For the original documentation with full quick-start guides and examples, see:</p>
<ul>
<li><a href="archive/README.html">archive/README.md</a> (English)</li>
<li><a href="archive/README_ZH.html">archive/README_ZH.md</a> (‰∏≠Êñá)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kt-kernel"><a class="header" href="#kt-kernel">KT-Kernel</a></h1>
<p>High-performance kernel operations for KTransformers, featuring CPU-optimized MoE inference with AMX, AVX, KML and blis (amd library) support.</p>
<ul>
<li><a href="#note">Note</a></li>
<li><a href="#features">Features</a></li>
<li><a href="#installation">Installation</a>
<ul>
<li><a href="#prerequisites">Prerequisites</a></li>
<li><a href="#quick-installation-recommended">Quick Installation (Recommended)</a></li>
<li><a href="#manual-configuration-advanced">Manual Configuration (Advanced)</a></li>
</ul>
</li>
<li><a href="#verification">Verification</a></li>
<li><a href="#integration-with-sglang">Integration with SGLang</a>
<ul>
<li><a href="#installation-steps">Installation Steps</a></li>
<li><a href="#complete-example-qwen3-30b-a3b">Complete Example: Qwen3-30B-A3B</a></li>
<li><a href="#kt-kernel-parameters">KT-Kernel Parameters</a></li>
</ul>
</li>
<li><a href="#direct-python-api-usage">Direct Python API Usage</a>
<ul>
<li><a href="#advanced-options">Advanced Options</a></li>
</ul>
</li>
<li><a href="#build-configuration">Build Configuration</a>
<ul>
<li><a href="#manual-installation">Manual Installation</a></li>
</ul>
</li>
<li><a href="#error-troubleshooting">Error Troubleshooting</a>
<ul>
<li><a href="#cuda-not-found">CUDA Not Found</a></li>
<li><a href="#hwloc-not-found">hwloc Not Found</a></li>
</ul>
</li>
<li><a href="#weight-quantization">Weight Quantization</a></li>
<li><a href="#before-commit">Before Commit!</a></li>
</ul>
<h2 id="note"><a class="header" href="#note">Note</a></h2>
<p><strong>Current Support Status:</strong></p>
<ul>
<li>‚úÖ <strong>Intel CPUs with AMX</strong>: Fully supported (using weights converted to INT4/INT8 format)</li>
<li>‚úÖ <strong>Universal CPU (llamafile backend)</strong>: Supported (using GGUF-format weights)</li>
<li>‚úÖ <strong>AMD CPUs with BLIS</strong>: Supported (for int8 prefill &amp; decode)</li>
</ul>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<ul>
<li><strong>CPU-Optimized MoE Kernels</strong>: High-throughput MoE expert kernels optimized for instruction sets.</li>
<li><strong>AMX INT4/INT8 Backend</strong>: INT4 / INT8 quantized expert inference backend for AMX-capable servers.</li>
<li><strong>Llamafile CPU Backend</strong>: AVX2/AVX512-based MoE backend built on Llamafile for universal CPU deployment.</li>
<li><strong>NUMA-Aware Execution</strong>: Thread pool and memory layout designed for multi-socket / multi-NUMA machines.</li>
</ul>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<h3 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h3>
<p>First, initialize git submodules:</p>
<pre><code class="language-bash">git submodule update --init --recursive
</code></pre>
<h3 id="quick-installation-recommended"><a class="header" href="#quick-installation-recommended">Quick Installation (Recommended)</a></h3>
<p>Step 0: Create and activate a conda environment (recommended):</p>
<pre><code class="language-bash">conda create -n kt-kernel python=3.11 -y
conda activate kt-kernel
</code></pre>
<p>You can now install in two clear steps using the same script.</p>
<p>Option A: Two-step (specify dependencies installation and build separately)</p>
<pre><code class="language-bash"># 1) Install system prerequisites (cmake, hwloc, pkg-config)
./install.sh deps

# 2) Build and install kt-kernel (auto-detects CPU instruction set)
#    By default, the script cleans the local ./build directory before compiling
./install.sh build
</code></pre>
<p>Option B: One-step</p>
<pre><code class="language-bash">./install.sh
</code></pre>
<p>The install script will:</p>
<ul>
<li>Auto-detect CPU capabilities (AMX support)</li>
<li>Install <code>cmake</code> via conda (if available)</li>
<li>Install system dependencies (<code>libhwloc-dev</code>, <code>pkg-config</code>) based on your OS</li>
</ul>
<p><strong>What gets configured automatically:</strong></p>
<ul>
<li>AMX CPU detected ‚Üí <code>NATIVE + AMX=ON</code></li>
<li>No AMX detected ‚Üí <code>NATIVE + AMX=OFF</code></li>
</ul>
<p>‚ö†Ô∏è <strong>Important for LLAMAFILE backend users:</strong>
If you have an AMX-capable CPU but plan to use the LLAMAFILE backend, do NOT use the default auto-detection build.
Use ‚Äúmanual mode‚Äù with <code>CPUINFER_CPU_INSTRUCT</code> set to <code>AVX512</code> or <code>AVX2</code> instead of <code>NATIVE</code> to avoid compilation issues (see below).</p>
<h3 id="manual-configuration-advanced"><a class="header" href="#manual-configuration-advanced">Manual Configuration (Advanced)</a></h3>
<p>If you need specific build options (e.g., for LLAMAFILE backend, compatibility, or binary distribution):</p>
<pre><code class="language-bash"># Example for LLAMAFILE backend on AMX CPU with AVX512
export CPUINFER_CPU_INSTRUCT=AVX512  # Options: NATIVE, AVX512, AVX2, FANCY
export CPUINFER_ENABLE_AMX=OFF       # Options: ON, OFF

# Build only (skip auto-detection of instruction set)
./install.sh build --manual
</code></pre>
<p>For advanced build options and binary distribution, see the <a href="#build-configuration">Build Configuration</a> section. If you encounter issues, refer to <a href="#error-troubleshooting">Error Troubleshooting</a>.</p>
<h2 id="verification"><a class="header" href="#verification">Verification</a></h2>
<pre><code class="language-bash">python -c "from kt_kernel import KTMoEWrapper; print('‚úì kt-kernel installed successfully')"
</code></pre>
<h2 id="integration-with-sglang"><a class="header" href="#integration-with-sglang">Integration with SGLang</a></h2>
<p>KT-Kernel can be used standalone via <a href="#direct-python-api-usage">Direct Python API</a> or integrated with SGLang for production deployment. This section describes SGLang integration to enable CPU-GPU heterogeneous inference, where ‚Äúhot‚Äù experts run on GPU and ‚Äúcold‚Äù experts run on CPU for optimal resource utilization.</p>
<h3 id="installation-steps"><a class="header" href="#installation-steps">Installation Steps</a></h3>
<h4 id="1-install-sglang"><a class="header" href="#1-install-sglang">1. Install SGLang</a></h4>
<pre><code class="language-bash">git clone https://github.com/sgl-project/sglang.git
cd sglang
pip install -e "python[all]"
</code></pre>
<h4 id="2-prepare-weights"><a class="header" href="#2-prepare-weights">2. Prepare Weights</a></h4>
<p>You need both GPU weights and CPU-side expert weights for heterogeneous inference. The exact format depends on the backend:</p>
<p><strong>GPU Weights (for all backends):</strong><br>Use the model weights required by SGLang for GPU inference (for example, the original or already-quantized model directory from Hugging Face).</p>
<p><strong>CPU Weights (AMX backend: <code>AMXINT4</code> / <code>AMXINT8</code>):</strong>
Quantize weights to AMX-optimized INT4/INT8 format using the provided script:</p>
<pre><code class="language-bash">python scripts/convert_cpu_weights.py \
  --input-path /path/to/model \
  --input-type bf16 \
  --output /path/to/cpu-weights \
  --quant-method int8  # or int4 or moe_int8 (for amd now) 
</code></pre>
<ul>
<li><code>--input-path</code>: Path to GPU-side original weights</li>
<li><code>--input-type</code>: Depends on your GPU weights type (<code>fp8</code>, <code>fp16</code>, or <code>bf16</code>)</li>
</ul>
<p>In SGLang integration, <code>--kt-weight-path</code> should point to this converted CPU weights directory.</p>
<p><strong>Supported input formats:</strong> FP8, FP16, BF16 ‚Üí INT4/INT8.</p>
<p><strong>CPU Weights (LLAMAFILE backend: <code>LLAMAFILE</code>):</strong>
LLAMAFILE uses pre-quantized <strong>GGUF</strong> weights on the CPU side directly, without running <code>convert_cpu_weights.py</code>. You need to:</p>
<ul>
<li>Download a GGUF model directly from the web (e.g., GGUF repos on Hugging Face / Modelscope);</li>
<li>In SGLang integration, use that GGUF directory as <code>--kt-weight-path</code>.
KT-Kernel supports multiple GGUF quantization formats such as <code>Q4_KM</code>, <code>Q4_K</code>, <code>Q5_K</code>, etc. Choose based on your latency and accuracy requirements.</li>
</ul>
<h4 id="3-launch-sglang-server"><a class="header" href="#3-launch-sglang-server">3. Launch SGLang Server</a></h4>
<p>Start the SGLang server with your normal SGLang parameters, and add the following KT-Kernel specific parameters to enable CPU-GPU heterogeneous inference:</p>
<p><strong>KT-Kernel Parameters to Add:</strong></p>
<ul>
<li><code>--kt-method</code>: Backend method (AMXINT4, AMXINT8, or LLAMAFILE)</li>
<li><code>--kt-weight-path</code>: Path to the converted CPU weights</li>
<li><code>--kt-cpuinfer</code>: Number of CPU inference threads (set to physical cores)</li>
<li><code>--kt-threadpool-count</code>: Number of thread pools (set to NUMA node count)</li>
<li><code>--kt-num-gpu-experts</code>: Number of experts to keep on GPU</li>
<li><code>--kt-max-deferred-experts-per-token</code>: Deferred experts for pipelined execution</li>
</ul>
<p>Example:</p>
<pre><code class="language-bash">python -m sglang.launch_server \
  [your normal SGLang parameters...] \
  --kt-method AMXINT8 \
  --kt-weight-path /path/to/cpu-weights \
  --kt-cpuinfer 64 \
  --kt-threadpool-count 2 \
  --kt-num-gpu-experts 32 \
  --kt-max-deferred-experts-per-token 2
</code></pre>
<p>See <a href="#kt-kernel-parameters">KT-Kernel Parameters</a> section below for detailed parameter tuning guidelines.</p>
<h3 id="complete-example-qwen3-30b-a3b"><a class="header" href="#complete-example-qwen3-30b-a3b">Complete Example: Qwen3-30B-A3B</a></h3>
<p>This example demonstrates the full workflow from downloading weights to launching the server, showing both <strong>AMX backend</strong> and <strong>LLAMAFILE backend</strong> options.</p>
<p><strong>Hardware Configuration:</strong></p>
<ul>
<li><strong>GPU</strong>: NVIDIA RTX 4090 24GB</li>
<li><strong>CPU</strong>: 2x Intel Xeon Gold 6454S (64 physical cores total, 128 threads, 2 NUMA nodes)</li>
<li><strong>Model</strong>: <a href="https://huggingface.co/Qwen/Qwen3-30B-A3B">Qwen3-30B-A3B</a></li>
</ul>
<p><strong>How to verify your system configuration:</strong></p>
<pre><code class="language-bash"># Check CPU configuration
lscpu | grep -E "^CPU\(s\)|Thread\(s\) per core|Socket\(s\)|NUMA node\(s\)"
# Expected output example:
CPU(s):                                  128
Thread(s) per core:                      2
Socket(s):                               2
NUMA node(s):                            2
# ‚Üí Physical cores = CPU(s) / Thread(s) per core = 128 / 2 = 64
</code></pre>
<p><strong>Parameter Rationale:</strong></p>
<ul>
<li><code>--kt-cpuinfer 64</code>: Set to physical cores (64), not hyperthreads (128)</li>
<li><code>--kt-threadpool-count 2</code>: 2 NUMA nodes detected (dual-socket system)</li>
<li><code>--kt-num-gpu-experts 32</code>: With 24GB GPU memory, we can fit ~32 experts on GPU for this model (varies by model architecture and actual memory usage)</li>
<li><code>--kt-max-deferred-experts-per-token 2</code>: Enable pipelined execution; allows CPU to process next batch while GPU completes current batch</li>
</ul>
<hr>
<h4 id="option-a-amx-backend-amxint8"><a class="header" href="#option-a-amx-backend-amxint8">Option A: AMX Backend (AMXINT8)</a></h4>
<p>For Intel CPUs with AMX instruction set support.</p>
<p><strong>Step 1: Download model weights</strong></p>
<pre><code class="language-bash"># Install huggingface-cli if not already installed
pip install huggingface-hub

# Download model from Hugging Face
huggingface-cli download Qwen/Qwen3-30B-A3B --local-dir /mnt/data/models/Qwen3-30B-A3B
</code></pre>
<p><strong>Step 2: Convert to CPU weights (AMXINT8)</strong></p>
<pre><code class="language-bash">python scripts/convert_cpu_weights.py \
  --input-path /mnt/data/models/Qwen3-30B-A3B \
  --input-type bf16 \
  --output /mnt/data/models/Qwen3-30B-A3B-INT8 \
  --quant-method int8
</code></pre>
<p><strong>Step 3: Launch SGLang server</strong></p>
<pre><code class="language-bash">python -m sglang.launch_server \
  --host 0.0.0.0 \
  --port 8000 \
  --model /mnt/data/models/Qwen3-30B-A3B \
  --trust-remote-code \
  --mem-fraction-static 0.92 \
  --chunked-prefill-size 4096 \
  --served-model-name Qwen3-30B-A3B \
  --enable-mixed-chunk \
  --kt-method AMXINT8 \
  --kt-weight-path /mnt/data/models/Qwen3-30B-A3B-INT8 \
  --kt-cpuinfer 64 \
  --kt-threadpool-count 2 \
  --kt-num-gpu-experts 32 \
  --kt-max-deferred-experts-per-token 2
</code></pre>
<hr>
<h4 id="option-b-llamafile-backend-gguf"><a class="header" href="#option-b-llamafile-backend-gguf">Option B: LLAMAFILE Backend (GGUF)</a></h4>
<p>For universal CPUs (no AMX required), using pre-quantized GGUF weights directly.</p>
<p><strong>Step 1: Download GPU weights (original model)</strong></p>
<pre><code class="language-bash">pip install huggingface-hub

huggingface-cli download Qwen/Qwen3-30B-A3B --local-dir /mnt/data/models/Qwen3-30B-A3B
</code></pre>
<p><strong>Step 2: Download CPU weights (GGUF format)</strong></p>
<pre><code class="language-bash">huggingface-cli download Qwen/Qwen3-30B-A3B-GGUF Qwen3-30B-A3B-Q4_K_M.gguf \
  --local-dir /mnt/data/models/Qwen3-30B-A3B-Q4_K_M
</code></pre>
<p><strong>Step 3: Launch SGLang server</strong></p>
<pre><code class="language-bash">python -m sglang.launch_server \
  --host 0.0.0.0 \
  --port 8000 \
  --model /mnt/data/models/Qwen3-30B-A3B \
  --trust-remote-code \
  --mem-fraction-static 0.92 \
  --chunked-prefill-size 4096 \
  --served-model-name Qwen3-30B-A3B \
  --enable-mixed-chunk \
  --kt-method LLAMAFILE \
  --kt-weight-path /mnt/data/models/Qwen3-30B-A3B-Q4_K_M \
  --kt-cpuinfer 64 \
  --kt-threadpool-count 2 \
  --kt-num-gpu-experts 32 \
  --kt-max-deferred-experts-per-token 2
</code></pre>
<h3 id="kt-kernel-parameters"><a class="header" href="#kt-kernel-parameters">KT-Kernel Parameters</a></h3>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Parameter</th><th>Description</th><th>Example Value</th></tr>
</thead>
<tbody>
<tr><td><code>--kt-method</code></td><td>CPU inference backend method</td><td><code>AMXINT4</code>, <code>AMXINT8</code>, or <code>LLAMAFILE</code></td></tr>
<tr><td><code>--kt-weight-path</code></td><td>Path to quantized CPU weights</td><td><code>/path/to/cpu-weights</code></td></tr>
<tr><td><code>--kt-cpuinfer</code></td><td>Number of CPU inference threads</td><td><code>64</code> (adjust based on CPU cores)</td></tr>
<tr><td><code>--kt-threadpool-count</code></td><td>Number of thread pools for parallel execution</td><td><code>2</code> (typically 1-4)</td></tr>
<tr><td><code>--kt-num-gpu-experts</code></td><td>Number of experts to keep on GPU</td><td><code>32</code> (remaining experts go to CPU)</td></tr>
<tr><td><code>--kt-max-deferred-experts-per-token</code></td><td>Number of experts per token to defer for pipelined execution</td><td><code>2</code> (0 to disable, 1-4 recommended)</td></tr>
</tbody>
</table>
</div>
<p><strong>Parameter Guidelines:</strong></p>
<ul>
<li>
<p><strong><code>kt-method</code></strong>: Choose based on your CPU and weight format:</p>
<ul>
<li><code>AMXINT4</code>: Best performance on AMX CPUs with INT4 quantized weights (May cause huge accuracy drop for some models, e.g., Qwen3-30B-A3B)</li>
<li><code>AMXINT8</code>: Higher accuracy with INT8 quantized weights on AMX CPUs</li>
<li><code>LLAMAFILE</code>: GGUF-based backend</li>
</ul>
</li>
<li>
<p><strong><code>kt-cpuinfer</code></strong>: Set to the number of <strong>physical CPU cores</strong> (not hyperthreads).</p>
<ul>
<li>Check physical cores: <code>lscpu | grep -E "^CPU\(s\)|Thread\(s\) per core"</code></li>
<li>Physical cores = CPU(s) / Thread(s) per core</li>
<li>Example: If CPU(s)=128 and Thread(s) per core=2, then physical cores = 64</li>
<li><strong>Important</strong>: Do NOT set to hyperthread count - this will degrade performance</li>
</ul>
</li>
<li>
<p><strong><code>kt-threadpool-count</code></strong>: Set to the number of <strong>NUMA nodes</strong>.</p>
<ul>
<li>Check NUMA count: <code>lscpu | grep "NUMA node(s)"</code></li>
<li>Or use: <code>numactl --hardware | grep "available"</code></li>
<li><strong>Note</strong>: NUMA node count is NOT necessarily the number of physical CPUs
<ul>
<li>It represents memory domains, which may be divided within a single CPU or across multiple CPUs</li>
<li>Use the NUMA node count from <code>lscpu</code>, regardless of physical CPU count</li>
</ul>
</li>
<li>Typical values: 1-2 for single-socket, 2-4 for dual-socket systems</li>
<li>This enables better memory bandwidth utilization across NUMA domains</li>
</ul>
</li>
<li>
<p><strong><code>kt-num-gpu-experts</code></strong>: Determine based on GPU memory and profiling:</p>
<ul>
<li>More GPU experts = lower latency but higher GPU memory usage (May cause OOM)</li>
</ul>
</li>
<li>
<p><strong><code>kt-max-deferred-experts-per-token</code></strong>: Enables pipelined execution:</p>
<ul>
<li><code>0</code>: Synchronous execution (simpler, higher latency)</li>
<li><code>1-4</code>: Deferred execution (recommended range; good latency/quality balance, requires tuning)</li>
<li><code>5-7</code>: Highest latency reduction but may introduce noticeable accuracy loss; use with care</li>
</ul>
</li>
</ul>
<h2 id="direct-python-api-usage"><a class="header" href="#direct-python-api-usage">Direct Python API Usage</a></h2>
<p>For standalone usage without SGLang, you can use KT-Kernel directly via Python API:</p>
<pre><code class="language-python">from kt_kernel import KTMoEWrapper

# Initialize the MoE wrapper
wrapper = KTMoEWrapper(
    layer_idx=0,
    num_experts=8,
    num_experts_per_tok=2,
    hidden_size=4096,
    moe_intermediate_size=14336,
    num_gpu_experts=2,
    cpuinfer_threads=32,
    threadpool_count=2,
    weight_path="/path/to/weights",
    chunked_prefill_size=512,
    method="AMXINT4"  # Options: "AMXINT4", "AMXINT8", "LLAMAFILE"
)

# Load weights (from disk - pre-quantized)
wrapper.load_weights(physical_to_logical_map)

# Or load weights from tensors (online quantization)
wrapper.load_weights_from_tensors(gate_proj, up_proj, down_proj, physical_to_logical_map)

# Run inference
output = wrapper.forward(hidden_states, topk_ids, topk_weights, cuda_stream)

# Or use async API for better performance
wrapper.submit_forward(hidden_states, topk_ids, topk_weights, cuda_stream)
# ... do other work ...
output = wrapper.sync_forward(hidden_states, cuda_stream)
</code></pre>
<h3 id="advanced-options"><a class="header" href="#advanced-options">Advanced Options</a></h3>
<pre><code class="language-python"># Initialize with additional options
wrapper = KTMoEWrapper(
    layer_idx=0,
    num_experts=8,
    num_experts_per_tok=2,
    hidden_size=4096,
    moe_intermediate_size=14336,
    num_gpu_experts=2,
    cpuinfer_threads=32,
    threadpool_count=2,
    weight_path="/path/to/weights",
    chunked_prefill_size=512,
    method="AMXINT4",
    cpu_save=False,  # Keep weights in CPU memory after loading
    max_deferred_experts_per_token=0  # Number of experts to defer (for pipelined execution)
)

# Pre-allocate buffers for specific batch sizes (improves performance)
KTMoEWrapper.set_capture_batch_sizes([1, 2, 4, 8, 16])

# Query captured batch sizes
batch_sizes = KTMoEWrapper.get_capture_batch_sizes()

# Clear buffer cache to free memory
KTMoEWrapper.clear_buffer_cache()
</code></pre>
<h2 id="build-configuration"><a class="header" href="#build-configuration">Build Configuration</a></h2>
<h3 id="manual-installation"><a class="header" href="#manual-installation">Manual Installation</a></h3>
<p>If you prefer manual installation without the <code>install.sh</code> script, follow these steps:</p>
<h4 id="1-install-system-dependencies"><a class="header" href="#1-install-system-dependencies">1. Install System Dependencies</a></h4>
<p><strong>Prerequisites:</strong></p>
<ul>
<li><code>cmake</code> (recommended: <code>conda install -y cmake</code>)</li>
<li><code>libhwloc-dev</code> and <code>pkg-config</code></li>
</ul>
<h4 id="2-set-build-configuration"><a class="header" href="#2-set-build-configuration">2. Set Build Configuration</a></h4>
<p><strong>Core Options:</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Variable</th><th>Options</th><th>Description</th></tr>
</thead>
<tbody>
<tr><td><code>CPUINFER_CPU_INSTRUCT</code></td><td><code>NATIVE</code>, <code>AVX512</code>, <code>AVX2</code>, <code>FANCY</code></td><td>CPU instruction set to use</td></tr>
<tr><td><code>CPUINFER_ENABLE_AMX</code></td><td><code>ON</code>, <code>OFF</code></td><td>Enable Intel AMX support</td></tr>
<tr><td><code>CPUINFER_BUILD_TYPE</code></td><td><code>Release</code>, <code>Debug</code>, <code>RelWithDebInfo</code></td><td>Build type (default: <code>Release</code>)</td></tr>
<tr><td><code>CPUINFER_PARALLEL</code></td><td>Number</td><td>Parallel build jobs (default: auto-detect)</td></tr>
<tr><td><code>CPUINFER_VERBOSE</code></td><td><code>0</code>, <code>1</code></td><td>Verbose build output (default: <code>0</code>)</td></tr>
</tbody>
</table>
</div>
<p><strong>Instruction Set Details:</strong></p>
<ul>
<li><strong><code>NATIVE</code></strong>: Auto-detect and use all available CPU instructions (<code>-march=native</code>) - <strong>Recommended for best performance</strong></li>
<li><strong><code>AVX512</code></strong>: Explicit AVX512 support for Skylake-SP and Cascade Lake</li>
<li><strong><code>AVX2</code></strong>: AVX2 support for maximum compatibility</li>
<li><strong><code>FANCY</code></strong>: AVX512 with full extensions (AVX512F/BW/DQ/VL/VNNI) for Ice Lake+ and Zen 4+. Use this when building pre-compiled binaries to distribute to users with modern CPUs. For local builds, prefer <code>NATIVE</code> for better performance.</li>
</ul>
<p><strong>Example Configurations:</strong></p>
<pre><code class="language-bash"># Maximum performance on AMX CPU
export CPUINFER_CPU_INSTRUCT=NATIVE
export CPUINFER_ENABLE_AMX=ON

# AVX512 CPU without AMX
export CPUINFER_CPU_INSTRUCT=AVX512
export CPUINFER_ENABLE_AMX=OFF

# Compatibility build
export CPUINFER_CPU_INSTRUCT=AVX2
export CPUINFER_ENABLE_AMX=OFF

# Debug build for development
export CPUINFER_BUILD_TYPE=Debug
export CPUINFER_VERBOSE=1
</code></pre>
<h4 id="3-build-and-install"><a class="header" href="#3-build-and-install">3. Build and Install</a></h4>
<pre><code class="language-bash"># Editable installation (for development)
pip install -e .

# Standard installation
pip install .
</code></pre>
<h2 id="error-troubleshooting"><a class="header" href="#error-troubleshooting">Error Troubleshooting</a></h2>
<h3 id="cuda-not-found"><a class="header" href="#cuda-not-found">CUDA Not Found</a></h3>
<pre><code> -- Looking for a CUDA compiler - NOTFOUND
  CMake Error at CMakeLists.txt:389 (message):
    KTRANSFORMERS_USE_CUDA=ON but CUDA compiler not found
</code></pre>
<p>Make sure you have the CUDA toolkit installed and <code>nvcc</code> is in your system PATH.</p>
<p>Try <code>export CMAKE_ARGS="-D CMAKE_CUDA_COMPILER=$(which nvcc)"</code> and reinstall again.</p>
<h3 id="hwloc-not-found"><a class="header" href="#hwloc-not-found">hwloc Not Found</a></h3>
<p>Run <code>sudo apt install libhwloc-dev</code> if on a Debian-based system or build from source: https://www.open-mpi.org/projects/hwloc/.</p>
<pre><code>wget https://download.open-mpi.org/release/hwloc/v2.12/hwloc-2.12.2.tar.gz
tar -xzf hwloc-2.12.2.tar.gz
cd hwloc-2.12.2
./configure
make
sudo make install
</code></pre>
<h2 id="weight-quantization"><a class="header" href="#weight-quantization">Weight Quantization</a></h2>
<p>For AMX backends (<code>AMXINT4</code> / <code>AMXINT8</code>), CPU-side experts must be converted to AMX-friendly INT4/INT8 format using the provided script:</p>
<pre><code class="language-bash">python scripts/convert_cpu_weights.py \
  --input-path /path/to/model \
  --input-type bf16 \
  --output /path/to/output \
  --quant-method int4
</code></pre>
<p><strong>Supported formats:</strong> FP8, FP16, BF16 ‚Üí INT4/INT8</p>
<p>For LLAMAFILE backend (<code>LLAMAFILE</code>), CPU-side experts are loaded directly from <strong>GGUF</strong> weights. You do <strong>not</strong> need to run the AMX conversion script; instead, download a GGUF model from the web (e.g., a GGUF repo on Hugging Face) and point <code>weight_path</code> / SGLang <code>--kt-weight-path</code> (or <code>--model</code> when appropriate) to that GGUF directory. KT-Kernel supports multiple GGUF quantization types such as <code>Q4_KM</code>, <code>Q4_K</code>, <code>Q5_K</code>, etc.</p>
<hr>
<p>For detailed documentation, advanced options, and low-memory mode, see <a href="en/kt-kernel/scripts/README.html">scripts/README.md</a>.</p>
<h2 id="before-commit"><a class="header" href="#before-commit">Before Commit!</a></h2>
<p>Commit messages should follow the Conventional Commits specification: https://www.conventionalcommits.org/</p>
<p>Please format your code before committing:</p>
<pre><code class="language-shell">cmake -B build
cd build
make format
</code></pre>
<p>You may need a newer clang-format (at least version 18). In a conda environment:</p>
<pre><code class="language-shell">conda install -c conda-forge clang-format=18
rm -rf build
</code></pre>
<p>It‚Äôs also recommended to install black for Python code formatting:</p>
<pre><code class="language-shell">conda install black
</code></pre>
<div style="break-before: page; page-break-before: always;"></div>
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#fine-tuning-results-examples">Fine-Tuning Results (Examples)</a></li>
</ul>
</li>
<li><a href="#quick-to-start">Quick to Start</a>
<ul>
<li><a href="#environment-setup">Environment Setup</a></li>
<li><a href="#core-feature-1-use-ktransformers-backend-to-fine-tune-ultra-large-moe-models">Core Feature 1: Use KTransformers backend to fine-tune ultra-large MoE models</a></li>
<li><a href="#core-feature-2-chat-with-the-fine-tuned-model-base--lora-adapter">Core Feature 2: Chat with the fine-tuned model (base + LoRA adapter)</a></li>
<li><a href="#core-feature-3-batch-inference--metrics-base--lora-adapter">Core Feature 3: Batch inference + metrics (base + LoRA adapter)</a></li>
</ul>
</li>
<li><a href="#kt-fine-tuning-speed-user-side-view">KT Fine-Tuning Speed (User-Side View)</a>
<ul>
<li><a href="#end-to-end-performance">End-to-End Performance</a></li>
<li><a href="#gpucpu-memory-footprint">GPU/CPU Memory Footprint</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h1 id="ktransformers-fine-tuning--llama-factory-integration--user-guide"><a class="header" href="#ktransformers-fine-tuning--llama-factory-integration--user-guide">KTransformers Fine-Tuning √ó LLaMA-Factory Integration ‚Äì User Guide</a></h1>
<p><strong>MadSys Lab, KVCache-AI Team, Approaching AI, LLaMA-Factory Team</strong></p>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p>From <strong>DeepSeek-V3/R1</strong> to <strong>Qwen3-MoE</strong> and <strong>Kimi-K2</strong>, each wave of open-sourced large models brings leaps in performance and scale. However, many researchers and developers are constrained by expensive GPUs and models with tens or even hundreds of billions of parameters, making it <strong>hard to fine-tune very large models under limited resources</strong>. To bridge this gap, we propose a practical approach: combining <strong>KTransformers</strong> with <strong>LLaMA-Factory</strong>. With just <strong>2‚Äì4 RTX 4090s</strong> and a high-memory CPU, you can fine-tune ultra-large MoE models like DeepSeek-671B.</p>
<p>Our goal is to give resource-constrained researchers a <strong>local path to explore fine-tuning ultra-large models</strong>, and also a fast way to customize smaller models (e.g., 14B/30B) for specific scenarios. We validate the setup using <strong>stylized dialogue</strong>, <strong>Westernized translation tone</strong>, and <strong>medical Q&amp;A</strong> as representative tasks, showing that <strong>personalized adaptation can be achieved within hours</strong>.</p>
<p>As shown below, LLaMA-Factory is the unified orchestration/configuration layer for the whole fine-tuning workflow‚Äîhandling data, training scheduling, LoRA injection, and inference interfaces. <strong>KTransformers</strong> acts as a pluggable high-performance backend that takes over core operators like Attention/MoE under the same training configs, enabling efficient <strong>GPU+CPU heterogeneous cooperation</strong>.</p>
<p><img src="assets/image-20251011010558909.png" alt="image-20251011010558909"></p>
<p>Within LLaMA-Factory, we compared LoRA fine-tuning with <strong>HuggingFace</strong>, <strong>Unsloth</strong>, and <strong>KTransformers</strong> backends. KTransformers is the <strong>only workable 4090-class solution</strong> for ultra-large MoE models (e.g., 671B) and also delivers higher throughput and lower GPU memory on smaller MoE models (e.g., DeepSeek-14B).</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Under LoRA (BF16) + <a href="https://github.com/mindsRiverPonder/LLM-practice">NekoQA-10K stylized dialogue</a></th><th>HuggingFace Backend</th><th>Unsloth Backend</th><th>KTransformers Backend</th></tr>
</thead>
<tbody>
<tr><td>[14B-DeepSeekV2-Lite] LoRA fine-tuning throughput</td><td>303.58 token/s</td><td>455.37 token/s</td><td>530.38 token/s</td></tr>
<tr><td>[14B-DeepSeekV2-Lite] GPU memory</td><td>32.12 GB</td><td>9.64 GB</td><td>6.08 GB</td></tr>
<tr><td>[671B-DeepSeekV3] LoRA fine-tuning throughput</td><td><font color="red">Too Huge to run</font></td><td><font color="red">NOT SUPPORT</font></td><td>40.35 token/s</td></tr>
<tr><td>[671B-DeepSeekV3] GPU memory (sum across GPUs)</td><td>theoretical 1400 GB ‚Ä†</td><td><font color="red">NOT SUPPORT</font></td><td>70 GB ‚Ä†</td></tr>
</tbody>
</table>
</div>
<p>‚Ä† <strong>1400 GB</strong> is a <strong>theoretical</strong> FP16 full-parameter resident footprint (not runnable). <strong>70 GB</strong> is the <strong>measured peak</strong> with KT strategy (Attention on GPU + layered MoE offload).</p>
<p><img src="assets/image-compare_model.png" alt="ÊåâÁÖßÊ®°ÂûãÂàíÂàÜÁöÑÂØπÊØîÂõæ_02"></p>
<h3 id="fine-tuning-results-examples"><a class="header" href="#fine-tuning-results-examples">Fine-Tuning Results (Examples)</a></h3>
<h4 id="stylized-dialogue-catgirl-tone"><a class="header" href="#stylized-dialogue-catgirl-tone">Stylized Dialogue (CatGirl tone)</a></h4>
<p>Dataset: <a href="https://zhuanlan.zhihu.com/p/1934983798233231689">NekoQA-10K</a>. Goal: improve style consistency and recognizability.</p>
<p>The figure compares responses from the base vs. fine-tuned models. The fine-tuned model maintains the target tone and address terms more consistently (red boxes), validating the effectiveness of <strong>style-transfer fine-tuning</strong>.</p>
<p><img src="assets/image-20251016175046882.png" alt="image-20251016175046882"></p>
<h4 id="benchmarks"><a class="header" href="#benchmarks">Benchmarks</a></h4>
<p>We use:</p>
<p>(1) <a href="https://github.com/Benson114/Translational-Style-ChatLLM">Translational-Style-ChatLLM</a>, which asks for an exaggerated, Westernized translation tone‚Äîclear, stylized customization.</p>
<p>(2) <a href="https://aclanthology.org/2025.acl-long.96/">AfriMed-QA</a> (ACL 2025), a medical dataset for African contexts with strong domain specificity, including multiple-choice and short-answer sub-tasks‚Äîwell-suited for vertical fine-tuning evaluation.</p>
<p>The tables show metrics before vs. after LoRA fine-tuning. We observe <strong>large improvements</strong> across metrics, verifying fine-tuning effectiveness:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Translational-Style dataset</th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>ROUGE-1</th><th>ROUGE-2</th><th>ROUGE-L</th></tr>
</thead>
<tbody>
<tr><td>V2-Lite (no LoRA)</td><td>20.66</td><td>8.33</td><td>4.54</td><td>2.89</td><td>22.71</td><td>4.52</td><td>19.19</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V2-Lite</strong></td><td><strong>35.41</strong></td><td><strong>22.44</strong></td><td><strong>15.42</strong></td><td><strong>11.18</strong></td><td><strong>42.03</strong></td><td><strong>18.38</strong></td><td><strong>33.10</strong></td></tr>
<tr><td>V3 base (no LoRA)</td><td>8.49</td><td>3.34</td><td>1.62</td><td>0.96</td><td>15.91</td><td>2.55</td><td>10.07</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V3</strong></td><td><strong>37.02</strong></td><td><strong>23.70</strong></td><td><strong>16.21</strong></td><td><strong>11.49</strong></td><td><strong>43.43</strong></td><td><strong>18.96</strong></td><td><strong>34.54</strong></td></tr>
</tbody>
</table>
</div>
<div class="table-wrapper">
<table>
<thead>
<tr><th>AfriMed-QA (short answer)</th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>ROUGE-1</th><th>ROUGE-2</th><th>ROUGE-L</th></tr>
</thead>
<tbody>
<tr><td>V2-Lite (no LoRA)</td><td>13.58</td><td>11.12</td><td>9.10</td><td>7.23</td><td>22.48</td><td>7.81</td><td>11.73</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V2-Lite</strong></td><td><strong>35.90</strong></td><td><strong>27.63</strong></td><td><strong>22.99</strong></td><td><strong>19.15</strong></td><td><strong>35.25</strong></td><td><strong>17.50</strong></td><td><strong>28.44</strong></td></tr>
<tr><td>V3 base (no LoRA)</td><td>12.75</td><td>10.27</td><td>8.05</td><td>5.99</td><td>20.33</td><td>5.65</td><td>10.11</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V3</strong></td><td><strong>42.42</strong></td><td><strong>34.12</strong></td><td><strong>28.95</strong></td><td><strong>24.54</strong></td><td><strong>41.97</strong></td><td><strong>22.37</strong></td><td><strong>33.28</strong></td></tr>
</tbody>
</table>
</div>
<div class="table-wrapper">
<table>
<thead>
<tr><th>AfriMed-QA (multiple choice)</th><th>Accuracy</th></tr>
</thead>
<tbody>
<tr><td>V2-Lite (no LoRA)</td><td>0.0645</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V2-Lite</strong></td><td><strong>0.4812</strong></td></tr>
<tr><td>V3 base (no LoRA)</td><td>0.5833</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V3</strong></td><td><strong>0.7930</strong></td></tr>
</tbody>
</table>
</div>
<p>Even for ultra-large MoE models, <strong>KTransformers-backed fine-tuning</strong> achieves strong task performance quickly.</p>
<h2 id="quick-to-start"><a class="header" href="#quick-to-start">Quick to Start</a></h2>
<p>This section shows how to install and use <strong>LLaMA-Factory + KTransformers</strong> for fine-tuning and inference:</p>
<ul>
<li>Environment setup</li>
<li>Fine-tune ultra-large MoE models with KTransformers backend</li>
<li>Load the fine-tuned model (base + LoRA adapter) for chat/inference</li>
<li>Batch inference and metric evaluation</li>
</ul>
<h3 id="environment-setup"><a class="header" href="#environment-setup">Environment Setup</a></h3>
<p>According to the following example, install both the <strong>KTransformers</strong> and <strong>LLaMA-Factory</strong> environments simultaneously.
This time, to simplify the installation process of KTransformers, we have specially packaged a wheel file to avoid local compilation.
The detailed installation steps are as follows:
(Note: Make sure your local <strong>Python version</strong>, <strong>Torch version</strong>, <strong>CUDA version</strong>, and the <strong>KTransformers wheel filename</strong> correspond correctly.)</p>
<pre><code class="language-shell"># 1. Create a conda environment
conda create -n Kllama python=3.12 # choose from : [3.10, 3.11, 3.12, 3.13]
conda install -y -c conda-forge libstdcxx-ng gcc_impl_linux-64
conda install -y -c nvidia/label/cuda-11.8.0 cuda-runtime

# 2. Install the LLaMA-Factory environment
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation

# 3. Install the KTransformers wheel that matches your Torch and Python versions, from https://github.com/kvcache-ai/ktransformers/releases/tag/v0.4.1 (Note: The CUDA version can differ from that in the wheel filename.)
pip install ktransformers-0.4.1+cu128torch27fancy-cp312-cp312-linux_x86_64.whl

# 4. Install flash-attention, download the corresponding file based on your Python and Torch versions from: https://github.com/Dao-AILab/flash-attention/releases
pip install flash_attn-2.8.3+cu12torch2.7cxx11abiTRUE-cp312-cp312-linux_x86_64.whl
# abi=True/False can find from below
# import torch
# print(torch._C._GLIBCXX_USE_CXX11_ABI)

# 5. (Optional) If you want to use flash_infer (otherwise it defaults to triton)
git clone https://github.com/kvcache-ai/custom_flashinfer.git
pip install custom_flashinfer/
</code></pre>
<p><strong>Usage tip:</strong> In LLaMA-Factory YAML, set <code>use_kt: true</code> and pick a <code>kt_optimize_rule</code> file to have KTransformers handle the core compute. The features below show typical configs.</p>
<h3 id="core-feature-1-use-ktransformers-backend-to-fine-tune-ultra-large-moe-models"><a class="header" href="#core-feature-1-use-ktransformers-backend-to-fine-tune-ultra-large-moe-models">Core Feature 1: Use KTransformers backend to fine-tune ultra-large MoE models</a></h3>
<p>Run the command: <code>USE_KT=1 llamafactory-cli train examples/train_lora/deepseek3_lora_sft_kt.yaml</code>.</p>
<p>Note: You <strong>must</strong> provide a <strong>BF16</strong> model. DeepSeek-V3-671B is released in FP8 by default; convert with <a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/fp8_cast_bf16.py">DeepSeek-V3/inference/fp8_cast_bf16.py</a>.</p>
<pre><code class="language-yaml">### model
model_name_or_path: opensourcerelease/DeepSeek-V3-bf16
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_target: all

### dataset
dataset: identity
template: deepseek
cutoff_len: 2048
max_samples: 100000
overwrite_cache: true
preprocessing_num_workers: 16
dataloader_num_workers: 4

### output
output_dir: saves/Kllama_deepseekV3
logging_steps: 10
save_steps: 500
plot_loss: true
overwrite_output_dir: true
save_only_model: false
report_to: none  # choices: [none, wandb, tensorboard, swanlab, mlflow]

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000
resume_from_checkpoint: null

### ktransformers
use_kt: true # use KTransformers as LoRA sft backend
kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V3-Chat-sft-amx-multi-gpu.yaml
cpu_infer: 32
chunk_size: 8192
</code></pre>
<p><code>kt_optimize_rule</code> controls <strong>placement strategy</strong>. See also <a href="https://github.com/kvcache-ai/ktransformers/tree/main/ktransformers/optimize/optimize_rules">ktransformers/optimize_rules</a>. Naming hints (<code>*</code> = wildcard):</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Pattern</th><th>Meaning</th></tr>
</thead>
<tbody>
<tr><td>DeepSeek-V2-Lite-Chat-* / DeepSeek-V3-Chat-*</td><td>Target model variants</td></tr>
<tr><td><em>-sft-</em></td><td>Strategy for fine-tuning; others are for inference</td></tr>
<tr><td><em>-amx-</em></td><td>Use AMX on CPU; otherwise use <strong>llamafile</strong></td></tr>
<tr><td><em>-multi-gpu-X</em></td><td>Model parallel on X GPUs (X omitted ‚Üí default 2 GPUs)</td></tr>
</tbody>
</table>
</div>
<p>Example: <code>DeepSeek-V3-Chat-sft-amx-multi-gpu.yaml</code> = V3-Chat fine-tuning with AMX and 2-GPU model parallel.</p>
<p>We recommend <strong>AMX acceleration</strong> where available (<code>lscpu | grep amx</code>). AMX supports BF16/INT8. Example:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert parallelism
    kwargs:
      prefill_device: "cpu"
      prefill_op: "KExpertsTorch"
      generate_device: "cpu"
      generate_op: "KSFTExpertsCPU"
      out_device: "cuda"
      backend: "AMXInt8" # or "AMXBF16" or "llamafile" (default)
</code></pre>
<p>Outputs go to <code>output_dir</code> in safetensors format plus adapter metadata for later loading.</p>
<p><img src="assets/image-20251016171537997.png" alt="image-20251016171537997"></p>
<h3 id="core-feature-2-chat-with-the-fine-tuned-model-base--lora-adapter"><a class="header" href="#core-feature-2-chat-with-the-fine-tuned-model-base--lora-adapter">Core Feature 2: Chat with the fine-tuned model (base + LoRA adapter)</a></h3>
<p>Run the command: <code>llamafactory-cli chat examples/inference/deepseek3_lora_sft_kt.yaml</code>.</p>
<p>Use the safetensors adapter trained with KT for inference.</p>
<pre><code class="language-yaml">model_name_or_path: opensourcerelease/DeepSeek-V3-bf16
adapter_name_or_path: saves/Kllama_deepseekV3
template: deepseek
infer_backend: ktransformers  # choices: [huggingface, vllm, sglang, ktransformers]
trust_remote_code: true

use_kt: true # use KTransformers as LoRA sft backend to inference
kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V3-Chat-sft-amx-multi-gpu.yaml
cpu_infer: 32
chunk_size: 8192
</code></pre>
<p>We also support <strong>GGUF</strong> adapters: for safetensors, set the <strong>directory</strong>; for GGUF, set the <strong>file path</strong> in <code>adapter_name_or_path</code>.</p>
<p>During loading, LLaMA-Factory maps layer names to KT‚Äôs naming. You‚Äôll see logs like <code>Loaded adapter weight: XXX -&gt; XXX</code>:</p>
<p><img src="assets/image-20251016171526210.png" alt="image-20251016171526210"></p>
<h3 id="core-feature-3-batch-inference--metrics-base--lora-adapter"><a class="header" href="#core-feature-3-batch-inference--metrics-base--lora-adapter">Core Feature 3: Batch inference + metrics (base + LoRA adapter)</a></h3>
<p>Run the command: <code>API_PORT=8000 llamafactory-cli api examples/inference/deepseek3_lora_sft_kt.yaml</code>.
Invoke the KT fine-tuned adapter to provide the API; the usage logic of other APIs is consistent with the native LLaMA-Factory approach.</p>
<pre><code class="language-yaml">model_name_or_path: opensourcerelease/DeepSeek-V3-bf16
adapter_name_or_path: saves/Kllama_deepseekV3
template: deepseek
infer_backend: ktransformers  # choices: [huggingface, vllm, sglang, ktransformers]
trust_remote_code: true

use_kt: true # use KTransformers as LoRA sft backend to inference
kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V3-Chat-sft-amx-multi-gpu.yaml
cpu_infer: 32
chunk_size: 8192
</code></pre>
<h2 id="kt-fine-tuning-speed-user-side-view"><a class="header" href="#kt-fine-tuning-speed-user-side-view">KT Fine-Tuning Speed (User-Side View)</a></h2>
<h3 id="end-to-end-performance"><a class="header" href="#end-to-end-performance">End-to-End Performance</a></h3>
<p><strong>Definitions</strong></p>
<ul>
<li><code>step_time</code>: wall-clock time for a full optimization step (tensor movement + Attention + MoE + other compute).</li>
<li><code>tokens_per_step = GAS √ó qlen</code>; <code>token/s = tokens_per_step / step_time</code>.</li>
</ul>
<p><strong>Settings:</strong> <code>GAS=16</code>, <code>qlen=512</code> (‚Üí <code>tokens_per_step = 8192</code>); LoRA (<code>r=8, alpha=32, dropout=0.1</code>); <strong>AMX</strong> enabled; GPU: RTX 4090, CPU: Intel Xeon Platinum 8488C.</p>
<p><strong>Measured</strong></p>
<ul>
<li><strong>DeepSeek-V3-671B:</strong> <code>step_time = 203 s</code> ‚Üí <code>token/s ‚âà 8192 / 203 ‚âà 40.35</code></li>
<li><strong>DeepSeek-V2-Lite-14B:</strong> <code>step_time = 36 s</code> ‚Üí <code>token/s ‚âà 8192 / 36 ‚âà 227.6</code></li>
</ul>
<h3 id="gpucpu-memory-footprint"><a class="header" href="#gpucpu-memory-footprint">GPU/CPU Memory Footprint</a></h3>
<ul>
<li>DeepSeek-V3 (671B; 61 layers with 58 MoE): ~<strong>70 GB</strong> total GPU VRAM (multi-GPU), ~<strong>1.2‚Äì1.3 TB</strong> CPU RAM.</li>
<li>DeepSeek-V2-Lite (14B; 27 layers with 26 MoE): ~<strong>5.5 GB</strong> GPU VRAM, ~<strong>30 GB</strong> CPU RAM.</li>
</ul>
<h2 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h2>
<p>By integrating <strong>KTransformers LoRA fine-tuning</strong> into <strong>LLaMA-Factory</strong>, we provide a practical guide for efficient training and deployment of MoE LLMs. KT brings cutting-edge optimizations (DeepSeek/Qwen/Kimi support with AMX-accelerated kernels), and LoRA enables customization under very low GPU memory. LLaMA-Factory offers a friendly, unified interface.</p>
<p>This integration (akin to Unsloth-style speedups) means even models with tens to hundreds of billions of parameters can be fine-tuned and deployed with low latency on commodity hardware. You get <strong>memory savings, speed-ups, and usability</strong> together. We encourage you to try LLaMA-Factory + KT for your next MoE project and follow this guide. Feedback is welcome!</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kt-sft-docs"><a class="header" href="#kt-sft-docs">kt-sft Docs</a></h1>
<div style="break-before: page; page-break-before: always;"></div>
<ul>
<li><a href="#introduction-1">Introduction</a></li>
<li><a href="#overall-view-of-the-kt-fine-tuning-framework">Overall View of the KT Fine-Tuning Framework</a>
<ul>
<li><a href="#attention-lora--kt-coexist">Attention (LoRA + KT coexist)</a></li>
<li><a href="#moe-operator-encapsulation--backward">MoE (operator encapsulation + backward)</a></li>
<li><a href="#multi-gpu-loadingtraining-placement-strategy-instead-of-dataparallel">Multi-GPU Loading/Training: Placement strategy instead of DataParallel</a></li>
</ul>
</li>
<li><a href="#kt-lora-fine-tuning-evaluation">KT-LoRA Fine-Tuning Evaluation</a>
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#speed-tests">Speed Tests</a></li>
<li><a href="#memory-footprint">Memory Footprint</a></li>
</ul>
</li>
<li><a href="#conclusion-1">Conclusion</a></li>
</ul>
<h1 id="ktransformers-fine-tuning--llama-factory-integration--developer-technical-notes"><a class="header" href="#ktransformers-fine-tuning--llama-factory-integration--developer-technical-notes">KTransformers Fine-Tuning √ó LLaMA-Factory Integration ‚Äì Developer Technical Notes</a></h1>
<p><strong>MadSys Lab, KVCache-AI Team, Approaching AI, LLaMA-Factory Team</strong></p>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>Recent open-source LLMs‚Äîfrom DeepSeek-V3/R1 to Qwen-MoE and Kimi-K2‚Äîhave surged in performance and scale. Yet due to <strong>compute and memory constraints</strong>, it is difficult for typical researchers to fine-tune trillion-parameter-class models. We therefore integrate <strong>KTransformers</strong> with <strong>LLaMA-Factory</strong> so that, with <strong>2‚Äì4 RTX 4090 GPUs</strong> and sufficient CPU memory, one can fine-tune ultra-large Mixture-of-Experts (MoE) models such as DeepSeek-671B.</p>
<p>This architecture bridges resource gaps, enabling <strong>local fine-tuning of ultra-large models</strong>, while also supporting <strong>efficient scenario customization</strong> at 14B/30B scales. We validate on stylized dialogue, Westernized translation tone, and medical Q&amp;A, achieving rapid adaptation within hours.</p>
<p>Architecturally, LLaMA-Factory orchestrates data/config/training, LoRA insertion, and inference; KTransformers is a pluggable, high-performance operator backend that takes over Attention and MoE under the same training code, enabling <strong>GPU+CPU heterogeneity</strong> to accelerate training and reduce GPU memory.</p>
<p><img src="assets/image-20251011010558909.png" alt="image-20251011010558909"></p>
<p>We evaluated LoRA fine-tuning with HuggingFace default, Unsloth, and KTransformers backends (same settings and data). <strong>KTransformers</strong> is currently the only solution feasible on <strong>2‚Äì4√ó24GB 4090s</strong> for <strong>671B-scale MoE</strong>, and also shows higher throughput and lower GPU memory for 14B MoEs.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Under LoRA (BF16) + <a href="https://github.com/mindsRiverPonder/LLM-practice">NekoQA-10K stylized dialogue</a></th><th>HuggingFace Backend</th><th>Unsloth Backend</th><th>KTransformers Backend</th></tr>
</thead>
<tbody>
<tr><td>[14B-DeepSeekV2-Lite] LoRA fine-tuning throughput</td><td>303.58 token/s</td><td>455.37 token/s</td><td>530.38 token/s</td></tr>
<tr><td>[14B-DeepSeekV2-Lite] GPU memory</td><td>32.12 GB</td><td>9.64 GB</td><td>6.08 GB</td></tr>
<tr><td>[671B-DeepSeekV3] LoRA fine-tuning throughput</td><td><font color="red">Too Huge to run</font></td><td><font color="red">NOT SUPPORT</font></td><td>40.35 token/s</td></tr>
<tr><td>[671B-DeepSeekV3] GPU memory (sum across GPUs)</td><td>theoretical 1400 GB ‚Ä†</td><td><font color="red">NOT SUPPORT</font></td><td>70 GB ‚Ä†</td></tr>
</tbody>
</table>
</div>
<p>‚Ä† The <strong>1400 GB</strong> is the <strong>theoretical</strong> FP16 full-resident footprint (not runnable). <strong>70 GB</strong> is the <strong>measured peak</strong> with KT (Attention on GPU + layered MoE offload).</p>
<p>From the table above, it can be seen that for the 14B model, the KTransformers backend achieves approximately 75% higher throughput than the default HuggingFace solution, while using only about one-fifth of the GPU memory. For the 671B model, both HuggingFace and Unsloth fail to run on a single 4090 GPU, whereas KTransformers is able to perform LoRA fine-tuning at 40 tokens/s, keeping the GPU memory usage within 70 GB.</p>
<p><img src="assets/image-compare_model.png" alt="ÊåâÁÖßÊ®°ÂûãÂàíÂàÜÁöÑÂØπÊØîÂõæ_02"></p>
<h2 id="overall-view-of-the-kt-fine-tuning-framework"><a class="header" href="#overall-view-of-the-kt-fine-tuning-framework">Overall View of the KT Fine-Tuning Framework</a></h2>
<p>We detail how KTransformers takes over core operators in LLaMA-Factory‚Äôs fine-tuning framework to optimize Attention and MoE.</p>
<p>DeepSeek-V3/V2 MoE models comprise a small-parameter dense Attention part and a large-parameter sparse MoE part. For illustration, consider layer 2 of DeepSeek-V2-Lite-Chat (from which each layer includes both Attention and MoE). Attention compute and KV cache mainly reside on the GPU; the heavyweight MoE part is primarily executed on the CPU. We first cover <strong>Attention replacement and inheritance</strong>, then <strong>MoE encapsulation and backend interfacing</strong>, and finally <strong>multi-GPU placement</strong>.</p>
<h3 id="attention-lora--kt-coexist"><a class="header" href="#attention-lora--kt-coexist">Attention (LoRA + KT coexist)</a></h3>
<p>KTransformers provides operator injection (<code>BaseInjectedModule</code>), and PEFT provides LoRA layer insertion. For fine-tuning, we design <code>KTransformersLinearLora</code>, inheriting from both <code>KTransformersLinear</code> and <code>LoraLayer</code>:</p>
<ul>
<li><strong>Inheritance:</strong> <code>KTransformersLinearLora</code> retains KT‚Äôs high-performance paths (<code>prefill_linear</code>/<code>generate_linear</code>) while accepting LoRA parameters (<code>lora_A/lora_B</code>).</li>
<li><strong>Replacement:</strong> During preparation, we replace original <code>KTransformersLinear</code> layers (Q/K/V/O) with <code>KTransformersLinearLora</code>, preserving KT optimizations while enabling LoRA trainability.</li>
</ul>
<p><img src="assets/image-20251016182810716.png" alt="image-20251016182810716"></p>
<p>After replacement, LoRA is inserted at Q/K/V/O linear transforms (left), and <code>KTransformersLinearLora</code> contains both KT fast paths and LoRA matrices (right).</p>
<p><img src="assets/image-20251016182920722.png" alt="image-20251016182920722"></p>
<h3 id="moe-operator-encapsulation--backward"><a class="header" href="#moe-operator-encapsulation--backward">MoE (operator encapsulation + backward)</a></h3>
<h4 id="encapsulation"><a class="header" href="#encapsulation">Encapsulation</a></h4>
<p>Given large parameters and sparse compute, we encapsulate the expert computation as a <strong>differentiable black-box operator</strong>‚Äîtransparent upstream, replaceable downstream.</p>
<ul>
<li><strong>Upstream (PyTorch graph):</strong> we register a custom Autograd Function so the MoE layer appears as <strong>a single node</strong>. In the left figure (red box), only <code>KSFTExpertsCPU</code> is visible; on the right, the unencapsulated graph expands routing, dispatch, and FFN experts. Encapsulation makes the MoE layer behave like a standard <code>nn.Module</code> with gradients.</li>
<li><strong>Downstream (backend):</strong> inside the Autograd Function, pybind11 calls C++ extensions for forward/backward. Multiple <strong>pluggable backends</strong> exist (AMX BF16/INT8; <strong>llamafile</strong>). The backend can be switched via YAML (e.g., <code>"backend": "AMXBF16"</code> vs. <code>"llamafile"</code>).</li>
</ul>
<p><img src="assets/image-20250801174623919.png" alt="image-20250801174623919"></p>
<h4 id="backward-cpu"><a class="header" href="#backward-cpu">Backward (CPU)</a></h4>
<p>MoE backward frequently needs the transposed weights $W^\top$. To avoid repeated runtime transposes, we <strong>precompute/cache</strong> $W^\top$ at load time (blue box). We also <strong>cache necessary intermediate activations</strong> (e.g., expert projections, red box) to reuse in backward and reduce recomputation. We provide backward implementations for <strong>llamafile</strong> and <strong>AMX (INT8/BF16)</strong>, with NUMA-aware optimizations.</p>
<img src="assets/image-20251016182942726.png" alt="image-20251016182942726" style="zoom:33%;" />
<h3 id="multi-gpu-loadingtraining-placement-strategy-instead-of-dataparallel"><a class="header" href="#multi-gpu-loadingtraining-placement-strategy-instead-of-dataparallel">Multi-GPU Loading/Training: Placement strategy instead of DataParallel</a></h3>
<p>To lower <strong>per-GPU memory peaks</strong> on 2‚Äì4 GPUs, we use <strong>model parallelism + explicit placement</strong>, not DataParallel (which duplicates the whole model on each GPU).</p>
<p>Key changes:</p>
<ol>
<li><strong>KTrainer:</strong> takes over <code>.to(device)</code> to prevent ‚Äúmove whole model to a single GPU‚Äù. Using KT‚Äôs optimize-rule YAML, each layer declares <code>device: cuda:0/cuda:1/...</code> and is <strong>constructed directly on the target GPU</strong> (no extra copies).</li>
<li><strong>Disable automatic DataParallel:</strong> when <code>USE_KT=1</code>, we disable automatic DP wrappers from LLaMA-Factory/HF Trainer to avoid duplication and keep full control over sharding.</li>
<li><strong>Gradient aggregation:</strong> gradients are reduced to <code>cuda:0</code>. Intermediate activations stay local; only necessary tensors are transferred, cutting communication/activation overhead.</li>
</ol>
<p>Thus, we keep KT placement strategies under multi-GPU fine-tuning. Users choose a <code>kt_optimize_rule</code> with <code>multi-gpu</code>. For DeepSeek-671B, <code>DeepSeek-V3-Chat-sft-amx-multi-gpu.yaml</code> is a typical 2-GPU plan: KV/attention parts on each GPU; MoE experts sharded on CPU; both GPUs share the workload.</p>
<h2 id="kt-lora-fine-tuning-evaluation"><a class="header" href="#kt-lora-fine-tuning-evaluation">KT-LoRA Fine-Tuning Evaluation</a></h2>
<h3 id="setup"><a class="header" href="#setup">Setup</a></h3>
<p>LLaMA-Factory orchestration, KTransformers backend, LoRA (rank=8, Œ±=32, dropout=0.1, BF16), <code>GAS=16</code>, <code>qlen=512</code>, with the same KT optimize rule as training. We evaluate (a) stylized dialogue transfer and (b) two <strong>small-scale representative</strong> benchmarks: Translational-Style (generative) and AfriMed-QA (medical vertical; <strong>SAQ</strong> and <strong>MCQ</strong>). AMX is enabled; GPUs: 2√ó48GB RTX 4090; CPU: Intel Xeon Platinum 8488C.</p>
<h3 id="results"><a class="header" href="#results">Results</a></h3>
<h4 id="stylized-dialogue-catgirl-tone-1"><a class="header" href="#stylized-dialogue-catgirl-tone-1">Stylized Dialogue (CatGirl tone)</a></h4>
<p>Dataset: <a href="https://zhuanlan.zhihu.com/p/1934983798233231689">NekoQA-10K</a>. The fine-tuned model consistently exhibits the target style (red boxes) versus neutral/rational base (blue). This shows <strong>KT-LoRA injects style features</strong> into the generation distribution with low GPU cost.</p>
<p><img src="assets/image-20251016175848143.png" alt="image-20251016175848143"></p>
<h4 id="translational-style-benchmark-generative"><a class="header" href="#translational-style-benchmark-generative">Translational-Style benchmark (generative)</a></h4>
<p>Dataset: <a href="https://github.com/Benson114/Translational-Style-ChatLLM">Translational-Style-ChatLLM</a>. Metrics: BLEU-1/2/3/4, ROUGE-1/2/L.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Translational-Style dataset</th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>ROUGE-1</th><th>ROUGE-2</th><th>ROUGE-L</th></tr>
</thead>
<tbody>
<tr><td>V2-Lite (no LoRA)</td><td>20.66</td><td>8.33</td><td>4.54</td><td>2.89</td><td>22.71</td><td>4.52</td><td>19.19</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V2-Lite</strong></td><td><strong>35.41</strong></td><td><strong>22.44</strong></td><td><strong>15.42</strong></td><td><strong>11.18</strong></td><td><strong>42.03</strong></td><td><strong>18.38</strong></td><td><strong>33.10</strong></td></tr>
<tr><td>V3 base (no LoRA)</td><td>8.49</td><td>3.34</td><td>1.62</td><td>0.96</td><td>15.91</td><td>2.55</td><td>10.07</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V3</strong></td><td><strong>37.02</strong></td><td><strong>23.70</strong></td><td><strong>16.21</strong></td><td><strong>11.49</strong></td><td><strong>43.43</strong></td><td><strong>18.96</strong></td><td><strong>34.54</strong></td></tr>
</tbody>
</table>
</div>
<p>As shown by the test results in the tables above, under a unified workflow and placement strategy, <strong>both model scales exhibit consistent gains after fine-tuning</strong>, supporting the usability and effectiveness of the ‚ÄúKT backend + LoRA fine-tuning‚Äù combination for generative style control. At the same time, this indicates that KT‚Äôs heterogeneous placement and operator optimizations can stably support small-sample adaptation in the style domain.</p>
<h4 id="medical-vertical-benchmark-afrimed-saqmcq"><a class="header" href="#medical-vertical-benchmark-afrimed-saqmcq">Medical Vertical Benchmark (AfriMed-SAQ/MCQ)</a></h4>
<p>The dataset adopts <a href="https://aclanthology.org/2025.acl-long.96/">AfriMed-QA</a> (ACL 2025), a domain-specific dataset for the medical field in Africa with strong scenario customization characteristics, comprising two formats‚Äîmultiple-choice questions (MCQ) and short-answer questions (SAQ)‚Äîwhich in this case serve as the evaluation for vertical-domain fine-tuning. In terms of evaluation criteria, BLEU/ROUGE are used for SAQ, and Accuracy is used for MCQ.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>AfriMed-QA (SAQ)</th><th>BLEU-1</th><th>BLEU-2</th><th>BLEU-3</th><th>BLEU-4</th><th>ROUGE-1</th><th>ROUGE-2</th><th>ROUGE-L</th></tr>
</thead>
<tbody>
<tr><td>V2-Lite (no LoRA)</td><td>13.58</td><td>11.12</td><td>9.10</td><td>7.23</td><td>22.48</td><td>7.81</td><td>11.73</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V2-Lite</strong></td><td><strong>35.90</strong></td><td><strong>27.63</strong></td><td><strong>22.99</strong></td><td><strong>19.15</strong></td><td><strong>35.25</strong></td><td><strong>17.50</strong></td><td><strong>28.44</strong></td></tr>
<tr><td>V3 base (no LoRA)</td><td>12.75</td><td>10.27</td><td>8.05</td><td>5.99</td><td>20.33</td><td>5.65</td><td>10.11</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V3</strong></td><td><strong>42.42</strong></td><td><strong>34.12</strong></td><td><strong>28.95</strong></td><td><strong>24.54</strong></td><td><strong>41.97</strong></td><td><strong>22.37</strong></td><td><strong>33.28</strong></td></tr>
</tbody>
</table>
</div>
<div class="table-wrapper">
<table>
<thead>
<tr><th>AfriMed-QA (MCQ)</th><th>Accuracy</th></tr>
</thead>
<tbody>
<tr><td>V2-Lite (no LoRA)</td><td>0.0645</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V2-Lite</strong></td><td><strong>0.4812</strong></td></tr>
<tr><td>V3 base (no LoRA)</td><td>0.5833</td></tr>
<tr><td><strong>KT-LoRA fine-tuned V3</strong></td><td><strong>0.7930</strong></td></tr>
</tbody>
</table>
</div>
<p>As shown in the tables above, (1) DeepSeek-V3 (671B) after KT-LoRA fine-tuning achieves clearly higher performance than the fine-tuned DeepSeek-V2-Lite (14B) on both MCQ and SAQ, and it also surpasses the V3 base model. Within our small-scale setting, this preliminarily indicates that KT-LoRA fine-tuning of ultra-large-parameter models has practical significance in vertical domains.</p>
<p>(2) Across both SAQ/MCQ sub-tasks, KT-LoRA delivers consistent gains, indicating that‚Äîwith KT‚Äôs heterogeneous placement and backend operator support‚ÄîLoRA fine-tuning can effectively inject the key knowledge points of vertical domains such as medicine into the model.</p>
<h4 id="limitations"><a class="header" href="#limitations">Limitations</a></h4>
<p>At present, most of our testing is conducted on <strong>single datasets</strong> and at <strong>small scale</strong> (‚â§ 20k examples), with the goal of providing <strong>existence evidence of system effectiveness for KT-LoRA fine-tuning</strong>, rather than drawing generalized conclusions about algorithmic generalization or scaling laws. Our report primarily presents representative figures; to support stronger algorithmic claims, larger sample sizes, multi-lingual/multi-domain datasets, and multi-seed repeated experiments would be required‚Äîthese are beyond the scope of this work.</p>
<p><strong>We also warmly welcome everyone to join the open-source LLaMA-Factory KT fine-tuning project. If you have additional test results, we especially welcome you to record them in the shared spreadsheet below, and to include the corresponding <code>kt_optimize_rule</code> files, dataset examples, training/evaluation YAMLs, and detailed GPU-memory and CPU configurations for community reference and reproducibility~!</strong></p>
<h3 id="speed-tests"><a class="header" href="#speed-tests">Speed Tests</a></h3>
<h4 id="end-to-end-performance-1"><a class="header" href="#end-to-end-performance-1">End-to-End Performance</a></h4>
<p><strong>Definitions</strong></p>
<p><code>step_time</code>Ôºötime per optimization step (tensor movement + Attention + MoE + others).</p>
<p><code>tokens_per_step = GAS √ó qlen</code>Ôºõ<code>token/s = tokens_per_step / step_time</code>„ÄÇ We use <code>GAS=16</code>, <code>qlen=512</code> ‚Üí <code>tokens_per_step=8192</code>.</p>
<p><strong>Measured</strong></p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>Model</th><th>step_time (s)</th><th>tokens/step</th><th>token/s</th></tr>
</thead>
<tbody>
<tr><td>DeepSeek-V3-671B</td><td>203</td><td>8192</td><td><strong>40.35</strong></td></tr>
<tr><td>DeepSeek-V2-Lite-14B</td><td>36</td><td>8192</td><td><strong>227.6</strong></td></tr>
</tbody>
</table>
</div>
<h4 id="moe-compute-deepseek-v3-671b"><a class="header" href="#moe-compute-deepseek-v3-671b">MoE Compute (DeepSeek-V3-671B)</a></h4>
<p><strong>Theory</strong></p>
<ul>
<li>MoE per-layer, per-token FLOPs (forward+backward) approx.:
$$
\text{FLOPs}_{\text{per-layer, per-token}} \approx c \cdot k \cdot H \cdot I
$$</li>
</ul>
<p>‚Äã		with $k = 8$ÔºàTop-kÔºâÔºå$H = 7168$Ôºàhidden sizeÔºâÔºå$I = 2048$Ôºàintermediate sizeÔºâÔºå$c\approx16$Ôºà‚âà6 forward + ‚âà10 backward matmulsÔºâ„ÄÇ</p>
<ul>
<li>Per-step across all MoE layers:
$$
\text{FLOPs}<em>{\text{per-step}} \approx c \cdot qlen \cdot k \cdot H \cdot I \cdot L</em>{\text{MoE}}
$$</li>
</ul>
<p>‚Äã		Plugging $c=16, qlen=512, k=8, H=7168, I=2048, L_{MoE}=58$Ôºå$\text{FLOPs}_{\text{per-step}} \approx 55.8\ \text{TFLOPs}$.</p>
<p><strong>Measured (MoE TFLOPS on CPU)</strong></p>
<p>If the <strong>MoE-only</strong> time per step is <code>t_moe</code> (seconds), $\text{TFLOPS} = \text{FLOPs}_{\text{per-step}} / \text{step_per_second}.$</p>
<p>Use MoE-phase time, not full <code>step_time</code>, to get MoE throughput.</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>TFLOPS</th><th>Forward</th><th>Backward</th></tr>
</thead>
<tbody>
<tr><td>Average</td><td>17.55</td><td>18.41</td></tr>
</tbody>
</table>
</div>
<h3 id="memory-footprint"><a class="header" href="#memory-footprint">Memory Footprint</a></h3>
<ul>
<li>DeepSeek-V3 (671B; 58 MoE layers out of 61): ~<strong>70 GB</strong> total GPU, ~<strong>1.2‚Äì1.3 TB</strong> host memory.</li>
<li>DeepSeek-V2-Lite (14B; 26 MoE layers out of 27): ~<strong>5 GB</strong> GPU, ~<strong>30 GB</strong> host memory.</li>
</ul>
<h2 id="conclusion-1"><a class="header" href="#conclusion-1">Conclusion</a></h2>
<p>Integrating <strong>KTransformers LoRA</strong> with <strong>LLaMA-Factory</strong> provides a practical path to efficiently train and deploy MoE LLMs. KT contributes placement strategies and operator optimizations (DeepSeek/Qwen/Kimi support with AMX-accelerated kernels), and LoRA enables customization with very low GPU memory; LLaMA-Factory supplies a coherent user-level interface.</p>
<p>This means even tens-to-hundreds-of-billion-parameter MoE models can be fine-tuned and served with low latency on ordinary hardware. The approach balances <strong>memory savings</strong>, <strong>speed</strong>, and <strong>usability</strong>, turning ultra-large models into tools that developers can actually wield.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="tutorial-inject-operator-step-by-step"><a class="header" href="#tutorial-inject-operator-step-by-step">Tutorial: Inject Operator Step by Step</a></h1>
<blockquote>
<p>Author: Azure-Tang</p>
</blockquote>
<h2 id="tldr"><a class="header" href="#tldr">TL;DR</a></h2>
<p>This tutorial will guide you through the process of injecting custom operators into a model using the KTransformers framework. We will use the DeepSeekV2-Chat model as an example to demonstrate how to inject custom operators into the model step by step. The tutorial will cover the following topics:</p>
<ul>
<li><a href="#tldr">TL;DR</a></li>
<li><a href="#how-to-write-injection-rules">How to Write Injection Rules</a></li>
<li><a href="#understanding-model-structure">Understanding Model Structure</a></li>
<li><a href="#matrix-absorption-based-mla-injection">Matrix Absorption-based MLA Injection</a></li>
<li><a href="#injection-of-routed-experts">Injection of Routed Experts</a></li>
<li><a href="#injection-of-linear-layers">Injection of Linear Layers</a></li>
<li><a href="#injection-of-modules-with-pre-calculated-buffers">Injection of Modules with Pre-calculated Buffers</a></li>
<li><a href="#specifying-running-devices-for-modules">Specifying Running Devices for Modules</a></li>
<li><a href="#muti-gpu">Muti-GPU</a></li>
<li><a href="#how-to-write-a-new-operator-and-inject-into-the-model">How to Write a New Operator and Inject into the Model</a></li>
</ul>
<h2 id="how-to-write-injection-rules"><a class="header" href="#how-to-write-injection-rules">How to Write Injection Rules</a></h2>
<p>The basic form of the injection rules for the Inject framework is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.*$"  # Target module name
    class: torch.nn.Linear  # Target module
  replace:
    class: "default"
    kwargs:
      generate_device: "cuda:0"
      # your_op_param_1: 1234
      # your_op_param_2: 5678
  recursive: True
</code></pre>
<ul>
<li>match: This field marks the matching rules, which can appear in two forms, name and class. These two matching rules can appear together or separately; they only match when both criteria are met.</li>
<li>replace:
<ul>
<li>class: Python class that can be imported to replace the target module. If no replacement is desired, set to default.</li>
<li>kwargs: List of parameters needed for module initialization.
<ul>
<li>generate_device: The device for this module, can be set to ‚Äúcpu‚Äù, ‚Äúcuda‚Äù, ‚Äúcuda:1‚Äù, etc.</li>
</ul>
</li>
</ul>
</li>
<li>recursive: Whether to recursively inject this module‚Äôs submodules, default is True.</li>
</ul>
<p>For the recursive field: Some modules contain multiple submodules, such as the Self-attention module typically includes q/k/v/o four linear modules. If we replace the self-attention module but do not want the internal linear modules to be covered by other rules, set this rule to False.</p>
<h2 id="understanding-model-structure"><a class="header" href="#understanding-model-structure">Understanding Model Structure</a></h2>
<p>Using <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat">deepseek-ai/DeepSeek-V2-Lite-Chat</a> as an example, we can follow the above rules step by step to inject our custom module and run it. KTransformers offers a high degree of flexibility, allowing you to replace/experiment with basic operators. However, it also requires users to clearly understand the structure of the model they are running.</p>
<p>Fortunately, knowing the structure of a model is very simple. Open the file list on the <a href="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat/tree/main">deepseek-ai/DeepSeek-V2-Lite</a> homepage, and you can see the following files:</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="assets/model_structure_guild.png" width="60%">
  </picture>
</p>

<p>From the <code>.saftensors</code> file, we can see the name of each layer‚Äôs weights, corresponding to the match.name attribute in the injection rules.
From the <code>modeling_deepseek.py</code> file, we can see the specific implementation of each module class, with the class name corresponding to the match.class attribute in the injection rules.</p>
<p>The structure of the DeepSeekV2 model from the <code>.saftensors</code> and <code>modeling_deepseek.py</code> files is as follows:</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="assets/deepseekv2_structure.png" width="60%">
  </picture>
</p>

<p>Supported operators and their corresponding classes are as follows:</p>
<div class="table-wrapper">
<table>
<thead>
<tr><th>match</th><th>replace</th><th>backends</th><th>descriptions</th></tr>
</thead>
<tbody>
<tr><td>Linear</td><td>KTransformersLinear</td><td>KLinearMarlin</td><td>Marlin as backend</td></tr>
<tr><td></td><td></td><td>KLinearTorch</td><td>pytorch as backend</td></tr>
<tr><td></td><td></td><td>KLinearCPUInfer</td><td>llamafile as backend</td></tr>
<tr><td></td><td></td><td>KLinearFP8</td><td>Triton fp8_gemm kernel. Requires GPU be able to caluculate fp8 data</td></tr>
<tr><td>experts</td><td>KTransformersExperts</td><td>KExpertsTorch</td><td>pytorch as backend</td></tr>
<tr><td></td><td></td><td>KExpertsMarlin</td><td>Marlin as backend</td></tr>
<tr><td></td><td></td><td>KExpertsCPU</td><td>llamafile as backend</td></tr>
<tr><td>Attention</td><td>KDeepseekV2Attention</td><td>KDeepseekV2Attention</td><td>MLA implementation</td></tr>
<tr><td>MoE</td><td>KMistralSparseMoEBlock</td><td>KQwen2MoeSparseMoeBlock</td><td>MoE for Qwen2</td></tr>
<tr><td></td><td>KDeepseekV2MoE</td><td>KDeepseekV2MoE</td><td>MoE for DeepseekV2</td></tr>
<tr><td>Model</td><td>KQwen2MoeModel</td><td>KQwen2MoeModel</td><td>Model for Qwen2</td></tr>
<tr><td></td><td>KDeepseekV2Model</td><td>KDeepseekV2Model</td><td>Model for DeepseekV2</td></tr>
<tr><td>RoPE</td><td>RotaryEmbedding</td><td>RotaryEmbedding</td><td>RoPE module</td></tr>
<tr><td></td><td>YarnRotaryEmbedding</td><td>YarnRotaryEmbedding</td><td>RoPE module</td></tr>
</tbody>
</table>
</div>
<p>Then we start step-by-step injection of custom modules, our targets are:</p>
<ul>
<li>Replace the linear module with custom Marlin linear module.</li>
<li>Replace the self-attention module with a custom Absorption-based MLA module.</li>
<li>Replace the experts module with a custom Experts module.</li>
<li>Replace the MoE module with a custom MoE module.</li>
<li>Replace the RoPE module with a custom RoPE module.</li>
<li>Set the running device for each module.</li>
</ul>
<p>The full implementation of the injection rules can be found in the <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat.yaml">here</a>.</p>
<h2 id="matrix-absorption-based-mla-injection"><a class="header" href="#matrix-absorption-based-mla-injection">Matrix Absorption-based MLA Injection</a></h2>
<p>For the injection of the Attention module, we only need to use a regular expression to match the module names used in transformers and replace them with our own MLA module implementation. The YAML injection rule is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.self_attn$"  # Regular expression
  replace:
    class: ktransformers.operators.attention.KDeepseekV2Attention # Optimized MLA implementation
</code></pre>
<p>As you can see, each rule in the YAML file has two parts: match and replace. The match part specifies the module to be replaced, and the replace part specifies the module to be injected into the model along with the initialization keywords.</p>
<h2 id="injection-of-routed-experts"><a class="header" href="#injection-of-routed-experts">Injection of Routed Experts</a></h2>
<p>For Routed Experts (corresponding to the exps in the diagram), the module we inject is CPUInfer, which is wrapped in the wrapper module KTransformersExperts. KTransformersExperts has multiple implementations, and we need to specify keywords to tell the wrapper module which implementation we want to use and how we plan to use it.</p>
<p>In the source code of the transformer, MoE is implemented using nn.ModuleList. We do not want KTransformers to traverse all submodules in the list and inject them one by one, so in this rule, we set recursive: False to prevent recursive injection into the submodules of this module. The YAML rule is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # Custom MoE kernel with expert parallelism
    kwargs:
      generate_device: "cpu"
      generate_op: "MLPCPUExperts"
      out_device: "cuda"
  recursive: False # Don't recursively inject submodules of this module
</code></pre>
<p>If we inject Routed Experts as a custom module, we cannot use the interfaces in the original <code>nn.ModuleList</code>. Therefore, it is necessary to modify the forward function in the FFN module. The simplest method is to implement a new module with a custom forward function and inject it.</p>
<pre><code class="language-yaml">- match:
    class: ktransformers.models.modeling_deepseek.DeepseekV2MoE
  replace:
    class: ktransformers.operators.experts.KDeepseekV2MoE     # MLP module with custom forward function
</code></pre>
<h2 id="injection-of-linear-layers"><a class="header" href="#injection-of-linear-layers">Injection of Linear Layers</a></h2>
<p>For the remaining linear layer modules, we aim to use quantized operators to save storage space while improving performance. Since there is no current research on using MLA and quantization together, we do not want to inject linear into the MLA operator. Therefore, we can modify the regular expression and add a type check in the match part of the rule. Only modules that match both the name and class simultaneously will be injected. We also need to pass some keywords similar to the injection of Routed Experts. The YAML rule is as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\.(?!.*self_attn).*$"  # Regular expression
    class: torch.nn.Linear  # Only match modules matching name and class simultaneously
  replace:
    class: ktransformers.operators.linear.KTransformersLinear  # Optimized kernel on quantized data types
    kwargs:
      generate_device: "cuda"
      generate_op: "QuantizedLinearMarlin"
</code></pre>
<h2 id="injection-of-modules-with-pre-calculated-buffers"><a class="header" href="#injection-of-modules-with-pre-calculated-buffers">Injection of Modules with Pre-calculated Buffers</a></h2>
<p>To avoid occupying resources when initializing the injected original model, we use torch‚Äôs meta device to initialize the original model. The RoPE module pre-calculates some buffers during initialization, but no calculations are performed when using the meta device. Therefore, we need to compensate for the calculation of the buffer when loading the model. Simply, we inject a custom module into the rotary embedding module, which performs pre-calculation during loading. The YAML rule is as follows:</p>
<pre><code class="language-yaml">- match:
    class: ktransformers.models.modeling_deepseek.DeepseekV2YarnRotaryEmbedding
  replace:
    class: ktransformers.operators.RoPE.YarnRotaryEmbedding
</code></pre>
<h2 id="specifying-running-devices-for-modules"><a class="header" href="#specifying-running-devices-for-modules">Specifying Running Devices for Modules</a></h2>
<p>Finally, we set a fallback basic attribute generate_device for all modules:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.|^lm_head"
  replace:
    class: "default"
    kwargs:
      generate_device: "cuda"
  
- match:
    name: "^model.embed_tokens"
  replace:
    class: "default"
    kwargs:
        generate_device: "cpu"
</code></pre>
<p>Through these two rules, we place all previously unmatched layers (and their submodules) and lm_head on cuda, and the embedding on cpu. Note that the properties of a module will be determined by the first rule it matches. For example, if you later set a new replace.kwargs.generate_device in an injected module, the device set earlier will take precedence. If your computer has multiple cards, you can also configure the model to multiple cards.</p>
<h2 id="muti-gpu"><a class="header" href="#muti-gpu">Muti-GPU</a></h2>
<p>If you have multiple GPUs, you can set the device for each module to different GPUs.
DeepseekV2-Chat got 60 layers, if we got 2 GPUs, we can allocate 30 layers to each GPU. Complete multi GPU rule examples <a href="https://github.com/kvcache-ai/ktransformers/blob/main/ktransformers/optimize/optimize_rules/DeepSeek-V2-Chat-multi-gpu.yaml">here</a>.</p>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="assets/multi_gpu.png" width="60%">
  </picture>
</p>

<p>First of all, for multi-GPU, we have to inject an new operator <code>KDeepseekV2Model</code>. And set division of the layers to different GPUs. For our case, we have to set the <code>transfer_map</code> in the <code>KDeepseekV2Model</code> operatoras as follows:</p>
<pre><code class="language-yaml">- match:
    name: "^model$"
  replace:
    class: "ktransformers.operators.models.KDeepseekV2Model"
    kwargs:
      transfer_map: 
        30: "cuda:1"
</code></pre>
<p>And we have to set the device for each module in the model.</p>
<p>For example, for <code>routed experts</code>, the yaml for one GPU is:</p>
<pre><code class="language-yaml">- match:
    name: "^model\\.layers\\..*\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # Custom MoE kernel with expert parallelism
    kwargs:
      generate_device: "cuda:0"
      generate_op: "MLPCUDAExperts"
      out_device: "cuda:0"
  recursive: False # Don't recursively inject submodules of this module
</code></pre>
<p>But for two GPUs, we need to set the device for each module in the model.</p>
<pre><code class="language-yaml"># allcate 0-29 layers‚Äòs out_device to cuda:0
- match:
    name: "^model\\.layers\\.(0|[1-9]|[12][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:0"
  recursive: False # don't recursively inject submodules of this module

# allocate 30-59 layers‚Äòs out_device to cuda:1
- match:
    name: "^model\\.layers\\.([345][0-9])\\.mlp\\.experts$"
  replace:
    class: ktransformers.operators.experts.KTransformersExperts     # custom MoE Kernel with expert paralleism
    kwargs:
      generate_device: "cpu"
      generate_op:  "KExpertsCPU"
      out_device: "cuda:1"
  recursive: False # don't recursively inject submodules of this module
</code></pre>
<p>For other modules, we can set the device in the same way.</p>
<h2 id="how-to-write-a-new-operator-and-inject-into-the-model"><a class="header" href="#how-to-write-a-new-operator-and-inject-into-the-model">How to Write a New Operator and Inject into the Model</a></h2>
<p>In this section, we will explain how to write an operator that can be injected, using the implementation of a new linear as an example.</p>
<p>First, all injectable operators need to inherit from the BaseInjectedModule class, which inherits some attributes required by our injection framework. Its initialization function needs to meet the following basic format:</p>
<pre><code class="language-python">class LinearTorchInject(BaseInjectedModule):
    def __init__(
        self,
        key: str,
        gguf_loader: GGUFLoader,
        config: PretrainedConfig,
        orig_module: nn.Module = None,
        generate_device: str = "cuda",
        **kwargs,
    ):
        super().__init__(key, gguf_loader, config, orig_module, generate_device, **kwargs)
</code></pre>
<p>If users have other parameters that need to be passed to this class, they can also be included in the init function and re-passed in the kwargs parameter in the yaml file. For example, if our operator wants to pass a parameter <code>my_param</code>, the init function can be written as:</p>
<pre><code class="language-python">class LinearTorchInject(BaseInjectedModule):
    def __init__(
        self,
        key: str,
        gguf_loader: GGUFLoader,
        config: PretrainedConfig,
        orig_module: nn.Module = None,
        generate_device: str = "cuda",
        my_param: bool = True,
        **kwargs,
    ):
        super().__init__(key, gguf_loader, config, orig_module, generate_device, **kwargs)
        self.my_param = my_param
</code></pre>
<p>Then our injection rule can be written as:</p>
<pre><code class="language-yaml">- match: 
    name: "^model\\.layers\\..*$"  # Regular expression matches the module name.
    class: torch.nn.Linear  # Type restrictions can be added.
  replace:
    class: ktransformers.operators.linear.LinearTorchInject  # Inject module path
    kwargs: # Extra parameters
      generate_device: "cuda"
      my_param: True
</code></pre>
<p>For the linear module, it is also necessary to read weights from a gguf file. We provide the <code>KLinearBase</code> class to help users read weights from gguf files. Users only need to inherit and implement the load, unload, and forward functions. Therefore, a fully injectable linear class would look like this:</p>
<pre><code class="language-python">class LinearTorchInject(BaseInjectedModule, KLinearBase):
    def __init__(
        self,
        key: str,
        gguf_loader: GGUFLoader,
        config: PretrainedConfig,
        orig_module: nn.Module = None,
        generate_device: str = "cuda",
        **kwargs,
    ):
        super().__init__(key, gguf_loader, config, orig_module, generate_device, **kwargs)
        KLinearBase.__init__(self)
        self.has_bias = False
        self.dtype = torch.get_default_dtype()
        self.w = None
        self.has_bias = False
    
    def load(self, w: dict | nn.Parameter | tuple | None = None, device: str|None = None):
        if device is None: device = self.device
        if w is None: w = self.load_weight(device=device)

        if isinstance(w, nn.Parameter):
            self.w = w.to(dtype=self.dtype).view(self.out_features, self.in_features).T
            self.has_bias = False
        elif isinstance(w, tuple):
            self.w = w[0].to(dtype=self.dtype).view(self.out_features, self.in_features).T
            self.bias = w[1].to(dtype=self.dtype)
            self.has_bias = True
        else:
            raise ValueError("Invalid weight type")
        self.w = self.w.to(device)
        if self.has_bias:
            self.bias = self.bias.to(device)

    def unload(self):
        if self.w is not None:
            self.w = None
        if self.has_bias:
            self.bias = None

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        dtype = x.dtype
        out_device = x.device
        x = x.to(device=self.device, dtype=self.dtype)
        x = x @ self.w
        if self.has_bias:
            x = x + self.bias
        x = x.to(dtype=dtype, device=out_device)
        return x
</code></pre>
<p>Note that the <code>self.load_weight</code> function is provided by the KLinearBase class to help users load weights from a gguf file into the module. The implementation details of KLinearBase can be found on <a href="https://github.com/kvcache-ai/ktransformers/blob/44f57270c9514d79fab224186d90ccf61059331a/ktransformers/operators/linear.py#L31">GITHUB</a>.</p>
<div style="break-before: page; page-break-before: always;"></div>
<h1 id="kt-kernel-docs"><a class="header" href="#kt-kernel-docs">kt-kernel Docs</a></h1>
<p>To be written‚Ä¶</p>
<div style="break-before: page; page-break-before: always;"></div><!-- omit in toc -->
<h1 id="see-the-issue-faq-page"><a class="header" href="#see-the-issue-faq-page">see the issue <a href="https://github.com/kvcache-ai/ktransformers/issues/1608">FAQ page</a></a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <template id=fa-eye><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M288 32c-80.8 0-145.5 36.8-192.6 80.6C48.6 156 17.3 208 2.5 243.7c-3.3 7.9-3.3 16.7 0 24.6C17.3 304 48.6 356 95.4 399.4C142.5 443.2 207.2 480 288 480s145.5-36.8 192.6-80.6c46.8-43.5 78.1-95.4 93-131.1c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C433.5 68.8 368.8 32 288 32zM432 256c0 79.5-64.5 144-144 144s-144-64.5-144-144s64.5-144 144-144s144 64.5 144 144zM288 192c0 35.3-28.7 64-64 64c-11.5 0-22.3-3-31.6-8.4c-.2 2.8-.4 5.5-.4 8.4c0 53 43 96 96 96s96-43 96-96s-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6z"/></svg></span></template>
        <template id=fa-eye-slash><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M38.8 5.1C28.4-3.1 13.3-1.2 5.1 9.2S-1.2 34.7 9.2 42.9l592 464c10.4 8.2 25.5 6.3 33.7-4.1s6.3-25.5-4.1-33.7L525.6 386.7c39.6-40.6 66.4-86.1 79.9-118.4c3.3-7.9 3.3-16.7 0-24.6c-14.9-35.7-46.2-87.7-93-131.1C465.5 68.8 400.8 32 320 32c-68.2 0-125 26.3-169.3 60.8L38.8 5.1zM223.1 149.5C248.6 126.2 282.7 112 320 112c79.5 0 144 64.5 144 144c0 24.9-6.3 48.3-17.4 68.7L408 294.5c5.2-11.8 8-24.8 8-38.5c0-53-43-96-96-96c-2.8 0-5.6 .1-8.4 .4c5.3 9.3 8.4 20.1 8.4 31.6c0 10.2-2.4 19.8-6.6 28.3l-90.3-70.8zm223.1 298L373 389.9c-16.4 6.5-34.3 10.1-53 10.1c-79.5 0-144-64.5-144-144c0-6.9 .5-13.6 1.4-20.2L83.1 161.5C60.3 191.2 44 220.8 34.5 243.7c-3.3 7.9-3.3 16.7 0 24.6c14.9 35.7 46.2 87.7 93 131.1C174.5 443.2 239.2 480 320 480c47.8 0 89.9-12.9 126.2-32.5z"/></svg></span></template>
        <template id=fa-copy><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M502.6 70.63l-61.25-61.25C435.4 3.371 427.2 0 418.7 0H255.1c-35.35 0-64 28.66-64 64l.0195 256C192 355.4 220.7 384 256 384h192c35.2 0 64-28.8 64-64V93.25C512 84.77 508.6 76.63 502.6 70.63zM464 320c0 8.836-7.164 16-16 16H255.1c-8.838 0-16-7.164-16-16L239.1 64.13c0-8.836 7.164-16 16-16h128L384 96c0 17.67 14.33 32 32 32h47.1V320zM272 448c0 8.836-7.164 16-16 16H63.1c-8.838 0-16-7.164-16-16L47.98 192.1c0-8.836 7.164-16 16-16H160V128H63.99c-35.35 0-64 28.65-64 64l.0098 256C.002 483.3 28.66 512 64 512h192c35.2 0 64-28.8 64-64v-32h-47.1L272 448z"/></svg></span></template>
        <template id=fa-play><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M73 39c-14.8-9.1-33.4-9.4-48.5-.9S0 62.6 0 80V432c0 17.4 9.4 33.4 24.5 41.9s33.7 8.1 48.5-.9L361 297c14.3-8.7 23-24.2 23-41s-8.7-32.2-23-41L73 39z"/></svg></span></template>
        <template id=fa-clock-rotate-left><span class=fa-svg><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc. --><path d="M75 75L41 41C25.9 25.9 0 36.6 0 57.9V168c0 13.3 10.7 24 24 24H134.1c21.4 0 32.1-25.9 17-41l-30.8-30.8C155 85.5 203 64 256 64c106 0 192 86 192 192s-86 192-192 192c-40.8 0-78.6-12.7-109.7-34.4c-14.5-10.1-34.4-6.6-44.6 7.9s-6.6 34.4 7.9 44.6C151.2 495 201.7 512 256 512c141.4 0 256-114.6 256-256S397.4 0 256 0C185.3 0 121.3 28.7 75 75zm181 53c-13.3 0-24 10.7-24 24V256c0 6.4 2.5 12.5 7 17l72 72c9.4 9.4 24.6 9.4 33.9 0s9.4-24.6 0-33.9l-65-65V152c0-13.3-10.7-24-24-24z"/></svg></span></template>



        <script>
            window.playground_copyable = true;
        </script>

        <script src="ace-2a3cd908.js"></script>
        <script src="mode-rust-2c9d5c9a.js"></script>
        <script src="editor-16ca416c.js"></script>
        <script src="theme-dawn-4493f9c8.js"></script>
        <script src="theme-tomorrow_night-9dbe62a9.js"></script>

        <script src="elasticlunr-ef4e11c1.min.js"></script>
        <script src="mark-09e88c2c.min.js"></script>
        <script src="searcher-c2a407aa.js"></script>

        <script src="clipboard-1626706a.min.js"></script>
        <script src="highlight-abc7f01d.js"></script>
        <script src="book-a0b12cfe.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
