{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6201cdec-70f7-4c22-b988-b23ece31979d",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <!-- <h1>KTransformers</h1> -->\n",
    "  <p align=\"center\">\n",
    "\n",
    "<picture>\n",
    "    <img alt=\"KTransformers\" src=\"https://github.com/user-attachments/assets/d5a2492f-a415-4456-af99-4ab102f13f8b\" width=50%>\n",
    "\n",
    "</picture>\n",
    "\n",
    "</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcfddc6-d51b-4aa8-b887-f7c817492316",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Introduction**\n",
    "[KTransformers](https://github.com/kvcache-ai/ktransformers), is designed to enhance the ü§ó Transformers experience through advanced kernel optimizations and placement/parallelism strategies. \n",
    "<br/> <br/>\n",
    "This tutorial serves as a guide for KTransformers-ft, aiming to to give resource-constrained researchers a **local path to explore fine-tuning ultra-large models (e.g., 671B/1000B)**, and also a fast way to customize smaller models (e.g., 14B/30B) for specific scenarios. We validate the setup using representative tasks such as stylized dialogue, Westernized translation tone, and medical Q&A, demonstrating that personalized adaptation can be achieved within hours.\n",
    "<br/> <br/>\n",
    "This tutorial takes DeepSeek-V2-Lite as a code example; for more details, refer to [KTransformers-Fine-Tuning_User-Guide](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/KTransformers-Fine-Tuning_User-Guide.md) and [KTransformers-Fine-Tuning_Developer-Technical-Notes](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/KTransformers-Fine-Tuning_Developer-Technical-Notes.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4167684-81f4-4e2b-a486-c33ec3bc92f0",
   "metadata": {},
   "source": [
    "# **Installation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5548a7f8-20d6-4ae4-a575-a3ef7a0ea5f8",
   "metadata": {},
   "source": [
    "### **1. Install torch and clone the repo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f39051d-eb14-44fa-af82-9ded23144985",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "remote: Enumerating objects: 446, done.\u001b[K\n",
      "remote: Counting objects: 100% (446/446), done.\u001b[K\n",
      "remote: Compressing objects: 100% (341/341), done.\u001b[K\n",
      "remote: Total 446 (delta 109), reused 326 (delta 87), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (446/446), 5.15 MiB | 1.61 MiB/s, done.\n",
      "Resolving deltas: 100% (109/109), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dd351f-9102-4d7d-951c-4306df9f4cd7",
   "metadata": {},
   "source": [
    "**(Optional)** If you want to choose your version of torch and cuda, please install separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5afa0c-1ed0-4190-ab50-967e553d6fd2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.7.0\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
      "Collecting torchvision==0.22.0\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio==2.7.0\n",
      "  Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting filelock (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch==2.7.0) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch==2.7.0) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch==2.7.0) (3.1.6)\n",
      "Collecting fsspec (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==9.1.0.70 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
      "Collecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.3.0.86 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
      "Collecting nvidia-nccl-cu11==2.21.5 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.8.86 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.3.0 (from torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting numpy (from torchvision==0.22.0)\n",
      "  Using cached https://download.pytorch.org/whl/numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.22.0)\n",
      "  Using cached https://download.pytorch.org/whl/pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch==2.7.0)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from jinja2->torch==2.7.0) (3.0.3)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (955.5 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (6.7 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "Using cached https://download.pytorch.org/whl/triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
      "Using cached https://download.pytorch.org/whl/pillow-11.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached https://download.pytorch.org/whl/fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "Using cached https://download.pytorch.org/whl/networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached https://download.pytorch.org/whl/numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Installing collected packages: mpmath, triton, sympy, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, numpy, networkx, fsspec, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22/22\u001b[0m [torchaudio]0m \u001b[32m21/22\u001b[0m [torchaudio]]]-cudnn-cu11]11]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.19.1 fsspec-2025.9.0 mpmath-1.3.0 networkx-3.5 numpy-2.3.3 nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 pillow-11.3.0 sympy-1.14.0 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711dcc79-056f-4483-a2e1-7e780af1def1",
   "metadata": {},
   "source": [
    "### **2. Install LLaMA-Factory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42f09df9-7db8-46e3-b11d-2946a57d2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"LLaMA-Factory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a6a5532-e5cc-463b-bdf8-030e547287fc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory\n",
      "  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets<=4.0.0,>=2.16.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting accelerate<=1.11.0,>=1.3.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting peft<=0.17.1,>=0.14.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting gradio<=5.45.0,>=4.38.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached gradio-5.45.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting matplotlib>=3.7.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting einops (from llamafactory==0.9.4.dev0)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy<2.0.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting pandas>=2.0.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting scipy (from llamafactory==0.9.4.dev0)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting sentencepiece (from llamafactory==0.9.4.dev0)\n",
      "  Using cached sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
      "Collecting tiktoken (from llamafactory==0.9.4.dev0)\n",
      "  Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting modelscope>=1.14.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached modelscope-1.31.0-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting hf-transfer (from llamafactory==0.9.4.dev0)\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting safetensors<=0.5.3 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting fire (from llamafactory==0.9.4.dev0)\n",
      "  Using cached fire-0.7.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting omegaconf (from llamafactory==0.9.4.dev0)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: packaging in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from llamafactory==0.9.4.dev0) (25.0)\n",
      "Collecting protobuf (from llamafactory==0.9.4.dev0)\n",
      "  Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pyyaml in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from llamafactory==0.9.4.dev0) (6.0.3)\n",
      "Collecting pydantic<=2.10.6 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting uvicorn (from llamafactory==0.9.4.dev0)\n",
      "  Using cached uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi (from llamafactory==0.9.4.dev0)\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1010)'))': /packages/94/fd/2e6f7d706899cc08690c5f6641e2ffbfffe019e8f16ce77104caa5730910/fastapi-0.121.1-py3-none-any.whl.metadata\u001b[0m\u001b[33m\n",
      "\u001b[0m  Downloading fastapi-0.121.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting sse-starlette (from llamafactory==0.9.4.dev0)\n",
      "  Using cached sse_starlette-3.0.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting av (from llamafactory==0.9.4.dev0)\n",
      "  Using cached av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
      "Collecting librosa (from llamafactory==0.9.4.dev0)\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting propcache!=0.4.0 (from llamafactory==0.9.4.dev0)\n",
      "  Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from llamafactory==0.9.4.dev0) (2.7.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from llamafactory==0.9.4.dev0) (0.22.0+cu118)\n",
      "Collecting nltk (from llamafactory==0.9.4.dev0)\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting jieba (from llamafactory==0.9.4.dev0)\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Collecting rouge-chinese (from llamafactory==0.9.4.dev0)\n",
      "  Using cached rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: psutil in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0) (7.1.3)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.19.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (2.32.5)\n",
      "Collecting tqdm>=4.66.3 (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.11.0)\n",
      "Collecting brotli>=1.1.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting ffmpy (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached ffmpy-0.6.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.13.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached gradio_client-1.13.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.28.1)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: jinja2<4.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.0.3)\n",
      "Collecting orjson~=3.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached orjson-3.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (11.3.0)\n",
      "Collecting pydub (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting ruff>=0.9.3 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached ruff-0.14.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (4.15.0)\n",
      "Collecting websockets<16.0,>=10.0 (from gradio-client==1.13.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.3.1)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached starlette-0.49.3-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi->llamafactory==0.9.4.dev0)\n",
      "  Using cached annotated_doc-0.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface_hub>=0.21.0->accelerate<=1.11.0,>=1.3.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=2.0.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pandas>=2.0.0->llamafactory==0.9.4.dev0) (2025.2)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<=2.10.6->llamafactory==0.9.4.dev0)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.4.dev0)\n",
      "  Using cached pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers!=4.52.0,!=4.57.0,<=4.57.1,>=4.49.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting click>=8.0.0 (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.7.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: setuptools in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (80.9.0)\n",
      "Requirement already satisfied: urllib3>=1.26 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from modelscope>=1.14.0->llamafactory==0.9.4.dev0) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->llamafactory==0.9.4.dev0) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from requests>=2.32.2->datasets<=4.0.0,>=2.16.0->llamafactory==0.9.4.dev0) (3.4.4)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.45.0,>=4.38.0->llamafactory==0.9.4.dev0)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.0.0->llamafactory==0.9.4.dev0) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->llamafactory==0.9.4.dev0) (1.3.0)\n",
      "Collecting termcolor (from fire->llamafactory==0.9.4.dev0)\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached audioread-3.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting numba>=0.51.0 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting scikit-learn>=1.1.0 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting joblib>=1.0 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from librosa->llamafactory==0.9.4.dev0) (5.2.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached soxr-1.0.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.6 kB)\n",
      "Collecting lazy_loader>=0.1 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.0->librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pooch>=1.1->librosa->llamafactory==0.9.4.dev0) (4.5.0)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.4.dev0)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: cffi>=1.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.4.dev0) (2.23)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.4.dev0)\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached gradio-5.45.0-py3-none-any.whl (60.4 MB)\n",
      "Using cached gradio_client-1.13.0-py3-none-any.whl (325 kB)\n",
      "Using cached aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading fastapi-0.121.1-py3-none-any.whl (109 kB)\n",
      "Using cached groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "Using cached orjson-3.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "Using cached safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Using cached starlette-0.49.3-py3-none-any.whl (74 kB)\n",
      "Using cached tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Using cached typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "Using cached websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Using cached aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached annotated_doc-0.0.3-py3-none-any.whl (5.5 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached brotli-1.2.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "Using cached click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Using cached docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "Using cached modelscope-1.31.0-py3-none-any.whl (5.9 MB)\n",
      "Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Using cached python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached ruff-0.14.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Using cached av-16.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (40.5 MB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached ffmpy-0.6.4-py3-none-any.whl (5.6 kB)\n",
      "Using cached fire-0.7.1-py3-none-any.whl (115 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.1.0-py3-none-any.whl (23 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached msgpack-1.1.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (427 kB)\n",
      "Using cached numba-0.62.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
      "Using cached llvmlite-0.45.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-1.0.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (238 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached protobuf-6.33.0-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Using cached rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Using cached sentencepiece-0.2.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Using cached sse_starlette-3.0.3-py3-none-any.whl (11 kB)\n",
      "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tiktoken-0.12.0-cp312-cp312-manylinux_2_28_x86_64.whl (1.2 MB)\n",
      "Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.4.dev0-0.editable-py3-none-any.whl size=28775 sha256=8eee51c440dda70e528d279275207bac81d33db5fceec86525f242c8930c31f2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-eac_6kue/wheels/7e/4d/79/29e1c1e0800d7151fcc16b46bc293cf3eb0cbfcf6c263f1b35\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: pytz, pydub, jieba, brotli, antlr4-python3-runtime, xxhash, websockets, tqdm, tomlkit, threadpoolctl, termcolor, shtab, shellingham, sentencepiece, semantic-version, safetensors, ruff, rouge-chinese, regex, python-multipart, pyparsing, pydantic-core, pyarrow, protobuf, propcache, orjson, omegaconf, numpy, multidict, msgpack, mdurl, llvmlite, lazy_loader, kiwisolver, joblib, hf-xet, hf-transfer, groovy, fsspec, frozenlist, fonttools, ffmpy, einops, docstring-parser, dill, cycler, click, av, audioread, annotated-types, annotated-doc, aiohappyeyeballs, aiofiles, yarl, uvicorn, tiktoken, starlette, sse-starlette, soxr, soundfile, scipy, pydantic, pooch, pandas, numba, nltk, multiprocess, modelscope, markdown-it-py, huggingface_hub, fire, contourpy, aiosignal, tokenizers, scikit-learn, safehttpx, rich, matplotlib, gradio-client, fastapi, aiohttp, accelerate, tyro, typer, transformers, librosa, peft, gradio, datasets, trl, llamafactory\n",
      "\u001b[2K  Attempting uninstall: numpy‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23/91\u001b[0m [protobuf]arrow]ors]\n",
      "\u001b[2K    Found existing installation: numpy 2.3.3[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23/91\u001b[0m [protobuf]\n",
      "\u001b[2K    Uninstalling numpy-2.3.3:0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23/91\u001b[0m [protobuf]\n",
      "\u001b[2K      Successfully uninstalled numpy-2.3.3‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23/91\u001b[0m [protobuf]\n",
      "\u001b[2K  Attempting uninstall: fsspec‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35/91\u001b[0m [hf-xet]kiwisolver]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.9.0\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35/91\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling fsspec-2025.9.0:[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35/91\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.9.0m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35/91\u001b[0m [hf-xet]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m91/91\u001b[0m [llamafactory] [trl]m \u001b[32m88/91\u001b[0m [datasets]ers]ser]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 aiofiles-24.1.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-doc-0.0.3 annotated-types-0.7.0 antlr4-python3-runtime-4.9.3 audioread-3.1.0 av-16.0.1 brotli-1.2.0 click-8.3.0 contourpy-1.3.3 cycler-0.12.1 datasets-4.0.0 dill-0.3.8 docstring-parser-0.17.0 einops-0.8.1 fastapi-0.121.1 ffmpy-0.6.4 fire-0.7.1 fonttools-4.60.1 frozenlist-1.8.0 fsspec-2025.3.0 gradio-5.45.0 gradio-client-1.13.0 groovy-0.1.2 hf-transfer-0.1.9 hf-xet-1.2.0 huggingface_hub-0.36.0 jieba-0.42.1 joblib-1.5.2 kiwisolver-1.4.9 lazy_loader-0.4 librosa-0.11.0 llamafactory-0.9.4.dev0 llvmlite-0.45.1 markdown-it-py-4.0.0 matplotlib-3.10.7 mdurl-0.1.2 modelscope-1.31.0 msgpack-1.1.2 multidict-6.7.0 multiprocess-0.70.16 nltk-3.9.2 numba-0.62.1 numpy-1.26.4 omegaconf-2.3.0 orjson-3.11.4 pandas-2.3.3 peft-0.17.1 pooch-1.8.2 propcache-0.4.1 protobuf-6.33.0 pyarrow-22.0.0 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 pyparsing-3.2.5 python-multipart-0.0.20 pytz-2025.2 regex-2025.11.3 rich-14.2.0 rouge-chinese-1.0.3 ruff-0.14.4 safehttpx-0.1.7 safetensors-0.5.3 scikit-learn-1.7.2 scipy-1.16.3 semantic-version-2.10.0 sentencepiece-0.2.1 shellingham-1.5.4 shtab-1.7.2 soundfile-0.13.1 soxr-1.0.0 sse-starlette-3.0.3 starlette-0.49.3 termcolor-3.2.0 threadpoolctl-3.6.0 tiktoken-0.12.0 tokenizers-0.22.1 tomlkit-0.13.3 tqdm-4.67.1 transformers-4.57.1 trl-0.9.6 typer-0.20.0 tyro-0.8.14 uvicorn-0.38.0 websockets-15.0.1 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\" --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c19762-70a7-402c-94f9-a71b277eb932",
   "metadata": {},
   "source": [
    "### **3. Install dependency libraries for GCC and CUDA**\n",
    "You need to install system-level dependency libraries. `libstdcxx-ng` and `gcc_impl_linux-64` ensure compilation compatibility, while cuda-runtime provides a GPU-accelerated runtime environment. **Please do NOT IGNORE this two commands! `nvidia/label/cuda-11.8.0 cuda-runtime` should be installed for every version of cuda for KT whl.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "202e672a-b30a-4bde-92d5-27500f435b30",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /mnt/data/lpl/anaconda3/envs/KNllama\n",
      "\n",
      "  added / updated specs:\n",
      "    - gcc_impl_linux-64\n",
      "    - libstdcxx-ng\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  binutils_impl_lin~ anaconda/pkgs/main/linux-64::binutils_impl_linux-64-2.44-h4b9a079_2 \n",
      "  gcc_impl_linux-64  conda-forge/linux-64::gcc_impl_linux-64-10.4.0-h5231bdf_19 \n",
      "  kernel-headers_li~ conda-forge/noarch::kernel-headers_linux-64-5.14.0-he073ed8_2 \n",
      "  libgcc-devel_linu~ conda-forge/linux-64::libgcc-devel_linux-64-10.4.0-hd38fd1e_19 \n",
      "  libsanitizer       conda-forge/linux-64::libsanitizer-10.4.0-h5246dfb_19 \n",
      "  sysroot_linux-64   conda-forge/noarch::sysroot_linux-64-2.34-h087de78_2 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  libstdcxx-ng       anaconda/pkgs/main::libstdcxx-ng-11.2~ --> conda-forge::libstdcxx-ng-13.2.0-hc0a3c3a_7 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\u001b[1;33mJupyter detected\u001b[0m\u001b[1;33m...\u001b[0m\n",
      "Channels:\n",
      " - nvidia/label/cuda-11.8.0\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /mnt/data/lpl/anaconda3/envs/KNllama\n",
      "\n",
      "  added / updated specs:\n",
      "    - cuda-runtime\n",
      "\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cuda-cudart        nvidia/label/cuda-11.8.0/linux-64::cuda-cudart-11.8.89-0 \n",
      "  cuda-libraries     nvidia/label/cuda-11.8.0/linux-64::cuda-libraries-11.8.0-0 \n",
      "  cuda-nvrtc         nvidia/label/cuda-11.8.0/linux-64::cuda-nvrtc-11.8.89-0 \n",
      "  cuda-runtime       nvidia/label/cuda-11.8.0/linux-64::cuda-runtime-11.8.0-0 \n",
      "  libcublas          nvidia/label/cuda-11.8.0/linux-64::libcublas-11.11.3.6-0 \n",
      "  libcufft           nvidia/label/cuda-11.8.0/linux-64::libcufft-10.9.0.58-0 \n",
      "  libcufile          nvidia/label/cuda-11.8.0/linux-64::libcufile-1.4.0.31-0 \n",
      "  libcurand          nvidia/label/cuda-11.8.0/linux-64::libcurand-10.3.0.86-0 \n",
      "  libcusolver        nvidia/label/cuda-11.8.0/linux-64::libcusolver-11.4.1.48-0 \n",
      "  libcusparse        nvidia/label/cuda-11.8.0/linux-64::libcusparse-11.7.5.86-0 \n",
      "  libnpp             nvidia/label/cuda-11.8.0/linux-64::libnpp-11.8.0.86-0 \n",
      "  libnvjpeg          nvidia/label/cuda-11.8.0/linux-64::libnvjpeg-11.9.0.86-0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -y -c conda-forge libstdcxx-ng gcc_impl_linux-64\n",
    "!conda install -y -c nvidia/label/cuda-11.8.0 cuda-runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6448f-1e27-4f16-885c-27738c2089dc",
   "metadata": {},
   "source": [
    "### **4. Install ktransformers and flash-attention**\n",
    "You need to download the corresponding version of python, cuda and torch from [downloading ktransformers whl](https://github.com/kvcache-ai/ktransformers/releases/tag/v0.4.1) and [downloading flash-attention whl](https://github.com/Dao-AILab/flash-attention/releases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c4a5e82-ae9f-490f-9f90-441cdd98041e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch._C._GLIBCXX_USE_CXX11_ABI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "837a2240-818d-499f-a1b5-641fa5c45339",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/lpl/LLaMA-Factory-KT/notebook/ktransformers-0.4.1+cu128torch27fancy-cp312-cp312-linux_x86_64.whl\n",
      "Requirement already satisfied: torch>=2.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (2.7.0+cu118)\n",
      "Collecting transformers==4.51.3 (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting peft==0.14.0 (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fastapi>=0.111.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (0.121.1)\n",
      "Requirement already satisfied: uvicorn>=0.30.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (0.38.0)\n",
      "Collecting langchain>=0.2.0 (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Downloading langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting blessed>=1.20.0 (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached blessed-1.23.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: accelerate>=0.31.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (1.11.0)\n",
      "Requirement already satisfied: sentencepiece>=0.1.97 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (0.2.1)\n",
      "Requirement already satisfied: setuptools in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (80.9.0)\n",
      "Collecting ninja (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: wheel in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (0.45.1)\n",
      "Collecting colorlog (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting build (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached build-1.3.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: fire in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (0.7.1)\n",
      "Requirement already satisfied: protobuf in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (6.33.0)\n",
      "Requirement already satisfied: datasets in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from ktransformers==0.4.1+cu128torch27fancy) (4.0.0)\n",
      "Collecting torchviz (from ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached torchviz-0.0.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (25.0)\n",
      "Requirement already satisfied: psutil in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (6.0.3)\n",
      "Requirement already satisfied: tqdm in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (4.67.1)\n",
      "Requirement already satisfied: safetensors in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (0.36.0)\n",
      "Requirement already satisfied: filelock in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from transformers==4.51.3->ktransformers==0.4.1+cu128torch27fancy) (3.19.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from transformers==4.51.3->ktransformers==0.4.1+cu128torch27fancy) (2025.11.3)\n",
      "Requirement already satisfied: requests in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from transformers==4.51.3->ktransformers==0.4.1+cu128torch27fancy) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.51.3->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from huggingface-hub>=0.25.0->peft==0.14.0->ktransformers==0.4.1+cu128torch27fancy) (1.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from blessed>=1.20.0->ktransformers==0.4.1+cu128torch27fancy) (0.2.13)\n",
      "Requirement already satisfied: starlette<0.50.0,>=0.40.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (0.49.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (2.10.6)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (0.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (2.27.2)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from starlette<0.50.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (3.11)\n",
      "Requirement already satisfied: sniffio>=1.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.50.0,>=0.40.0->fastapi>=0.111.0->ktransformers==0.4.1+cu128torch27fancy) (1.3.1)\n",
      "Collecting langchain-core<2.0.0,>=1.0.4 (from langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Downloading langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.4->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core<2.0.0,>=1.0.4->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached langsmith-0.4.41-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-core<2.0.0,>=1.0.4->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.4->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy) (3.6.0)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy) (3.11.4)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.4->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: certifi in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain>=0.2.0->ktransformers==0.4.1+cu128torch27fancy) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from requests->transformers==4.51.3->ktransformers==0.4.1+cu128torch27fancy) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from requests->transformers==4.51.3->ktransformers==0.4.1+cu128torch27fancy) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (1.14.0)\n",
      "Requirement already satisfied: networkx in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (3.5)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (1.3.0)\n",
      "Requirement already satisfied: click>=7.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from uvicorn>=0.30.1->ktransformers==0.4.1+cu128torch27fancy) (8.3.0)\n",
      "Collecting pyproject_hooks (from build->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from datasets->ktransformers==0.4.1+cu128torch27fancy) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from datasets->ktransformers==0.4.1+cu128torch27fancy) (0.3.8)\n",
      "Requirement already satisfied: pandas in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from datasets->ktransformers==0.4.1+cu128torch27fancy) (2.3.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from datasets->ktransformers==0.4.1+cu128torch27fancy) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (3.13.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->ktransformers==0.4.1+cu128torch27fancy) (1.22.0)\n",
      "Requirement already satisfied: termcolor in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from fire->ktransformers==0.4.1+cu128torch27fancy) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->ktransformers==0.4.1+cu128torch27fancy) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pandas->datasets->ktransformers==0.4.1+cu128torch27fancy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pandas->datasets->ktransformers==0.4.1+cu128torch27fancy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from pandas->datasets->ktransformers==0.4.1+cu128torch27fancy) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ktransformers==0.4.1+cu128torch27fancy) (1.17.0)\n",
      "Collecting graphviz (from torchviz->ktransformers==0.4.1+cu128torch27fancy)\n",
      "  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n",
      "Using cached peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached blessed-1.23.0-py3-none-any.whl (95 kB)\n",
      "Downloading langchain-1.0.5-py3-none-any.whl (93 kB)\n",
      "Downloading langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langgraph-1.0.2-py3-none-any.whl (156 kB)\n",
      "Using cached langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Using cached langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n",
      "Using cached langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Using cached langsmith-0.4.41-py3-none-any.whl (399 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "Using cached build-1.3.0-py3-none-any.whl (23 kB)\n",
      "Using cached colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Using cached graphviz-0.21-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: zstandard, tenacity, pyproject_hooks, ormsgpack, ninja, jsonpatch, graphviz, colorlog, blessed, requests-toolbelt, build, torchviz, tokenizers, langsmith, langgraph-sdk, transformers, langchain-core, peft, langgraph-checkpoint, langgraph-prebuilt, langgraph, langchain, ktransformers\n",
      "\u001b[2K  Attempting uninstall: tokenizers;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/23\u001b[0m [ormsgpack]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.137m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/23\u001b[0m [ormsgpack]\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.1:7m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/23\u001b[0m [ormsgpack]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.1;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m 3/23\u001b[0m [ormsgpack]\n",
      "\u001b[2K  Attempting uninstall: transformers‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;249;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/23\u001b[0m [langsmith]]\n",
      "\u001b[2K    Found existing installation: transformers 4.57.19;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/23\u001b[0m [langsmith]\n",
      "\u001b[2K    Uninstalling transformers-4.57.1:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15/23\u001b[0m [transformers]\n",
      "\u001b[2K      Successfully uninstalled transformers-4.57.18;5;237m‚ï∫\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m15/23\u001b[0m [transformers]\n",
      "\u001b[2K  Attempting uninstall: peft‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;249;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/23\u001b[0m [langchain-core]\n",
      "\u001b[2K    Found existing installation: peft 0.17.1\u001b[0m\u001b[38;2;249;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/23\u001b[0m [langchain-core]\n",
      "\u001b[2K    Uninstalling peft-0.17.1:‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;249;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/23\u001b[0m [langchain-core]\n",
      "\u001b[2K      Successfully uninstalled peft-0.17.1‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[38;2;249;38;114m‚ï∏\u001b[0m\u001b[38;5;237m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17/23\u001b[0m [peft]-core]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23/23\u001b[0m [ktransformers]\u001b[32m22/23\u001b[0m [ktransformers]in]\n",
      "\u001b[1A\u001b[2KSuccessfully installed blessed-1.23.0 build-1.3.0 colorlog-6.10.1 graphviz-0.21 jsonpatch-1.33 ktransformers-0.4.1+cu128torch27fancy langchain-1.0.5 langchain-core-1.0.4 langgraph-1.0.2 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.2 langgraph-sdk-0.2.9 langsmith-0.4.41 ninja-1.13.0 ormsgpack-1.12.0 peft-0.14.0 pyproject_hooks-1.2.0 requests-toolbelt-1.0.0 tenacity-9.1.2 tokenizers-0.21.4 torchviz-0.0.3 transformers-4.51.3 zstandard-0.25.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ../ktransformers-0.4.1+cu128torch27fancy-cp312-cp312-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3c78d9e-26e0-4f85-94ff-d6b028b194ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/lpl/LLaMA-Factory-KT/notebook/flash_attn-2.8.3+cu12torch2.7cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (2.7.0+cu118)\n",
      "Requirement already satisfied: einops in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (0.8.1)\n",
      "Requirement already satisfied: filelock in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (1.14.0)\n",
      "Requirement already satisfied: networkx in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (3.5)\n",
      "Requirement already satisfied: jinja2 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from sympy>=1.13.3->torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages (from jinja2->torch->flash-attn==2.8.3+cu12torch2.7cxx11abitrue) (3.0.3)\n",
      "Installing collected packages: flash-attn\n",
      "Successfully installed flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install ../flash_attn-2.8.3+cu12torch2.7cxx11abiTRUE-cp312-cp312-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593e2cb-5fbd-4d66-94fc-d2d74c4d8f65",
   "metadata": {},
   "source": [
    "# **How to Start**\n",
    "## Fine-tuning the Model with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db3349-8cdb-48cd-8b63-0ea70fe4af6f",
   "metadata": {},
   "source": [
    "LoRA (Low-Rank Adaptation) fine-tuning only trains small \"adapter\" weights for large models. However, under traditional frameworks, it still needs more than 1400GB GPU VRAM, which hardly handles on the 4090s machine. **KTransformers**, as high-performance backend engine, provides a solution for GPU/CPU Hybrid devices to further cut GPU memory usage and speed up training. As shown below, we compare KTransformers(ours) with other common LoRA fine-tuning backends (HuggingFace and Unsloth). KTransformers is the **only workable 4090-class solution** for ultra-large MoE models (e.g., 671B) and also delivers higher fine-tuning throughput. <br/>\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://typora-tuchuang-jimmy.oss-cn-beijing.aliyuncs.com/img/ÊåâÁÖßÊ®°ÂûãÂàíÂàÜÁöÑÂØπÊØîÂõæ_02.png\" alt=\"kt_unsloth_huggingface_compare\" width=\"70%\" height=\"auto\">\n",
    "</div>\n",
    "\n",
    "To make KTransformers-ft more easy-to-use, we cooperator with [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory/), a easy and efficiency model fine-tuning framework. As shown below, LLaMA-Factory is the unified configuration layer for the whole fine-tuning workflow. **KTransformers** acts as a high-performance backend that takes over core operators like Attention/MoE under the same training configs, enabling efficient **GPU+CPU heterogeneous cooperation**. <br/>\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://typora-tuchuang-jimmy.oss-cn-beijing.aliyuncs.com/img/image-20251011010558909.png\" alt=\"image-20251011010558909\" width=\"70%\" height=\"auto\">\n",
    "</div>\n",
    "\n",
    "This combination lets you fine-tune big models (like 671B/1000B) on consumer level GPUs (2-4 RTX 4090s) ‚Äî no need for expensive hardware. Here‚Äôs the training command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baf5b8fc-e910-4531-9f00-a2076c698eff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no balance_serve\n",
      "flashinfer not found, use triton for linux\n",
      "/mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "[INFO|2025-11-09 09:24:33] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:24:33,908 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:33,912 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:33,914 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:24:33,914 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:24:34,116 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-11-09 09:24:34] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 91 examples [00:00, 14003.07 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|‚ñà| 91/91 [00:00<00:00, 271.03 e\n",
      "Running tokenizer on dataset (num_proc=16): 100%|‚ñà| 91/91 [00:01<00:00, 68.21 ex\n",
      "training example:\n",
      "input_ids:\n",
      "[100000, 5726, 25, 2868, 185, 185, 77398, 25, 17464, 0, 304, 608, 17247, 1531, 11288, 274, 20838, 20308, 6907, 457, 17247, 9425, 20384, 1724, 481, 304, 4750, 340, 3571, 30, 100001]\n",
      "inputs:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>User: hi\n",
      "\n",
      "Assistant:Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, 17464, 0, 304, 608, 17247, 1531, 11288, 274, 20838, 20308, 6907, 457, 17247, 9425, 20384, 1724, 481, 304, 4750, 340, 3571, 30, 100001]\n",
      "labels:\n",
      "Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,065 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,067 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:24:37,067 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|2025-11-09 09:24:37] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,077 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,078 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:24:37,079 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "using custom modeling_xxx.py.\n",
      "[WARNING|logging.py:328] 2025-11-09 09:24:37,080 >> DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "[INFO|configuration_utils.py:1142] 2025-11-09 09:24:37,080 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001\n",
      "}\n",
      "\n",
      "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
      "Injecting model.embed_tokens as default\n",
      "Injecting model.layers as default\n",
      "Injecting model.layers.0 as default\n",
      "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.0.mlp as default\n",
      "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.act_fn as default\n",
      "Injecting model.layers.0.input_layernorm as default\n",
      "Injecting model.layers.0.post_attention_layernorm as default\n",
      "Injecting model.layers.1 as default\n",
      "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.1.mlp.gate as default\n",
      "Injecting model.layers.1.mlp.shared_experts as default\n",
      "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.1.input_layernorm as default\n",
      "Injecting model.layers.1.post_attention_layernorm as default\n",
      "Injecting model.layers.2 as default\n",
      "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.2.mlp.gate as default\n",
      "Injecting model.layers.2.mlp.shared_experts as default\n",
      "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.2.input_layernorm as default\n",
      "Injecting model.layers.2.post_attention_layernorm as default\n",
      "Injecting model.layers.3 as default\n",
      "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.3.mlp.gate as default\n",
      "Injecting model.layers.3.mlp.shared_experts as default\n",
      "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.3.input_layernorm as default\n",
      "Injecting model.layers.3.post_attention_layernorm as default\n",
      "Injecting model.layers.4 as default\n",
      "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.4.mlp.gate as default\n",
      "Injecting model.layers.4.mlp.shared_experts as default\n",
      "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.4.input_layernorm as default\n",
      "Injecting model.layers.4.post_attention_layernorm as default\n",
      "Injecting model.layers.5 as default\n",
      "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.5.mlp.gate as default\n",
      "Injecting model.layers.5.mlp.shared_experts as default\n",
      "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.5.input_layernorm as default\n",
      "Injecting model.layers.5.post_attention_layernorm as default\n",
      "Injecting model.layers.6 as default\n",
      "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.6.mlp.gate as default\n",
      "Injecting model.layers.6.mlp.shared_experts as default\n",
      "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.6.input_layernorm as default\n",
      "Injecting model.layers.6.post_attention_layernorm as default\n",
      "Injecting model.layers.7 as default\n",
      "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.7.mlp.gate as default\n",
      "Injecting model.layers.7.mlp.shared_experts as default\n",
      "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.7.input_layernorm as default\n",
      "Injecting model.layers.7.post_attention_layernorm as default\n",
      "Injecting model.layers.8 as default\n",
      "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.8.mlp.gate as default\n",
      "Injecting model.layers.8.mlp.shared_experts as default\n",
      "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.8.input_layernorm as default\n",
      "Injecting model.layers.8.post_attention_layernorm as default\n",
      "Injecting model.layers.9 as default\n",
      "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.9.mlp.gate as default\n",
      "Injecting model.layers.9.mlp.shared_experts as default\n",
      "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.9.input_layernorm as default\n",
      "Injecting model.layers.9.post_attention_layernorm as default\n",
      "Injecting model.layers.10 as default\n",
      "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.10.mlp.gate as default\n",
      "Injecting model.layers.10.mlp.shared_experts as default\n",
      "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.10.input_layernorm as default\n",
      "Injecting model.layers.10.post_attention_layernorm as default\n",
      "Injecting model.layers.11 as default\n",
      "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.11.mlp.gate as default\n",
      "Injecting model.layers.11.mlp.shared_experts as default\n",
      "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.11.input_layernorm as default\n",
      "Injecting model.layers.11.post_attention_layernorm as default\n",
      "Injecting model.layers.12 as default\n",
      "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.12.mlp.gate as default\n",
      "Injecting model.layers.12.mlp.shared_experts as default\n",
      "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.12.input_layernorm as default\n",
      "Injecting model.layers.12.post_attention_layernorm as default\n",
      "Injecting model.layers.13 as default\n",
      "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.13.mlp.gate as default\n",
      "Injecting model.layers.13.mlp.shared_experts as default\n",
      "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.13.input_layernorm as default\n",
      "Injecting model.layers.13.post_attention_layernorm as default\n",
      "Injecting model.layers.14 as default\n",
      "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.14.mlp.gate as default\n",
      "Injecting model.layers.14.mlp.shared_experts as default\n",
      "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.14.input_layernorm as default\n",
      "Injecting model.layers.14.post_attention_layernorm as default\n",
      "Injecting model.layers.15 as default\n",
      "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.15.mlp.gate as default\n",
      "Injecting model.layers.15.mlp.shared_experts as default\n",
      "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.15.input_layernorm as default\n",
      "Injecting model.layers.15.post_attention_layernorm as default\n",
      "Injecting model.layers.16 as default\n",
      "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.16.mlp.gate as default\n",
      "Injecting model.layers.16.mlp.shared_experts as default\n",
      "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.16.input_layernorm as default\n",
      "Injecting model.layers.16.post_attention_layernorm as default\n",
      "Injecting model.layers.17 as default\n",
      "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.17.mlp.gate as default\n",
      "Injecting model.layers.17.mlp.shared_experts as default\n",
      "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.17.input_layernorm as default\n",
      "Injecting model.layers.17.post_attention_layernorm as default\n",
      "Injecting model.layers.18 as default\n",
      "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.18.mlp.gate as default\n",
      "Injecting model.layers.18.mlp.shared_experts as default\n",
      "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.18.input_layernorm as default\n",
      "Injecting model.layers.18.post_attention_layernorm as default\n",
      "Injecting model.layers.19 as default\n",
      "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.19.mlp.gate as default\n",
      "Injecting model.layers.19.mlp.shared_experts as default\n",
      "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.19.input_layernorm as default\n",
      "Injecting model.layers.19.post_attention_layernorm as default\n",
      "Injecting model.layers.20 as default\n",
      "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.20.mlp.gate as default\n",
      "Injecting model.layers.20.mlp.shared_experts as default\n",
      "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.20.input_layernorm as default\n",
      "Injecting model.layers.20.post_attention_layernorm as default\n",
      "Injecting model.layers.21 as default\n",
      "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.21.mlp.gate as default\n",
      "Injecting model.layers.21.mlp.shared_experts as default\n",
      "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.21.input_layernorm as default\n",
      "Injecting model.layers.21.post_attention_layernorm as default\n",
      "Injecting model.layers.22 as default\n",
      "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.22.mlp.gate as default\n",
      "Injecting model.layers.22.mlp.shared_experts as default\n",
      "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.22.input_layernorm as default\n",
      "Injecting model.layers.22.post_attention_layernorm as default\n",
      "Injecting model.layers.23 as default\n",
      "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.23.mlp.gate as default\n",
      "Injecting model.layers.23.mlp.shared_experts as default\n",
      "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.23.input_layernorm as default\n",
      "Injecting model.layers.23.post_attention_layernorm as default\n",
      "Injecting model.layers.24 as default\n",
      "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.24.mlp.gate as default\n",
      "Injecting model.layers.24.mlp.shared_experts as default\n",
      "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.24.input_layernorm as default\n",
      "Injecting model.layers.24.post_attention_layernorm as default\n",
      "Injecting model.layers.25 as default\n",
      "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.25.mlp.gate as default\n",
      "Injecting model.layers.25.mlp.shared_experts as default\n",
      "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.25.input_layernorm as default\n",
      "Injecting model.layers.25.post_attention_layernorm as default\n",
      "Injecting model.layers.26 as default\n",
      "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.26.mlp.gate as default\n",
      "Injecting model.layers.26.mlp.shared_experts as default\n",
      "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.26.input_layernorm as default\n",
      "Injecting model.layers.26.post_attention_layernorm as default\n",
      "Injecting model.norm as default\n",
      "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
      "loading model.embed_tokens.weight to cpu\n",
      "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.0.self_attn.kv_b_proj.weight to cuda:0\n",
      "loading model.layers.0.input_layernorm.weight to cuda:0\n",
      "loading model.layers.0.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.1.mlp.gate.weight to cuda:0\n",
      "loading model.layers.1.input_layernorm.weight to cuda:0\n",
      "loading model.layers.1.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.2.mlp.gate.weight to cuda:0\n",
      "loading model.layers.2.input_layernorm.weight to cuda:0\n",
      "loading model.layers.2.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.3.mlp.gate.weight to cuda:0\n",
      "loading model.layers.3.input_layernorm.weight to cuda:0\n",
      "loading model.layers.3.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.4.mlp.gate.weight to cuda:0\n",
      "loading model.layers.4.input_layernorm.weight to cuda:0\n",
      "loading model.layers.4.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.5.mlp.gate.weight to cuda:0\n",
      "loading model.layers.5.input_layernorm.weight to cuda:0\n",
      "loading model.layers.5.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.6.mlp.gate.weight to cuda:0\n",
      "loading model.layers.6.input_layernorm.weight to cuda:0\n",
      "loading model.layers.6.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.7.mlp.gate.weight to cuda:0\n",
      "loading model.layers.7.input_layernorm.weight to cuda:0\n",
      "loading model.layers.7.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.8.mlp.gate.weight to cuda:0\n",
      "loading model.layers.8.input_layernorm.weight to cuda:0\n",
      "loading model.layers.8.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.9.mlp.gate.weight to cuda:0\n",
      "loading model.layers.9.input_layernorm.weight to cuda:0\n",
      "loading model.layers.9.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.10.mlp.gate.weight to cuda:0\n",
      "loading model.layers.10.input_layernorm.weight to cuda:0\n",
      "loading model.layers.10.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.11.mlp.gate.weight to cuda:0\n",
      "loading model.layers.11.input_layernorm.weight to cuda:0\n",
      "loading model.layers.11.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.12.mlp.gate.weight to cuda:0\n",
      "loading model.layers.12.input_layernorm.weight to cuda:0\n",
      "loading model.layers.12.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.13.mlp.gate.weight to cuda:0\n",
      "loading model.layers.13.input_layernorm.weight to cuda:0\n",
      "loading model.layers.13.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.14.mlp.gate.weight to cuda:0\n",
      "loading model.layers.14.input_layernorm.weight to cuda:0\n",
      "loading model.layers.14.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.15.mlp.gate.weight to cuda:0\n",
      "loading model.layers.15.input_layernorm.weight to cuda:0\n",
      "loading model.layers.15.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.16.mlp.gate.weight to cuda:0\n",
      "loading model.layers.16.input_layernorm.weight to cuda:0\n",
      "loading model.layers.16.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.17.mlp.gate.weight to cuda:0\n",
      "loading model.layers.17.input_layernorm.weight to cuda:0\n",
      "loading model.layers.17.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.18.mlp.gate.weight to cuda:0\n",
      "loading model.layers.18.input_layernorm.weight to cuda:0\n",
      "loading model.layers.18.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.19.mlp.gate.weight to cuda:0\n",
      "loading model.layers.19.input_layernorm.weight to cuda:0\n",
      "loading model.layers.19.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.20.mlp.gate.weight to cuda:0\n",
      "loading model.layers.20.input_layernorm.weight to cuda:0\n",
      "loading model.layers.20.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.21.mlp.gate.weight to cuda:0\n",
      "loading model.layers.21.input_layernorm.weight to cuda:0\n",
      "loading model.layers.21.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.22.mlp.gate.weight to cuda:0\n",
      "loading model.layers.22.input_layernorm.weight to cuda:0\n",
      "loading model.layers.22.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.23.mlp.gate.weight to cuda:0\n",
      "loading model.layers.23.input_layernorm.weight to cuda:0\n",
      "loading model.layers.23.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.24.mlp.gate.weight to cuda:0\n",
      "loading model.layers.24.input_layernorm.weight to cuda:0\n",
      "loading model.layers.24.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.25.mlp.gate.weight to cuda:0\n",
      "loading model.layers.25.input_layernorm.weight to cuda:0\n",
      "loading model.layers.25.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.26.mlp.gate.weight to cuda:0\n",
      "loading model.layers.26.input_layernorm.weight to cuda:0\n",
      "loading model.layers.26.post_attention_layernorm.weight to cuda:0\n",
      "loading model.norm.weight to cuda:0\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.model_utils.attention:143 >> Using vanilla attention implementation.\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.model_utils.misc:143 >> Found linear modules: down_proj,kv_a_proj_with_mqa,o_proj,q_proj,up_proj,orig_module,kv_b_proj,gate_proj,generate_linear,prefill_linear\n",
      "KT_model:PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DeepseekV2ForCausalLM(\n",
      "      (model): KDeepseekV2Model(\n",
      "        (orig_module): DeepseekV2Model(\n",
      "          (embed_tokens): Embedding(102400, 2048)\n",
      "          (layers): ModuleList(\n",
      "            (0): DeepseekV2DecoderLayer(\n",
      "              (self_attn): KDeepseekV2Attention(\n",
      "                (orig_module): DeepseekV2Attention(\n",
      "                  (q_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=3072, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_proj_with_mqa): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=576, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=576, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_layernorm): DeepseekV2RMSNorm()\n",
      "                  (kv_b_proj): lora.Linear(\n",
      "                    (orig_module): Linear(in_features=512, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=2048, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (rotary_emb): YarnRotaryEmbedding(\n",
      "                    (orig_module): DeepseekV2YarnRotaryEmbedding()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (mlp): DeepseekV2MLP(\n",
      "                (gate_proj): KTransformersLinearLora(\n",
      "                  (orig_module): Linear(\n",
      "                    in_features=2048, out_features=10944, bias=False\n",
      "                    (prefill_linear): KLinearTorch()\n",
      "                    (generate_linear): KLinearTorch()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=10944, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (up_proj): KTransformersLinearLora(\n",
      "                  (orig_module): Linear(\n",
      "                    in_features=2048, out_features=10944, bias=False\n",
      "                    (prefill_linear): KLinearTorch()\n",
      "                    (generate_linear): KLinearTorch()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=10944, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (down_proj): KTransformersLinearLora(\n",
      "                  (orig_module): Linear(\n",
      "                    in_features=10944, out_features=2048, bias=False\n",
      "                    (prefill_linear): KLinearTorch()\n",
      "                    (generate_linear): KLinearTorch()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=10944, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): DeepseekV2RMSNorm()\n",
      "              (post_attention_layernorm): DeepseekV2RMSNorm()\n",
      "            )\n",
      "            (1-26): 26 x DeepseekV2DecoderLayer(\n",
      "              (self_attn): KDeepseekV2Attention(\n",
      "                (orig_module): DeepseekV2Attention(\n",
      "                  (q_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=3072, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_proj_with_mqa): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=576, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=576, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_layernorm): DeepseekV2RMSNorm()\n",
      "                  (kv_b_proj): lora.Linear(\n",
      "                    (orig_module): Linear(in_features=512, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=2048, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (rotary_emb): YarnRotaryEmbedding(\n",
      "                    (orig_module): DeepseekV2YarnRotaryEmbedding()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (mlp): KDeepseekV2MoE(\n",
      "                (orig_module): DeepseekV2MoE(\n",
      "                  (experts): KTransformersExperts(\n",
      "                    (orig_module): ModuleList(\n",
      "                      (0-63): 64 x DeepseekV2MLP(\n",
      "                        (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "                        (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "                        (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "                        (act_fn): SiLU()\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (gate): MoEGate()\n",
      "                  (shared_experts): DeepseekV2MLP(\n",
      "                    (gate_proj): KTransformersLinearLora(\n",
      "                      (orig_module): Linear(\n",
      "                        in_features=2048, out_features=2816, bias=False\n",
      "                        (prefill_linear): KLinearTorch()\n",
      "                        (generate_linear): KLinearTorch()\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Identity()\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=8, out_features=2816, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                    )\n",
      "                    (up_proj): KTransformersLinearLora(\n",
      "                      (orig_module): Linear(\n",
      "                        in_features=2048, out_features=2816, bias=False\n",
      "                        (prefill_linear): KLinearTorch()\n",
      "                        (generate_linear): KLinearTorch()\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Identity()\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=8, out_features=2816, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                    )\n",
      "                    (down_proj): KTransformersLinearLora(\n",
      "                      (orig_module): Linear(\n",
      "                        in_features=2816, out_features=2048, bias=False\n",
      "                        (prefill_linear): KLinearTorch()\n",
      "                        (generate_linear): KLinearTorch()\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Identity()\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=2816, out_features=8, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                    )\n",
      "                    (act_fn): SiLU()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (input_layernorm): DeepseekV2RMSNorm()\n",
      "              (post_attention_layernorm): DeepseekV2RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (norm): DeepseekV2RMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (lm_head): KTransformersLinear(\n",
      "        (orig_module): Linear(\n",
      "          in_features=2048, out_features=102400, bias=False\n",
      "          (prefill_linear): KLinearTorch()\n",
      "          (generate_linear): KLinearTorch()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.loader:143 >> trainable params: 6,899,712 || all params: 276,772,352 || trainable%: 2.4929\n",
      "/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/train/ksft/workflow.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = KTrainer(\n",
      "[KTrainer] Due to the placement feature in KTransformers, skip moving model to cuda:0\n",
      "[INFO|trainer.py:748] 2025-11-09 09:24:52,399 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2414] 2025-11-09 09:24:52,706 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-11-09 09:24:52,706 >>   Num examples = 91\n",
      "[INFO|trainer.py:2416] 2025-11-09 09:24:52,706 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2417] 2025-11-09 09:24:52,706 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2419] 2025-11-09 09:24:52,706 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
      "[INFO|trainer.py:2420] 2025-11-09 09:24:52,706 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2421] 2025-11-09 09:24:52,706 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2422] 2025-11-09 09:24:52,706 >>   Total optimization steps = 33\n",
      "[INFO|trainer.py:2423] 2025-11-09 09:24:52,714 >>   Number of trainable parameters = 6,899,712\n",
      "{'loss': 3.0919, 'grad_norm': 2.9972543716430664, 'learning_rate': 9.284285880837946e-05, 'epoch': 0.88}\n",
      "{'loss': 1.8397, 'grad_norm': 2.0963785648345947, 'learning_rate': 4.729305457072913e-05, 'epoch': 1.7}\n",
      "{'loss': 1.3356, 'grad_norm': 1.625698447227478, 'learning_rate': 4.621229016452156e-06, 'epoch': 2.53}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [04:07<00:00,  7.65s/it][INFO|configuration_utils.py:691] 2025-11-09 09:29:00,239 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:29:00,239 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:2681] 2025-11-09 09:29:00,404 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 247.6895, 'train_samples_per_second': 1.102, 'train_steps_per_second': 0.133, 'train_loss': 2.0244007688580137, 'epoch': 2.79}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [04:07<00:00,  7.51s/it]\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:29:01,267 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:29:01,268 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.7912\n",
      "  total_flos               =     4262GF\n",
      "  train_loss               =     2.0244\n",
      "  train_runtime            = 0:04:07.68\n",
      "  train_samples_per_second =      1.102\n",
      "  train_steps_per_second   =      0.133\n",
      "Figure saved at: /mnt/data/lpl/test_adapter/Kllama_deepseekV2/training_loss.png\n",
      "[WARNING|2025-11-09 09:29:01] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-11-09 09:29:01] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-11-09 09:29:01,437 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!USE_KT=1 llamafactory-cli train examples/train_lora/deepseek2_lora_sft_kt.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc80b189-17ac-47a7-9889-b77e7a9d5304",
   "metadata": {},
   "source": [
    "Let‚Äôs break down the training command (`USE_KT=1 llamafactory-cli train examples/train_lora/deepseek2_lora_sft_kt.yaml`):\n",
    "- `USE_KT=1`: The \"switch\" to enable KTransformers optimization.  \n",
    "- `llamafactory-cli train`: The core command to start LLaMA-Factory‚Äôs fine-tuning tool.\n",
    "- `examples/train_lora/deepseek2_lora_sft_kt.yaml`: The configuration file that controls model, data, training rules and KTransformers settings ‚Äî we‚Äôll detail this next.\n",
    "\n",
    "**The LLaMA-Factory yaml (e.g. `deepseek2_lora_sft_kt.yaml`) is where you define how the fine-tuning works.** Below is a simplified version, you can use this directly for basic tasks like style transfer or domain Q&A. And We‚Äôll explain each section‚Äôs purpose and why the values are set this way in the following part--Custom your KTransformers-FineTuning + LLaMA-Factory.\n",
    "```yaml\n",
    "### model\n",
    "model_name_or_path: deepseek-ai/DeepSeek-V2-Lite\n",
    "\n",
    "### method\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "\n",
    "### dataset\n",
    "dataset: identity\n",
    "template: deepseek\n",
    "cutoff_len: 2048\n",
    "max_samples: 100000\n",
    "\n",
    "### output\n",
    "output_dir: saves/Kllama_deepseekV2\n",
    "logging_steps: 10\n",
    "save_steps: 500\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 8\n",
    "learning_rate: 1.0e-4\n",
    "num_train_epochs: 3.0\n",
    "\n",
    "### ktransformers\n",
    "use_kt: true # use KTransformers as LoRA sft backend\n",
    "kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V2-Lite-Chat-sft-amx.yaml\n",
    "cpu_infer: 32\n",
    "chunk_size: 8192\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7722d-89dd-40b1-ac27-7ca64e80fe47",
   "metadata": {},
   "source": [
    "## Chat with the Fine-tuned Model: Test Your Customized AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af428c6-4fce-4320-b3d3-af59726ab9ce",
   "metadata": {},
   "source": [
    "After finishing fine-tuning with KTransformers, **the next step is to chat with your model and verify the results!** This step loads the original base model plus the fine-tuned \"custom plugin\" (LoRA adapter) you saved earlier, letting you interact with the model in real time.  \n",
    "\n",
    "We‚Äôll use LLaMA-Factory‚Äôs `chat` command to launch the interactive interface. The core is the LLaMA-Factory YAML configuration file ‚Äî it tells the tool which model to load, how to optimize inference, and what style of dialogue to use. We take one of the example as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37191db1-a97c-407c-9626-af9fde6dd94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no balance_serve\n",
      "flashinfer not found, use triton for linux\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,741 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,741 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,741 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,742 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,742 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,742 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:30:51,956 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:51,960 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:51,961 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:30:51,961 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:30:52,169 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,184 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,185 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:30:52,185 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|2025-11-09 09:30:52] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,192 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,193 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:30:52,193 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "using custom modeling_xxx.py.\n",
      "[WARNING|logging.py:328] 2025-11-09 09:30:52,194 >> DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "[INFO|configuration_utils.py:1142] 2025-11-09 09:30:52,195 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001\n",
      "}\n",
      "\n",
      "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
      "Injecting model.embed_tokens as default\n",
      "Injecting model.layers as default\n",
      "Injecting model.layers.0 as default\n",
      "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.0.mlp as default\n",
      "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.act_fn as default\n",
      "Injecting model.layers.0.input_layernorm as default\n",
      "Injecting model.layers.0.post_attention_layernorm as default\n",
      "Injecting model.layers.1 as default\n",
      "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.1.mlp.gate as default\n",
      "Injecting model.layers.1.mlp.shared_experts as default\n",
      "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.1.input_layernorm as default\n",
      "Injecting model.layers.1.post_attention_layernorm as default\n",
      "Injecting model.layers.2 as default\n",
      "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.2.mlp.gate as default\n",
      "Injecting model.layers.2.mlp.shared_experts as default\n",
      "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.2.input_layernorm as default\n",
      "Injecting model.layers.2.post_attention_layernorm as default\n",
      "Injecting model.layers.3 as default\n",
      "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.3.mlp.gate as default\n",
      "Injecting model.layers.3.mlp.shared_experts as default\n",
      "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.3.input_layernorm as default\n",
      "Injecting model.layers.3.post_attention_layernorm as default\n",
      "Injecting model.layers.4 as default\n",
      "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.4.mlp.gate as default\n",
      "Injecting model.layers.4.mlp.shared_experts as default\n",
      "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.4.input_layernorm as default\n",
      "Injecting model.layers.4.post_attention_layernorm as default\n",
      "Injecting model.layers.5 as default\n",
      "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.5.mlp.gate as default\n",
      "Injecting model.layers.5.mlp.shared_experts as default\n",
      "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.5.input_layernorm as default\n",
      "Injecting model.layers.5.post_attention_layernorm as default\n",
      "Injecting model.layers.6 as default\n",
      "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.6.mlp.gate as default\n",
      "Injecting model.layers.6.mlp.shared_experts as default\n",
      "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.6.input_layernorm as default\n",
      "Injecting model.layers.6.post_attention_layernorm as default\n",
      "Injecting model.layers.7 as default\n",
      "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.7.mlp.gate as default\n",
      "Injecting model.layers.7.mlp.shared_experts as default\n",
      "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.7.input_layernorm as default\n",
      "Injecting model.layers.7.post_attention_layernorm as default\n",
      "Injecting model.layers.8 as default\n",
      "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.8.mlp.gate as default\n",
      "Injecting model.layers.8.mlp.shared_experts as default\n",
      "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.8.input_layernorm as default\n",
      "Injecting model.layers.8.post_attention_layernorm as default\n",
      "Injecting model.layers.9 as default\n",
      "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.9.mlp.gate as default\n",
      "Injecting model.layers.9.mlp.shared_experts as default\n",
      "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.9.input_layernorm as default\n",
      "Injecting model.layers.9.post_attention_layernorm as default\n",
      "Injecting model.layers.10 as default\n",
      "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.10.mlp.gate as default\n",
      "Injecting model.layers.10.mlp.shared_experts as default\n",
      "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.10.input_layernorm as default\n",
      "Injecting model.layers.10.post_attention_layernorm as default\n",
      "Injecting model.layers.11 as default\n",
      "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.11.mlp.gate as default\n",
      "Injecting model.layers.11.mlp.shared_experts as default\n",
      "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.11.input_layernorm as default\n",
      "Injecting model.layers.11.post_attention_layernorm as default\n",
      "Injecting model.layers.12 as default\n",
      "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.12.mlp.gate as default\n",
      "Injecting model.layers.12.mlp.shared_experts as default\n",
      "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.12.input_layernorm as default\n",
      "Injecting model.layers.12.post_attention_layernorm as default\n",
      "Injecting model.layers.13 as default\n",
      "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.13.mlp.gate as default\n",
      "Injecting model.layers.13.mlp.shared_experts as default\n",
      "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.13.input_layernorm as default\n",
      "Injecting model.layers.13.post_attention_layernorm as default\n",
      "Injecting model.layers.14 as default\n",
      "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.14.mlp.gate as default\n",
      "Injecting model.layers.14.mlp.shared_experts as default\n",
      "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.14.input_layernorm as default\n",
      "Injecting model.layers.14.post_attention_layernorm as default\n",
      "Injecting model.layers.15 as default\n",
      "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.15.mlp.gate as default\n",
      "Injecting model.layers.15.mlp.shared_experts as default\n",
      "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.15.input_layernorm as default\n",
      "Injecting model.layers.15.post_attention_layernorm as default\n",
      "Injecting model.layers.16 as default\n",
      "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.16.mlp.gate as default\n",
      "Injecting model.layers.16.mlp.shared_experts as default\n",
      "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.16.input_layernorm as default\n",
      "Injecting model.layers.16.post_attention_layernorm as default\n",
      "Injecting model.layers.17 as default\n",
      "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.17.mlp.gate as default\n",
      "Injecting model.layers.17.mlp.shared_experts as default\n",
      "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.17.input_layernorm as default\n",
      "Injecting model.layers.17.post_attention_layernorm as default\n",
      "Injecting model.layers.18 as default\n",
      "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.18.mlp.gate as default\n",
      "Injecting model.layers.18.mlp.shared_experts as default\n",
      "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.18.input_layernorm as default\n",
      "Injecting model.layers.18.post_attention_layernorm as default\n",
      "Injecting model.layers.19 as default\n",
      "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.19.mlp.gate as default\n",
      "Injecting model.layers.19.mlp.shared_experts as default\n",
      "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.19.input_layernorm as default\n",
      "Injecting model.layers.19.post_attention_layernorm as default\n",
      "Injecting model.layers.20 as default\n",
      "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.20.mlp.gate as default\n",
      "Injecting model.layers.20.mlp.shared_experts as default\n",
      "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.20.input_layernorm as default\n",
      "Injecting model.layers.20.post_attention_layernorm as default\n",
      "Injecting model.layers.21 as default\n",
      "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.21.mlp.gate as default\n",
      "Injecting model.layers.21.mlp.shared_experts as default\n",
      "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.21.input_layernorm as default\n",
      "Injecting model.layers.21.post_attention_layernorm as default\n",
      "Injecting model.layers.22 as default\n",
      "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.22.mlp.gate as default\n",
      "Injecting model.layers.22.mlp.shared_experts as default\n",
      "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.22.input_layernorm as default\n",
      "Injecting model.layers.22.post_attention_layernorm as default\n",
      "Injecting model.layers.23 as default\n",
      "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.23.mlp.gate as default\n",
      "Injecting model.layers.23.mlp.shared_experts as default\n",
      "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.23.input_layernorm as default\n",
      "Injecting model.layers.23.post_attention_layernorm as default\n",
      "Injecting model.layers.24 as default\n",
      "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.24.mlp.gate as default\n",
      "Injecting model.layers.24.mlp.shared_experts as default\n",
      "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.24.input_layernorm as default\n",
      "Injecting model.layers.24.post_attention_layernorm as default\n",
      "Injecting model.layers.25 as default\n",
      "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.25.mlp.gate as default\n",
      "Injecting model.layers.25.mlp.shared_experts as default\n",
      "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.25.input_layernorm as default\n",
      "Injecting model.layers.25.post_attention_layernorm as default\n",
      "Injecting model.layers.26 as default\n",
      "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.26.mlp.gate as default\n",
      "Injecting model.layers.26.mlp.shared_experts as default\n",
      "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.26.input_layernorm as default\n",
      "Injecting model.layers.26.post_attention_layernorm as default\n",
      "Injecting model.norm as default\n",
      "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
      "loading model.embed_tokens.weight to cpu\n",
      "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.0.self_attn.kv_b_proj.weight to cuda:0\n",
      "loading model.layers.0.input_layernorm.weight to cuda:0\n",
      "loading model.layers.0.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.1.mlp.gate.weight to cuda:0\n",
      "loading model.layers.1.input_layernorm.weight to cuda:0\n",
      "loading model.layers.1.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.2.mlp.gate.weight to cuda:0\n",
      "loading model.layers.2.input_layernorm.weight to cuda:0\n",
      "loading model.layers.2.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.3.mlp.gate.weight to cuda:0\n",
      "loading model.layers.3.input_layernorm.weight to cuda:0\n",
      "loading model.layers.3.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.4.mlp.gate.weight to cuda:0\n",
      "loading model.layers.4.input_layernorm.weight to cuda:0\n",
      "loading model.layers.4.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.5.mlp.gate.weight to cuda:0\n",
      "loading model.layers.5.input_layernorm.weight to cuda:0\n",
      "loading model.layers.5.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.6.mlp.gate.weight to cuda:0\n",
      "loading model.layers.6.input_layernorm.weight to cuda:0\n",
      "loading model.layers.6.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.7.mlp.gate.weight to cuda:0\n",
      "loading model.layers.7.input_layernorm.weight to cuda:0\n",
      "loading model.layers.7.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.8.mlp.gate.weight to cuda:0\n",
      "loading model.layers.8.input_layernorm.weight to cuda:0\n",
      "loading model.layers.8.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.9.mlp.gate.weight to cuda:0\n",
      "loading model.layers.9.input_layernorm.weight to cuda:0\n",
      "loading model.layers.9.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.10.mlp.gate.weight to cuda:0\n",
      "loading model.layers.10.input_layernorm.weight to cuda:0\n",
      "loading model.layers.10.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.11.mlp.gate.weight to cuda:0\n",
      "loading model.layers.11.input_layernorm.weight to cuda:0\n",
      "loading model.layers.11.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.12.mlp.gate.weight to cuda:0\n",
      "loading model.layers.12.input_layernorm.weight to cuda:0\n",
      "loading model.layers.12.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.13.mlp.gate.weight to cuda:0\n",
      "loading model.layers.13.input_layernorm.weight to cuda:0\n",
      "loading model.layers.13.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.14.mlp.gate.weight to cuda:0\n",
      "loading model.layers.14.input_layernorm.weight to cuda:0\n",
      "loading model.layers.14.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.15.mlp.gate.weight to cuda:0\n",
      "loading model.layers.15.input_layernorm.weight to cuda:0\n",
      "loading model.layers.15.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.16.mlp.gate.weight to cuda:0\n",
      "loading model.layers.16.input_layernorm.weight to cuda:0\n",
      "loading model.layers.16.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.17.mlp.gate.weight to cuda:0\n",
      "loading model.layers.17.input_layernorm.weight to cuda:0\n",
      "loading model.layers.17.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.18.mlp.gate.weight to cuda:0\n",
      "loading model.layers.18.input_layernorm.weight to cuda:0\n",
      "loading model.layers.18.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.19.mlp.gate.weight to cuda:0\n",
      "loading model.layers.19.input_layernorm.weight to cuda:0\n",
      "loading model.layers.19.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.20.mlp.gate.weight to cuda:0\n",
      "loading model.layers.20.input_layernorm.weight to cuda:0\n",
      "loading model.layers.20.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.21.mlp.gate.weight to cuda:0\n",
      "loading model.layers.21.input_layernorm.weight to cuda:0\n",
      "loading model.layers.21.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.22.mlp.gate.weight to cuda:0\n",
      "loading model.layers.22.input_layernorm.weight to cuda:0\n",
      "loading model.layers.22.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.23.mlp.gate.weight to cuda:0\n",
      "loading model.layers.23.input_layernorm.weight to cuda:0\n",
      "loading model.layers.23.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.24.mlp.gate.weight to cuda:0\n",
      "loading model.layers.24.input_layernorm.weight to cuda:0\n",
      "loading model.layers.24.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.25.mlp.gate.weight to cuda:0\n",
      "loading model.layers.25.input_layernorm.weight to cuda:0\n",
      "loading model.layers.25.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.26.mlp.gate.weight to cuda:0\n",
      "loading model.layers.26.input_layernorm.weight to cuda:0\n",
      "loading model.layers.26.post_attention_layernorm.weight to cuda:0\n",
      "loading model.norm.weight to cuda:0\n",
      "[INFO|2025-11-09 09:31:03] llamafactory.model.model_utils.attention:143 >> Using vanilla attention implementation.\n",
      "lora_config:{'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'peft_type': <PeftType.LORA: 'LORA'>, 'auto_mapping': None, 'base_model_name_or_path': None, 'revision': None, 'inference_mode': True, 'r': 8, 'target_modules': {'shared_experts.up_proj', 'kv_a_proj_with_mqa', 'q_proj', 'mlp.gate_proj', 'o_proj', 'shared_experts.down_proj', 'kv_b_proj', 'mlp.down_proj', 'mlp.up_proj', 'shared_experts.gate_proj'}, 'exclude_modules': None, 'lora_alpha': 16, 'lora_dropout': 0.0, 'fan_in_fan_out': False, 'bias': 'none', 'use_rslora': False, 'modules_to_save': None, 'init_lora_weights': True, 'layers_to_transform': None, 'layers_pattern': None, 'rank_pattern': {}, 'alpha_pattern': {}, 'megatron_config': None, 'megatron_core': 'megatron.core', 'loftq_config': {}, 'eva_config': None, 'use_dora': False, 'layer_replication': None, 'runtime_config': LoraRuntimeConfig(ephemeral_gpu_offload=False), 'lora_bias': False, '_custom_modules': None}\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.mlp.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.mlp.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.mlp.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.mlp.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "[INFO|2025-11-09 09:31:03] llamafactory.model.adapter:143 >> Loaded adapter(s): /mnt/data/lpl/test_adapter/Kllama_deepseekV2\n",
      "[INFO|2025-11-09 09:31:03] llamafactory.model.loader:143 >> all params: 276,772,352\n",
      "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
      "\n",
      "User: \n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/data/lpl/anaconda3/envs/KNllama/bin/llamafactory-cli\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/cli.py\", line 24, in main\n",
      "    launcher.launch()\n",
      "  File \"/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/launcher.py\", line 144, in launch\n",
      "    run_chat()\n",
      "  File \"/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 186, in run_chat\n",
      "    query = input(\"\\nUser: \")\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli chat examples/inference/deepseek2_lora_sft_kt.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c18255-66d0-4189-a714-6050160a0637",
   "metadata": {},
   "source": [
    "To know exactly what you‚Äôre running, we break down the full command (`llamafactory-cli chat examples/inference/deepseek2_lora_sft_kt.yaml`):\n",
    "- `llamafactory-cli chat`: The core command to launch LLaMA-Factory‚Äôs interactive chat tool.\n",
    "- `examples/inference/deepseek2_lora_sft_kt.yaml`: The configuration file for inference (controls model loading, optimization, and dialogue settings).\n",
    "- No need for `USE_KT=1` here ‚Äî we‚Äôll enable KTransformers directly in the YAML (but it still needs to match the training settings!).\n",
    "\n",
    "**The LLaMA-Factory configuration file for inference (`examples/inference/deepseek2_lora_sft_kt.yaml`) controls the generate config for specific tasks.** Below is a simplified version, you can use this directly to chat with your fine-tuned model. Most setting is linked to your training config ‚Äî we‚Äôll still explain the details in next part.\n",
    "```yaml\n",
    "model_name_or_path: deepseek-ai/DeepSeek-V2-Lite\n",
    "adapter_name_or_path: saves/Kllama_deepseekV2\n",
    "template: deepseek\n",
    "infer_backend: ktransformers  # choices: [huggingface, vllm, sglang, ktransformers]\n",
    "trust_remote_code: true\n",
    "\n",
    "use_kt: true # use KTransformers as LoRA sft backend to inference\n",
    "kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V2-Lite-Chat-sft-amx.yaml\n",
    "cpu_infer: 32\n",
    "chunk_size: 8192\n",
    "```\n",
    "`kt_optimize_rule` needs as same as the kt_optimize_rule in LoRA Fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18814c5c-3b73-44cc-a608-505c1e870437",
   "metadata": {},
   "source": [
    "# **Custom your KTransformers-FineTuning + LLaMA-Factory**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8072427f-46d4-41fb-8850-e33a2446e031",
   "metadata": {},
   "source": [
    "Once you‚Äôve got the basic fine-tuning workflow down, you‚Äôll likely want to **adapt the process to your specific needs**‚Äîwhether that‚Äôs training on your own data, squeezing more performance out of limited GPU memory, or speeding up training for large datasets. Below‚Äôs a hands-on guide to customizing every part of the process, with clear explanations of why each setting matters and how to tweak it.\n",
    "\n",
    "## 1. Fine-tuning Customization: Tailor Training to Your Needs  \n",
    "To start customizing, you‚Äôll still use the core training command: `USE_KT=1 llamafactory-cli train examples/train_lora/deepseek2_lora_sft_kt.yaml`. Notably, it performs even better than the default setup when adapted to your specific needs. <br/>\n",
    "### Full example **LLaMA-Factory YAML** for DeepSeek-V2-Lite\n",
    "```yaml\n",
    "### model\n",
    "model_name_or_path: deepseek-ai/DeepSeek-V2-Lite\n",
    "trust_remote_code: true\n",
    "\n",
    "### method\n",
    "stage: sft\n",
    "do_train: true\n",
    "finetuning_type: lora\n",
    "lora_rank: 8\n",
    "lora_target: all\n",
    "\n",
    "### dataset\n",
    "dataset: identity\n",
    "template: deepseek\n",
    "cutoff_len: 2048\n",
    "max_samples: 100000\n",
    "overwrite_cache: true\n",
    "preprocessing_num_workers: 16\n",
    "dataloader_num_workers: 4\n",
    "\n",
    "### output\n",
    "output_dir: saves/Kllama_deepseekV2Lite\n",
    "logging_steps: 10\n",
    "save_steps: 500\n",
    "plot_loss: true\n",
    "overwrite_output_dir: true\n",
    "save_only_model: false\n",
    "report_to: none  # choices: [none, wandb, tensorboard, swanlab, mlflow]\n",
    "\n",
    "### train\n",
    "per_device_train_batch_size: 1\n",
    "gradient_accumulation_steps: 8\n",
    "learning_rate: 1.0e-4\n",
    "num_train_epochs: 3.0\n",
    "lr_scheduler_type: cosine\n",
    "warmup_ratio: 0.1\n",
    "bf16: true\n",
    "ddp_timeout: 180000000\n",
    "resume_from_checkpoint: null\n",
    "\n",
    "### ktransformers\n",
    "use_kt: true # use KTransformers as LoRA sft backend\n",
    "kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V2-Chat-sft-amx.yaml\n",
    "cpu_infer: 32\n",
    "chunk_size: 8192\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abc1968-6208-4344-9c82-335d7fe1d27c",
   "metadata": {},
   "source": [
    "---\n",
    "### A. Pick & Prepare Your Model\n",
    "The first step in customization is choosing the right base model, and ensuring it works with KTransformers. The `model_name_or_path` setting (shown in LLaMA-Factory YAML before) controls this, and getting it right avoids common errors.\n",
    "- **Use a public model**: Directly set to Hugging Face Hub names (e.g., `deepseek-ai/DeepSeek-V2-Lite`, `Qwen/Qwen2-MoE-72B`).  \n",
    "- **Use a local model**: Replace with your local folder path (e.g., `/mnt/data/models/DeepSeek-V2-Lite`).\n",
    "\n",
    "**Critical Requirement**: The model must be in **BF16 format**.  \n",
    "  - FP8 models (like DeepSeek-V3‚Äôs default release) aren‚Äôt compatible with KTransformers‚Äô optimization.  \n",
    "  - Fix: Convert FP8 to BF16 with **[this official script](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/fp8_cast_bf16.py)**.\n",
    "\n",
    "---\n",
    "\n",
    "### B. Tune LoRA: Balance Fitting Capability & Memory  \n",
    "LoRA trains tiny \"adapter\" weights instead of the entire model. Tweaking these two settings in LLaMA-Factory YAML (`lora_rank`, `lora_target`) lets you balance how well the model learns your data and how much GPU memory it uses:\n",
    "\n",
    "| Setting         | What it does                                                                 | Scenario & Recommendation                                                                 |\n",
    "|-----------------|-----------------------------------------------------------------------------|-------------------------------------------------------------------------------------------|\n",
    "| `lora_rank`     | Controls the \"power\" of LoRA adapters (higher = more fitting, more memory). | - Small dataset (‚â§5k samples) or limited GPU: 4-8 (balances speed/memory).<br>- Large dataset (‚â•20k samples): 16-32 (better fits custom data). |\n",
    "| `lora_target`   | Which layers get LoRA (applies only to linear layers).                      | - Quick fine-tuning (e.g., style transfer): `q_proj,v_proj` (only attention layers‚Äîfaster).<br>- Deep customization (e.g., medical Q&A): `all` (all linear layers‚Äîmore accurate). |\n",
    "\n",
    "**Tip**: Pair `lora_rank=8` with `lora_alpha=32` (alpha = 4√ó rank) for stable training This ratio is tested to work well for most tasks, from chatbots to domain Q&A.  \n",
    "\n",
    "---\n",
    "\n",
    "### C. Use Your Own Dataset\n",
    "Fine-tuning‚Äôs value lies in training on your own data, such as company documents, customer support logs, or domain-specific Q&A. Below is how to replace the default (identity) dataset with yours:  \n",
    "\n",
    "1. **Add a custom dataset**:  \n",
    "   - Step 1: Organize your data into LLaMA-Factory‚Äôs format (e.g., JSON with `instruction`, `input`, `output` fields‚Äîsee [dataset examples](https://github.com/hiyouga/LLaMA-Factory/tree/main/data)).  \n",
    "   - Step 2: Register your dataset in [LLaMA-Factory/data/dataset_info.json](https://github.com/hiyouga/LLaMA-Factory/blob/main/data/dataset_info.json) (copy the format of built-in datasets‚Äîjust add your dataset name and file path).\n",
    "     For example,\n",
    "     ```json\n",
    "     \"niko\": {\n",
    "        \"file_name\": \"../niko_train.json\"\n",
    "      },\n",
    "      ```\n",
    "   - Step 3: You may replace `dataset: identity` in LLaMA-Factory YAML to your dataset name (e.g. `dataset: niko`).\n",
    "2. **Tweak dataset settings for better results**:  \n",
    "   - `cutoff_len`: Truncates long texts (e.g., set to 4096 for long documents, 2048 for short dialogues‚Äînever exceed `model_max_length`).  \n",
    "   - `max_samples`: Limit samples to avoid overfitting (use 100 for debugging, `None` for full training‚Äîgreat if your dataset is huge).  \n",
    "   - `template`: Must match your model (e.g., `deepseek` for DeepSeek, `llama3` for LLaMA3, more refer to [supported-models](https://github.com/hiyouga/LLaMA-Factory/tree/main?tab=readme-ov-file#supported-models))‚Äîmismatched templates break response formatting!  \n",
    "\n",
    "---\n",
    "\n",
    "### D. Save GPU Memory & Speed Up Training  \n",
    "If you‚Äôre hitting GPU memory limits or waiting too long for training, adjust these settings in LLaMA-Factory YAML:  \n",
    "\n",
    "| Challenge               | Setting to Tweak                          | How to Adjust                                                                 |\n",
    "|-------------------------|-------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| GPU memory is tight     | `per_device_train_batch_size` + `gradient_accumulation_steps` | Set `per_device_train_batch_size=1` (smallest batch) + `gradient_accumulation_steps=16` (simulates a batch of 16‚Äîno memory penalty!). |\n",
    "| Model overfits (bad generalization) | `lora_dropout` + `num_train_epochs` | Add `lora_dropout: 0.1` (prevents overfitting) + reduce `num_train_epochs` to 2 (3 is default‚Äîovertraining hurts!). |\n",
    "\n",
    "**Key Train Configs Recap**:  \n",
    "- `learning_rate`: 1e-4~2e-4 for LoRA (stick to this range‚Äîtoo high = unstable, too low = slow learning).  \n",
    "- `save_steps`: Save checkpoints every 100-500 steps (frequent saves = safe, but don‚Äôt overdo it‚Äîeach checkpoint takes storage!).  \n",
    "- `output_dir`: Customize the save path (e.g., `saves/medical_qa_deepseek` instead of the default‚Äîkeeps your projects organized!).  \n",
    "\n",
    "---\n",
    "\n",
    "### E. KTransformers Optimization: Unlock Maximum Performance  \n",
    "KTransformers is what makes fine-tuning large models (like 671B-parameter DeepSeek-V3) possible on modest hardware. These settings control how it optimizes layer placement (GPU vs. CPU) and computation speed:\n",
    "\n",
    "| Setting               | What it does                                                                 | How to Customize                                                                 |\n",
    "|-----------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------------------|\n",
    "| `use_kt`              | Enables KTransformers backend (must be `true`‚Äîotherwise, no optimization!). | Leave as `true`‚Äîthis is what makes 671B models trainable on 2√ó4090s!             |\n",
    "| `cpu_infer`           | Number of CPU threads for MoE/linear computations.                          | Set to half your CPU cores (e.g., 32 for a 64-core CPU‚Äîtoo many threads = bottlenecks!). |\n",
    "| `chunk_size`          | Block size for long text processing (affects memory and speed).             | Default 8192 works for most tasks; increase to 16384 for extra-long texts (e.g., book summaries). |\n",
    "| `kt_optimize_rule`    | Defines where layers run (GPU/CPU) and which kernels to use (core of KT!).  | - Use the pre-built rule for your model (e.g., `DeepSeek-V2-Lite-Chat-sft-amx.yaml`).<br>- For faster speed: Use `AMXInt8`/`AMXBF16` as backend (if your CPU supports AMX‚Äîcheck with `lscpu | grep amx`).<br>- For compatibility: Fall back to `llamafile` if AMX isn‚Äôt supported. |\n",
    "\n",
    "#### Example Custom `kt_optimize_rule` (shown in the table above)  \n",
    "This rule tells KTransformers to offload heavy MoE layers to the CPU (saving GPU memory) and use AMX for fast CPU computation. Use it as a template for your own model: (Details tutorial could be seen in **[here](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/injection_tutorial.md)**)\n",
    "```yaml\n",
    "- match:\n",
    "    name: \"^model\\\\.layers\\\\..*\\\\.mlp\\\\.experts$\"  # Target all MoE expert layers\n",
    "  replace:\n",
    "    class: ktransformers.operators.experts.KTransformersExperts  # KT's optimized MoE kernel\n",
    "    kwargs:\n",
    "      prefill_device: \"cuda\"  # Fast pre-processing on GPU\n",
    "      prefill_op: \"KExpertsTorch\"\n",
    "      generate_device: \"cpu\"  # Heavy MoE compute on CPU (saves GPU memory)\n",
    "      generate_op: \"KSFTExpertsCPU\"  # KT's SFT-optimized MoE operator\n",
    "      out_device: \"cuda\"  # Send results back to GPU for next steps\n",
    "      backend: \"AMXInt8\"  # Options: AMXInt8 (fastest) > AMXBF16 > llamafile (default)\n",
    "```\n",
    "**Alert:** Never mix KLinearMarlin with LoRA fine-tuning‚Äîreplace it with KLinearTorch (as in the example) to avoid compatibility issues!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93840117-084b-44fa-8b2e-6389e4a52bf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no balance_serve\n",
      "flashinfer not found, use triton for linux\n",
      "/mnt/data/lpl/anaconda3/envs/KNllama/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "[INFO|2025-11-09 09:24:33] llamafactory.hparams.parser:468 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,692 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:24:33,908 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:33,912 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:33,914 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:24:33,914 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:24:33,915 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:24:34,116 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-11-09 09:24:34] llamafactory.data.loader:143 >> Loading dataset identity.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 91 examples [00:00, 14003.07 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|‚ñà| 91/91 [00:00<00:00, 271.03 e\n",
      "Running tokenizer on dataset (num_proc=16): 100%|‚ñà| 91/91 [00:01<00:00, 68.21 ex\n",
      "training example:\n",
      "input_ids:\n",
      "[100000, 5726, 25, 2868, 185, 185, 77398, 25, 17464, 0, 304, 608, 17247, 1531, 11288, 274, 20838, 20308, 6907, 457, 17247, 9425, 20384, 1724, 481, 304, 4750, 340, 3571, 30, 100001]\n",
      "inputs:\n",
      "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>User: hi\n",
      "\n",
      "Assistant:Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, 17464, 0, 304, 608, 17247, 1531, 11288, 274, 20838, 20308, 6907, 457, 17247, 9425, 20384, 1724, 481, 304, 4750, 340, 3571, 30, 100001]\n",
      "labels:\n",
      "Hello! I am {{name}}, an AI assistant developed by {{author}}. How can I assist you today?<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,065 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,067 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:24:37,067 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|2025-11-09 09:24:37] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,077 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:24:37,078 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:24:37,079 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "using custom modeling_xxx.py.\n",
      "[WARNING|logging.py:328] 2025-11-09 09:24:37,080 >> DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "[INFO|configuration_utils.py:1142] 2025-11-09 09:24:37,080 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001\n",
      "}\n",
      "\n",
      "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
      "Injecting model.embed_tokens as default\n",
      "Injecting model.layers as default\n",
      "Injecting model.layers.0 as default\n",
      "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.0.mlp as default\n",
      "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.act_fn as default\n",
      "Injecting model.layers.0.input_layernorm as default\n",
      "Injecting model.layers.0.post_attention_layernorm as default\n",
      "Injecting model.layers.1 as default\n",
      "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.1.mlp.gate as default\n",
      "Injecting model.layers.1.mlp.shared_experts as default\n",
      "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.1.input_layernorm as default\n",
      "Injecting model.layers.1.post_attention_layernorm as default\n",
      "Injecting model.layers.2 as default\n",
      "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.2.mlp.gate as default\n",
      "Injecting model.layers.2.mlp.shared_experts as default\n",
      "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.2.input_layernorm as default\n",
      "Injecting model.layers.2.post_attention_layernorm as default\n",
      "Injecting model.layers.3 as default\n",
      "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.3.mlp.gate as default\n",
      "Injecting model.layers.3.mlp.shared_experts as default\n",
      "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.3.input_layernorm as default\n",
      "Injecting model.layers.3.post_attention_layernorm as default\n",
      "Injecting model.layers.4 as default\n",
      "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.4.mlp.gate as default\n",
      "Injecting model.layers.4.mlp.shared_experts as default\n",
      "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.4.input_layernorm as default\n",
      "Injecting model.layers.4.post_attention_layernorm as default\n",
      "Injecting model.layers.5 as default\n",
      "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.5.mlp.gate as default\n",
      "Injecting model.layers.5.mlp.shared_experts as default\n",
      "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.5.input_layernorm as default\n",
      "Injecting model.layers.5.post_attention_layernorm as default\n",
      "Injecting model.layers.6 as default\n",
      "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.6.mlp.gate as default\n",
      "Injecting model.layers.6.mlp.shared_experts as default\n",
      "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.6.input_layernorm as default\n",
      "Injecting model.layers.6.post_attention_layernorm as default\n",
      "Injecting model.layers.7 as default\n",
      "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.7.mlp.gate as default\n",
      "Injecting model.layers.7.mlp.shared_experts as default\n",
      "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.7.input_layernorm as default\n",
      "Injecting model.layers.7.post_attention_layernorm as default\n",
      "Injecting model.layers.8 as default\n",
      "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.8.mlp.gate as default\n",
      "Injecting model.layers.8.mlp.shared_experts as default\n",
      "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.8.input_layernorm as default\n",
      "Injecting model.layers.8.post_attention_layernorm as default\n",
      "Injecting model.layers.9 as default\n",
      "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.9.mlp.gate as default\n",
      "Injecting model.layers.9.mlp.shared_experts as default\n",
      "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.9.input_layernorm as default\n",
      "Injecting model.layers.9.post_attention_layernorm as default\n",
      "Injecting model.layers.10 as default\n",
      "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.10.mlp.gate as default\n",
      "Injecting model.layers.10.mlp.shared_experts as default\n",
      "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.10.input_layernorm as default\n",
      "Injecting model.layers.10.post_attention_layernorm as default\n",
      "Injecting model.layers.11 as default\n",
      "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.11.mlp.gate as default\n",
      "Injecting model.layers.11.mlp.shared_experts as default\n",
      "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.11.input_layernorm as default\n",
      "Injecting model.layers.11.post_attention_layernorm as default\n",
      "Injecting model.layers.12 as default\n",
      "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.12.mlp.gate as default\n",
      "Injecting model.layers.12.mlp.shared_experts as default\n",
      "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.12.input_layernorm as default\n",
      "Injecting model.layers.12.post_attention_layernorm as default\n",
      "Injecting model.layers.13 as default\n",
      "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.13.mlp.gate as default\n",
      "Injecting model.layers.13.mlp.shared_experts as default\n",
      "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.13.input_layernorm as default\n",
      "Injecting model.layers.13.post_attention_layernorm as default\n",
      "Injecting model.layers.14 as default\n",
      "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.14.mlp.gate as default\n",
      "Injecting model.layers.14.mlp.shared_experts as default\n",
      "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.14.input_layernorm as default\n",
      "Injecting model.layers.14.post_attention_layernorm as default\n",
      "Injecting model.layers.15 as default\n",
      "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.15.mlp.gate as default\n",
      "Injecting model.layers.15.mlp.shared_experts as default\n",
      "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.15.input_layernorm as default\n",
      "Injecting model.layers.15.post_attention_layernorm as default\n",
      "Injecting model.layers.16 as default\n",
      "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.16.mlp.gate as default\n",
      "Injecting model.layers.16.mlp.shared_experts as default\n",
      "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.16.input_layernorm as default\n",
      "Injecting model.layers.16.post_attention_layernorm as default\n",
      "Injecting model.layers.17 as default\n",
      "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.17.mlp.gate as default\n",
      "Injecting model.layers.17.mlp.shared_experts as default\n",
      "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.17.input_layernorm as default\n",
      "Injecting model.layers.17.post_attention_layernorm as default\n",
      "Injecting model.layers.18 as default\n",
      "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.18.mlp.gate as default\n",
      "Injecting model.layers.18.mlp.shared_experts as default\n",
      "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.18.input_layernorm as default\n",
      "Injecting model.layers.18.post_attention_layernorm as default\n",
      "Injecting model.layers.19 as default\n",
      "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.19.mlp.gate as default\n",
      "Injecting model.layers.19.mlp.shared_experts as default\n",
      "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.19.input_layernorm as default\n",
      "Injecting model.layers.19.post_attention_layernorm as default\n",
      "Injecting model.layers.20 as default\n",
      "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.20.mlp.gate as default\n",
      "Injecting model.layers.20.mlp.shared_experts as default\n",
      "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.20.input_layernorm as default\n",
      "Injecting model.layers.20.post_attention_layernorm as default\n",
      "Injecting model.layers.21 as default\n",
      "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.21.mlp.gate as default\n",
      "Injecting model.layers.21.mlp.shared_experts as default\n",
      "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.21.input_layernorm as default\n",
      "Injecting model.layers.21.post_attention_layernorm as default\n",
      "Injecting model.layers.22 as default\n",
      "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.22.mlp.gate as default\n",
      "Injecting model.layers.22.mlp.shared_experts as default\n",
      "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.22.input_layernorm as default\n",
      "Injecting model.layers.22.post_attention_layernorm as default\n",
      "Injecting model.layers.23 as default\n",
      "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.23.mlp.gate as default\n",
      "Injecting model.layers.23.mlp.shared_experts as default\n",
      "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.23.input_layernorm as default\n",
      "Injecting model.layers.23.post_attention_layernorm as default\n",
      "Injecting model.layers.24 as default\n",
      "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.24.mlp.gate as default\n",
      "Injecting model.layers.24.mlp.shared_experts as default\n",
      "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.24.input_layernorm as default\n",
      "Injecting model.layers.24.post_attention_layernorm as default\n",
      "Injecting model.layers.25 as default\n",
      "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.25.mlp.gate as default\n",
      "Injecting model.layers.25.mlp.shared_experts as default\n",
      "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.25.input_layernorm as default\n",
      "Injecting model.layers.25.post_attention_layernorm as default\n",
      "Injecting model.layers.26 as default\n",
      "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.26.mlp.gate as default\n",
      "Injecting model.layers.26.mlp.shared_experts as default\n",
      "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.26.input_layernorm as default\n",
      "Injecting model.layers.26.post_attention_layernorm as default\n",
      "Injecting model.norm as default\n",
      "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
      "loading model.embed_tokens.weight to cpu\n",
      "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.0.self_attn.kv_b_proj.weight to cuda:0\n",
      "loading model.layers.0.input_layernorm.weight to cuda:0\n",
      "loading model.layers.0.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.1.mlp.gate.weight to cuda:0\n",
      "loading model.layers.1.input_layernorm.weight to cuda:0\n",
      "loading model.layers.1.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.2.mlp.gate.weight to cuda:0\n",
      "loading model.layers.2.input_layernorm.weight to cuda:0\n",
      "loading model.layers.2.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.3.mlp.gate.weight to cuda:0\n",
      "loading model.layers.3.input_layernorm.weight to cuda:0\n",
      "loading model.layers.3.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.4.mlp.gate.weight to cuda:0\n",
      "loading model.layers.4.input_layernorm.weight to cuda:0\n",
      "loading model.layers.4.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.5.mlp.gate.weight to cuda:0\n",
      "loading model.layers.5.input_layernorm.weight to cuda:0\n",
      "loading model.layers.5.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.6.mlp.gate.weight to cuda:0\n",
      "loading model.layers.6.input_layernorm.weight to cuda:0\n",
      "loading model.layers.6.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.7.mlp.gate.weight to cuda:0\n",
      "loading model.layers.7.input_layernorm.weight to cuda:0\n",
      "loading model.layers.7.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.8.mlp.gate.weight to cuda:0\n",
      "loading model.layers.8.input_layernorm.weight to cuda:0\n",
      "loading model.layers.8.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.9.mlp.gate.weight to cuda:0\n",
      "loading model.layers.9.input_layernorm.weight to cuda:0\n",
      "loading model.layers.9.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.10.mlp.gate.weight to cuda:0\n",
      "loading model.layers.10.input_layernorm.weight to cuda:0\n",
      "loading model.layers.10.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.11.mlp.gate.weight to cuda:0\n",
      "loading model.layers.11.input_layernorm.weight to cuda:0\n",
      "loading model.layers.11.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.12.mlp.gate.weight to cuda:0\n",
      "loading model.layers.12.input_layernorm.weight to cuda:0\n",
      "loading model.layers.12.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.13.mlp.gate.weight to cuda:0\n",
      "loading model.layers.13.input_layernorm.weight to cuda:0\n",
      "loading model.layers.13.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.14.mlp.gate.weight to cuda:0\n",
      "loading model.layers.14.input_layernorm.weight to cuda:0\n",
      "loading model.layers.14.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.15.mlp.gate.weight to cuda:0\n",
      "loading model.layers.15.input_layernorm.weight to cuda:0\n",
      "loading model.layers.15.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.16.mlp.gate.weight to cuda:0\n",
      "loading model.layers.16.input_layernorm.weight to cuda:0\n",
      "loading model.layers.16.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.17.mlp.gate.weight to cuda:0\n",
      "loading model.layers.17.input_layernorm.weight to cuda:0\n",
      "loading model.layers.17.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.18.mlp.gate.weight to cuda:0\n",
      "loading model.layers.18.input_layernorm.weight to cuda:0\n",
      "loading model.layers.18.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.19.mlp.gate.weight to cuda:0\n",
      "loading model.layers.19.input_layernorm.weight to cuda:0\n",
      "loading model.layers.19.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.20.mlp.gate.weight to cuda:0\n",
      "loading model.layers.20.input_layernorm.weight to cuda:0\n",
      "loading model.layers.20.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.21.mlp.gate.weight to cuda:0\n",
      "loading model.layers.21.input_layernorm.weight to cuda:0\n",
      "loading model.layers.21.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.22.mlp.gate.weight to cuda:0\n",
      "loading model.layers.22.input_layernorm.weight to cuda:0\n",
      "loading model.layers.22.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.23.mlp.gate.weight to cuda:0\n",
      "loading model.layers.23.input_layernorm.weight to cuda:0\n",
      "loading model.layers.23.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.24.mlp.gate.weight to cuda:0\n",
      "loading model.layers.24.input_layernorm.weight to cuda:0\n",
      "loading model.layers.24.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.25.mlp.gate.weight to cuda:0\n",
      "loading model.layers.25.input_layernorm.weight to cuda:0\n",
      "loading model.layers.25.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.26.mlp.gate.weight to cuda:0\n",
      "loading model.layers.26.input_layernorm.weight to cuda:0\n",
      "loading model.layers.26.post_attention_layernorm.weight to cuda:0\n",
      "loading model.norm.weight to cuda:0\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.model_utils.attention:143 >> Using vanilla attention implementation.\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.model_utils.misc:143 >> Found linear modules: down_proj,kv_a_proj_with_mqa,o_proj,q_proj,up_proj,orig_module,kv_b_proj,gate_proj,generate_linear,prefill_linear\n",
      "KT_model:PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): DeepseekV2ForCausalLM(\n",
      "      (model): KDeepseekV2Model(\n",
      "        (orig_module): DeepseekV2Model(\n",
      "          (embed_tokens): Embedding(102400, 2048)\n",
      "          (layers): ModuleList(\n",
      "            (0): DeepseekV2DecoderLayer(\n",
      "              (self_attn): KDeepseekV2Attention(\n",
      "                (orig_module): DeepseekV2Attention(\n",
      "                  (q_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=3072, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_proj_with_mqa): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=576, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=576, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_layernorm): DeepseekV2RMSNorm()\n",
      "                  (kv_b_proj): lora.Linear(\n",
      "                    (orig_module): Linear(in_features=512, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=2048, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (rotary_emb): YarnRotaryEmbedding(\n",
      "                    (orig_module): DeepseekV2YarnRotaryEmbedding()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (mlp): DeepseekV2MLP(\n",
      "                (gate_proj): KTransformersLinearLora(\n",
      "                  (orig_module): Linear(\n",
      "                    in_features=2048, out_features=10944, bias=False\n",
      "                    (prefill_linear): KLinearTorch()\n",
      "                    (generate_linear): KLinearTorch()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=10944, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (up_proj): KTransformersLinearLora(\n",
      "                  (orig_module): Linear(\n",
      "                    in_features=2048, out_features=10944, bias=False\n",
      "                    (prefill_linear): KLinearTorch()\n",
      "                    (generate_linear): KLinearTorch()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=10944, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (down_proj): KTransformersLinearLora(\n",
      "                  (orig_module): Linear(\n",
      "                    in_features=10944, out_features=2048, bias=False\n",
      "                    (prefill_linear): KLinearTorch()\n",
      "                    (generate_linear): KLinearTorch()\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=10944, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                )\n",
      "                (act_fn): SiLU()\n",
      "              )\n",
      "              (input_layernorm): DeepseekV2RMSNorm()\n",
      "              (post_attention_layernorm): DeepseekV2RMSNorm()\n",
      "            )\n",
      "            (1-26): 26 x DeepseekV2DecoderLayer(\n",
      "              (self_attn): KDeepseekV2Attention(\n",
      "                (orig_module): DeepseekV2Attention(\n",
      "                  (q_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=3072, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=3072, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_proj_with_mqa): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=576, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=576, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (kv_a_layernorm): DeepseekV2RMSNorm()\n",
      "                  (kv_b_proj): lora.Linear(\n",
      "                    (orig_module): Linear(in_features=512, out_features=4096, bias=False)\n",
      "                    (lora_dropout): ModuleDict(\n",
      "                      (default): Identity()\n",
      "                    )\n",
      "                    (lora_A): ModuleDict(\n",
      "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
      "                    )\n",
      "                    (lora_B): ModuleDict(\n",
      "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                    )\n",
      "                    (lora_embedding_A): ParameterDict()\n",
      "                    (lora_embedding_B): ParameterDict()\n",
      "                    (lora_magnitude_vector): ModuleDict()\n",
      "                  )\n",
      "                  (o_proj): KTransformersLinearLora(\n",
      "                    (orig_module): Linear(\n",
      "                      in_features=2048, out_features=2048, bias=False\n",
      "                      (prefill_linear): KLinearTorch()\n",
      "                      (generate_linear): KLinearTorch()\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Identity()\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                  )\n",
      "                  (rotary_emb): YarnRotaryEmbedding(\n",
      "                    (orig_module): DeepseekV2YarnRotaryEmbedding()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (mlp): KDeepseekV2MoE(\n",
      "                (orig_module): DeepseekV2MoE(\n",
      "                  (experts): KTransformersExperts(\n",
      "                    (orig_module): ModuleList(\n",
      "                      (0-63): 64 x DeepseekV2MLP(\n",
      "                        (gate_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "                        (up_proj): Linear(in_features=2048, out_features=1408, bias=False)\n",
      "                        (down_proj): Linear(in_features=1408, out_features=2048, bias=False)\n",
      "                        (act_fn): SiLU()\n",
      "                      )\n",
      "                    )\n",
      "                  )\n",
      "                  (gate): MoEGate()\n",
      "                  (shared_experts): DeepseekV2MLP(\n",
      "                    (gate_proj): KTransformersLinearLora(\n",
      "                      (orig_module): Linear(\n",
      "                        in_features=2048, out_features=2816, bias=False\n",
      "                        (prefill_linear): KLinearTorch()\n",
      "                        (generate_linear): KLinearTorch()\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Identity()\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=8, out_features=2816, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                    )\n",
      "                    (up_proj): KTransformersLinearLora(\n",
      "                      (orig_module): Linear(\n",
      "                        in_features=2048, out_features=2816, bias=False\n",
      "                        (prefill_linear): KLinearTorch()\n",
      "                        (generate_linear): KLinearTorch()\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Identity()\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=8, out_features=2816, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                    )\n",
      "                    (down_proj): KTransformersLinearLora(\n",
      "                      (orig_module): Linear(\n",
      "                        in_features=2816, out_features=2048, bias=False\n",
      "                        (prefill_linear): KLinearTorch()\n",
      "                        (generate_linear): KLinearTorch()\n",
      "                        (lora_dropout): ModuleDict(\n",
      "                          (default): Identity()\n",
      "                        )\n",
      "                        (lora_A): ModuleDict(\n",
      "                          (default): Linear(in_features=2816, out_features=8, bias=False)\n",
      "                        )\n",
      "                        (lora_B): ModuleDict(\n",
      "                          (default): Linear(in_features=8, out_features=2048, bias=False)\n",
      "                        )\n",
      "                        (lora_embedding_A): ParameterDict()\n",
      "                        (lora_embedding_B): ParameterDict()\n",
      "                        (lora_magnitude_vector): ModuleDict()\n",
      "                      )\n",
      "                    )\n",
      "                    (act_fn): SiLU()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "              (input_layernorm): DeepseekV2RMSNorm()\n",
      "              (post_attention_layernorm): DeepseekV2RMSNorm()\n",
      "            )\n",
      "          )\n",
      "          (norm): DeepseekV2RMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (lm_head): KTransformersLinear(\n",
      "        (orig_module): Linear(\n",
      "          in_features=2048, out_features=102400, bias=False\n",
      "          (prefill_linear): KLinearTorch()\n",
      "          (generate_linear): KLinearTorch()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO|2025-11-09 09:24:52] llamafactory.model.loader:143 >> trainable params: 6,899,712 || all params: 276,772,352 || trainable%: 2.4929\n",
      "/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/train/ksft/workflow.py:77: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = KTrainer(\n",
      "[KTrainer] Due to the placement feature in KTransformers, skip moving model to cuda:0\n",
      "[INFO|trainer.py:748] 2025-11-09 09:24:52,399 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2414] 2025-11-09 09:24:52,706 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-11-09 09:24:52,706 >>   Num examples = 91\n",
      "[INFO|trainer.py:2416] 2025-11-09 09:24:52,706 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2417] 2025-11-09 09:24:52,706 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2419] 2025-11-09 09:24:52,706 >>   Training with DataParallel so batch size has been adjusted to: 8\n",
      "[INFO|trainer.py:2420] 2025-11-09 09:24:52,706 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:2421] 2025-11-09 09:24:52,706 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2422] 2025-11-09 09:24:52,706 >>   Total optimization steps = 33\n",
      "[INFO|trainer.py:2423] 2025-11-09 09:24:52,714 >>   Number of trainable parameters = 6,899,712\n",
      "{'loss': 3.0919, 'grad_norm': 2.9972543716430664, 'learning_rate': 9.284285880837946e-05, 'epoch': 0.88}\n",
      "{'loss': 1.8397, 'grad_norm': 2.0963785648345947, 'learning_rate': 4.729305457072913e-05, 'epoch': 1.7}\n",
      "{'loss': 1.3356, 'grad_norm': 1.625698447227478, 'learning_rate': 4.621229016452156e-06, 'epoch': 2.53}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [04:07<00:00,  7.65s/it][INFO|configuration_utils.py:691] 2025-11-09 09:29:00,239 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:29:00,239 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|trainer.py:2681] 2025-11-09 09:29:00,404 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 247.6895, 'train_samples_per_second': 1.102, 'train_steps_per_second': 0.133, 'train_loss': 2.0244007688580137, 'epoch': 2.79}\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 33/33 [04:07<00:00,  7.51s/it]\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:29:01,267 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:29:01,268 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.7912\n",
      "  total_flos               =     4262GF\n",
      "  train_loss               =     2.0244\n",
      "  train_runtime            = 0:04:07.68\n",
      "  train_samples_per_second =      1.102\n",
      "  train_steps_per_second   =      0.133\n",
      "Figure saved at: /mnt/data/lpl/test_adapter/Kllama_deepseekV2/training_loss.png\n",
      "[WARNING|2025-11-09 09:29:01] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-11-09 09:29:01] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-11-09 09:29:01,437 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!USE_KT=1 llamafactory-cli train examples/train_lora/deepseek2_lora_sft_kt.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d0b4db-65f7-4683-88d0-3269c962224c",
   "metadata": {},
   "source": [
    "## 2. Chat with the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc5e95-9567-4b8a-94d7-eec410d94a6b",
   "metadata": {},
   "source": [
    "After completing fine-tuning, the next critical step is to test your customized model through real-time interaction. Running `llamafactory-cli chat examples/inference/deepseek2_lora_sft_kt.yaml` loads the base model and your fine-tuned LoRA adapter. Below‚Äôs a detailed guide to customizing the chat process, with clear explanations of each setting‚Äôs role and how to fit it to your specific tasks.\n",
    "\n",
    "### Full example LLaMA-Factory YAML for inference\n",
    "```yaml\n",
    "model_name_or_path: deepseek-ai/DeepSeek-V2-Lite\n",
    "adapter_name_or_path: saves/Kllama_deepseekV2Lite\n",
    "template: deepseek\n",
    "infer_backend: ktransformers  # choices: [huggingface, vllm, sglang, ktransformers]\n",
    "trust_remote_code: true\n",
    "\n",
    "use_kt: true # use KTransformers as LoRA sft backend to inference\n",
    "kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V2-Chat-sft-amx.yaml\n",
    "cpu_infer: 32\n",
    "chunk_size: 8192\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### A. Load Your Fine-Tuned Adapter (Two Supported Formats)  \n",
    "The `adapter_name_or_path` setting in LLaMA-Factory YAML points to your trained LoRA weights. Two formats are supported:  \n",
    "- **Folder Format (Default)**: If training saved a folder (e.g., `saves/Kllama_deepseekV2`) with `.safetensors` files, set it directly (e.g., `adapter_name_or_path: saves/Kllama_deepseekV2`).  \n",
    "- **GGUF Format (Single File)**: If you exported the adapter to a `.gguf` file (for portability), set the full path (e.g., `adapter_name_or_path: saves/my_adapter.gguf`).  \n",
    "\n",
    "---\n",
    "\n",
    "### B. Tweak Response Quality (Generation Configs)  \n",
    "Optional generation parameters let you adjust the model‚Äôs responses to fit specific use cases, whether you need factual accuracy, creative expression, or concise answers. Add these to your YAML and modify based on your needs:\n",
    "```yaml\n",
    "# Optional generation configs (add to your inference YAML)\n",
    "max_new_tokens: 1024  # Max length of responses (512 = short, 2048 = long)\n",
    "temperature: 0.7      # Randomness (0.1 = factual/consistent, 1.0 = creative/diverse)\n",
    "top_p: 0.9            # Focus (0.8-0.95 = avoids irrelevant content)\n",
    "repetition_penalty: 1.1  # Reduces repetition (1.0 = no penalty, 1.2 = strict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### C. KTransformers Inference Backend  \n",
    "The KTransformers-related settings directly impact inference performance‚Äîthey must align with your training configuration to maintain optimization effects (e.g., low memory usage, fast speed):\n",
    "- `infer_backend` determines how the model generates responses‚Äîpick based on your needs. You need to choose `ktransformers`, if you LoRA fine-tuning it with ktransformers.\n",
    "- `use_kt: true`: Must match training‚Äîdisables KT optimization if set to `false` (slower inference!).  \n",
    "- `kt_optimize_rule`: Use the **exact same file** as training (e.g., `DeepSeek-V2-Lite-Chat-sft-amx.yaml`)‚Äîensures layers map correctly.  \n",
    "\n",
    "---\n",
    "\n",
    "### How to Verify Inference Works\n",
    "After launching the chat command, check the logs for these key messages to confirm the model is running correctly:\n",
    "1. `Loaded adapter weight: XXX -> XXX`: LoRA adapter is loaded correctly.  \n",
    "2. `KTransformers inference enabled`: KT optimization is active.  \n",
    "3. `Backend: AMXInt8`: AMX acceleration is working (if supported).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c08b31f7-32a4-4d51-b6c0-d063d7785371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no balance_serve\n",
      "flashinfer not found, use triton for linux\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,741 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,741 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,741 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,742 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,742 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,742 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:30:51,956 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:51,960 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:51,961 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:30:51,961 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-11-09 09:30:51,962 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-11-09 09:30:52,169 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,184 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,185 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:30:52,185 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "[INFO|2025-11-09 09:30:52] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,192 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:691] 2025-11-09 09:30:52,193 >> loading configuration file /mnt/data3/models/DeepSeek-V2-Lite-Chat/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-11-09 09:30:52,193 >> Model config DeepseekV2Config {\n",
      "  \"architectures\": [\n",
      "    \"DeepseekV2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_deepseek.DeepseekV2Config\",\n",
      "    \"AutoModel\": \"modeling_deepseek.DeepseekV2Model\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_deepseek.DeepseekV2ForCausalLM\"\n",
      "  },\n",
      "  \"aux_loss_alpha\": 0.001,\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001,\n",
      "  \"ep_size\": 1,\n",
      "  \"first_k_dense_replace\": 1,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 10944,\n",
      "  \"kv_lora_rank\": 512,\n",
      "  \"max_position_embeddings\": 163840,\n",
      "  \"model_type\": \"deepseek_v2\",\n",
      "  \"moe_intermediate_size\": 1408,\n",
      "  \"moe_layer_freq\": 1,\n",
      "  \"n_group\": 1,\n",
      "  \"n_routed_experts\": 64,\n",
      "  \"n_shared_experts\": 2,\n",
      "  \"norm_topk_prob\": false,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_experts_per_tok\": 6,\n",
      "  \"num_hidden_layers\": 27,\n",
      "  \"num_key_value_heads\": 16,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"q_lora_rank\": null,\n",
      "  \"qk_nope_head_dim\": 128,\n",
      "  \"qk_rope_head_dim\": 64,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"beta_fast\": 32,\n",
      "    \"beta_slow\": 1,\n",
      "    \"factor\": 40,\n",
      "    \"mscale\": 0.707,\n",
      "    \"mscale_all_dim\": 0.707,\n",
      "    \"original_max_position_embeddings\": 4096,\n",
      "    \"type\": \"yarn\"\n",
      "  },\n",
      "  \"rope_theta\": 10000,\n",
      "  \"routed_scaling_factor\": 1.0,\n",
      "  \"scoring_func\": \"softmax\",\n",
      "  \"seq_aux\": true,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"topk_group\": 1,\n",
      "  \"topk_method\": \"greedy\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"v_head_dim\": 128,\n",
      "  \"vocab_size\": 102400\n",
      "}\n",
      "\n",
      "using custom modeling_xxx.py.\n",
      "[WARNING|logging.py:328] 2025-11-09 09:30:52,194 >> DeepseekV2ForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From üëâv4.50üëà onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "[INFO|configuration_utils.py:1142] 2025-11-09 09:30:52,195 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 100000,\n",
      "  \"eos_token_id\": 100001\n",
      "}\n",
      "\n",
      "Injecting model as ktransformers.operators.models . KDeepseekV2Model\n",
      "Injecting model.embed_tokens as default\n",
      "Injecting model.layers as default\n",
      "Injecting model.layers.0 as default\n",
      "Injecting model.layers.0.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.0.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.0.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.0.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.0.mlp as default\n",
      "Injecting model.layers.0.mlp.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.0.mlp.act_fn as default\n",
      "Injecting model.layers.0.input_layernorm as default\n",
      "Injecting model.layers.0.post_attention_layernorm as default\n",
      "Injecting model.layers.1 as default\n",
      "Injecting model.layers.1.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.1.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.1.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.1.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.1.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.1.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.1.mlp.gate as default\n",
      "Injecting model.layers.1.mlp.shared_experts as default\n",
      "Injecting model.layers.1.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.1.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.1.input_layernorm as default\n",
      "Injecting model.layers.1.post_attention_layernorm as default\n",
      "Injecting model.layers.2 as default\n",
      "Injecting model.layers.2.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.2.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.2.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.2.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.2.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.2.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.2.mlp.gate as default\n",
      "Injecting model.layers.2.mlp.shared_experts as default\n",
      "Injecting model.layers.2.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.2.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.2.input_layernorm as default\n",
      "Injecting model.layers.2.post_attention_layernorm as default\n",
      "Injecting model.layers.3 as default\n",
      "Injecting model.layers.3.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.3.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.3.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.3.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.3.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.3.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.3.mlp.gate as default\n",
      "Injecting model.layers.3.mlp.shared_experts as default\n",
      "Injecting model.layers.3.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.3.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.3.input_layernorm as default\n",
      "Injecting model.layers.3.post_attention_layernorm as default\n",
      "Injecting model.layers.4 as default\n",
      "Injecting model.layers.4.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.4.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.4.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.4.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.4.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.4.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.4.mlp.gate as default\n",
      "Injecting model.layers.4.mlp.shared_experts as default\n",
      "Injecting model.layers.4.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.4.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.4.input_layernorm as default\n",
      "Injecting model.layers.4.post_attention_layernorm as default\n",
      "Injecting model.layers.5 as default\n",
      "Injecting model.layers.5.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.5.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.5.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.5.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.5.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.5.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.5.mlp.gate as default\n",
      "Injecting model.layers.5.mlp.shared_experts as default\n",
      "Injecting model.layers.5.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.5.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.5.input_layernorm as default\n",
      "Injecting model.layers.5.post_attention_layernorm as default\n",
      "Injecting model.layers.6 as default\n",
      "Injecting model.layers.6.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.6.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.6.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.6.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.6.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.6.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.6.mlp.gate as default\n",
      "Injecting model.layers.6.mlp.shared_experts as default\n",
      "Injecting model.layers.6.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.6.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.6.input_layernorm as default\n",
      "Injecting model.layers.6.post_attention_layernorm as default\n",
      "Injecting model.layers.7 as default\n",
      "Injecting model.layers.7.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.7.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.7.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.7.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.7.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.7.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.7.mlp.gate as default\n",
      "Injecting model.layers.7.mlp.shared_experts as default\n",
      "Injecting model.layers.7.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.7.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.7.input_layernorm as default\n",
      "Injecting model.layers.7.post_attention_layernorm as default\n",
      "Injecting model.layers.8 as default\n",
      "Injecting model.layers.8.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.8.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.8.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.8.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.8.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.8.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.8.mlp.gate as default\n",
      "Injecting model.layers.8.mlp.shared_experts as default\n",
      "Injecting model.layers.8.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.8.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.8.input_layernorm as default\n",
      "Injecting model.layers.8.post_attention_layernorm as default\n",
      "Injecting model.layers.9 as default\n",
      "Injecting model.layers.9.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.9.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.9.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.9.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.9.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.9.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.9.mlp.gate as default\n",
      "Injecting model.layers.9.mlp.shared_experts as default\n",
      "Injecting model.layers.9.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.9.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.9.input_layernorm as default\n",
      "Injecting model.layers.9.post_attention_layernorm as default\n",
      "Injecting model.layers.10 as default\n",
      "Injecting model.layers.10.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.10.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.10.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.10.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.10.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.10.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.10.mlp.gate as default\n",
      "Injecting model.layers.10.mlp.shared_experts as default\n",
      "Injecting model.layers.10.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.10.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.10.input_layernorm as default\n",
      "Injecting model.layers.10.post_attention_layernorm as default\n",
      "Injecting model.layers.11 as default\n",
      "Injecting model.layers.11.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.11.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.11.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.11.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.11.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.11.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.11.mlp.gate as default\n",
      "Injecting model.layers.11.mlp.shared_experts as default\n",
      "Injecting model.layers.11.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.11.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.11.input_layernorm as default\n",
      "Injecting model.layers.11.post_attention_layernorm as default\n",
      "Injecting model.layers.12 as default\n",
      "Injecting model.layers.12.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.12.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.12.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.12.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.12.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.12.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.12.mlp.gate as default\n",
      "Injecting model.layers.12.mlp.shared_experts as default\n",
      "Injecting model.layers.12.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.12.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.12.input_layernorm as default\n",
      "Injecting model.layers.12.post_attention_layernorm as default\n",
      "Injecting model.layers.13 as default\n",
      "Injecting model.layers.13.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.13.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.13.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.13.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.13.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.13.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.13.mlp.gate as default\n",
      "Injecting model.layers.13.mlp.shared_experts as default\n",
      "Injecting model.layers.13.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.13.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.13.input_layernorm as default\n",
      "Injecting model.layers.13.post_attention_layernorm as default\n",
      "Injecting model.layers.14 as default\n",
      "Injecting model.layers.14.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.14.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.14.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.14.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.14.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.14.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.14.mlp.gate as default\n",
      "Injecting model.layers.14.mlp.shared_experts as default\n",
      "Injecting model.layers.14.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.14.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.14.input_layernorm as default\n",
      "Injecting model.layers.14.post_attention_layernorm as default\n",
      "Injecting model.layers.15 as default\n",
      "Injecting model.layers.15.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.15.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.15.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.15.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.15.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.15.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.15.mlp.gate as default\n",
      "Injecting model.layers.15.mlp.shared_experts as default\n",
      "Injecting model.layers.15.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.15.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.15.input_layernorm as default\n",
      "Injecting model.layers.15.post_attention_layernorm as default\n",
      "Injecting model.layers.16 as default\n",
      "Injecting model.layers.16.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.16.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.16.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.16.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.16.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.16.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.16.mlp.gate as default\n",
      "Injecting model.layers.16.mlp.shared_experts as default\n",
      "Injecting model.layers.16.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.16.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.16.input_layernorm as default\n",
      "Injecting model.layers.16.post_attention_layernorm as default\n",
      "Injecting model.layers.17 as default\n",
      "Injecting model.layers.17.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.17.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.17.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.17.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.17.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.17.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.17.mlp.gate as default\n",
      "Injecting model.layers.17.mlp.shared_experts as default\n",
      "Injecting model.layers.17.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.17.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.17.input_layernorm as default\n",
      "Injecting model.layers.17.post_attention_layernorm as default\n",
      "Injecting model.layers.18 as default\n",
      "Injecting model.layers.18.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.18.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.18.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.18.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.18.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.18.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.18.mlp.gate as default\n",
      "Injecting model.layers.18.mlp.shared_experts as default\n",
      "Injecting model.layers.18.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.18.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.18.input_layernorm as default\n",
      "Injecting model.layers.18.post_attention_layernorm as default\n",
      "Injecting model.layers.19 as default\n",
      "Injecting model.layers.19.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.19.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.19.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.19.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.19.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.19.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.19.mlp.gate as default\n",
      "Injecting model.layers.19.mlp.shared_experts as default\n",
      "Injecting model.layers.19.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.19.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.19.input_layernorm as default\n",
      "Injecting model.layers.19.post_attention_layernorm as default\n",
      "Injecting model.layers.20 as default\n",
      "Injecting model.layers.20.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.20.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.20.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.20.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.20.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.20.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.20.mlp.gate as default\n",
      "Injecting model.layers.20.mlp.shared_experts as default\n",
      "Injecting model.layers.20.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.20.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.20.input_layernorm as default\n",
      "Injecting model.layers.20.post_attention_layernorm as default\n",
      "Injecting model.layers.21 as default\n",
      "Injecting model.layers.21.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.21.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.21.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.21.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.21.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.21.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.21.mlp.gate as default\n",
      "Injecting model.layers.21.mlp.shared_experts as default\n",
      "Injecting model.layers.21.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.21.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.21.input_layernorm as default\n",
      "Injecting model.layers.21.post_attention_layernorm as default\n",
      "Injecting model.layers.22 as default\n",
      "Injecting model.layers.22.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.22.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.22.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.22.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.22.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.22.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.22.mlp.gate as default\n",
      "Injecting model.layers.22.mlp.shared_experts as default\n",
      "Injecting model.layers.22.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.22.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.22.input_layernorm as default\n",
      "Injecting model.layers.22.post_attention_layernorm as default\n",
      "Injecting model.layers.23 as default\n",
      "Injecting model.layers.23.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.23.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.23.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.23.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.23.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.23.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.23.mlp.gate as default\n",
      "Injecting model.layers.23.mlp.shared_experts as default\n",
      "Injecting model.layers.23.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.23.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.23.input_layernorm as default\n",
      "Injecting model.layers.23.post_attention_layernorm as default\n",
      "Injecting model.layers.24 as default\n",
      "Injecting model.layers.24.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.24.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.24.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.24.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.24.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.24.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.24.mlp.gate as default\n",
      "Injecting model.layers.24.mlp.shared_experts as default\n",
      "Injecting model.layers.24.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.24.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.24.input_layernorm as default\n",
      "Injecting model.layers.24.post_attention_layernorm as default\n",
      "Injecting model.layers.25 as default\n",
      "Injecting model.layers.25.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.25.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.25.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.25.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.25.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.25.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.25.mlp.gate as default\n",
      "Injecting model.layers.25.mlp.shared_experts as default\n",
      "Injecting model.layers.25.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.25.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.25.input_layernorm as default\n",
      "Injecting model.layers.25.post_attention_layernorm as default\n",
      "Injecting model.layers.26 as default\n",
      "Injecting model.layers.26.self_attn as ktransformers.operators.attention . KDeepseekV2Attention\n",
      "Injecting model.layers.26.self_attn.q_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_proj_with_mqa as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.kv_a_layernorm as default\n",
      "Injecting model.layers.26.self_attn.kv_b_proj as default\n",
      "Injecting model.layers.26.self_attn.o_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.self_attn.rotary_emb as ktransformers.operators.RoPE . YarnRotaryEmbedding\n",
      "Injecting model.layers.26.mlp as ktransformers.operators.experts . KDeepseekV2MoE\n",
      "Injecting model.layers.26.mlp.experts as ktransformers.operators.experts . KTransformersExperts\n",
      "Injecting model.layers.26.mlp.gate as default\n",
      "Injecting model.layers.26.mlp.shared_experts as default\n",
      "Injecting model.layers.26.mlp.shared_experts.gate_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.up_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.down_proj as ktransformers.operators.linear . KTransformersLinear\n",
      "Injecting model.layers.26.mlp.shared_experts.act_fn as default\n",
      "Injecting model.layers.26.input_layernorm as default\n",
      "Injecting model.layers.26.post_attention_layernorm as default\n",
      "Injecting model.norm as default\n",
      "Injecting lm_head as ktransformers.operators.linear . KTransformersLinear\n",
      "loading model.embed_tokens.weight to cpu\n",
      "loading model.layers.0.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.0.self_attn.kv_b_proj.weight to cuda:0\n",
      "loading model.layers.0.input_layernorm.weight to cuda:0\n",
      "loading model.layers.0.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.1.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.1.mlp.gate.weight to cuda:0\n",
      "loading model.layers.1.input_layernorm.weight to cuda:0\n",
      "loading model.layers.1.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.2.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.2.mlp.gate.weight to cuda:0\n",
      "loading model.layers.2.input_layernorm.weight to cuda:0\n",
      "loading model.layers.2.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.3.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.3.mlp.gate.weight to cuda:0\n",
      "loading model.layers.3.input_layernorm.weight to cuda:0\n",
      "loading model.layers.3.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.4.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.4.mlp.gate.weight to cuda:0\n",
      "loading model.layers.4.input_layernorm.weight to cuda:0\n",
      "loading model.layers.4.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.5.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.5.mlp.gate.weight to cuda:0\n",
      "loading model.layers.5.input_layernorm.weight to cuda:0\n",
      "loading model.layers.5.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.6.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.6.mlp.gate.weight to cuda:0\n",
      "loading model.layers.6.input_layernorm.weight to cuda:0\n",
      "loading model.layers.6.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.7.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.7.mlp.gate.weight to cuda:0\n",
      "loading model.layers.7.input_layernorm.weight to cuda:0\n",
      "loading model.layers.7.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.8.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.8.mlp.gate.weight to cuda:0\n",
      "loading model.layers.8.input_layernorm.weight to cuda:0\n",
      "loading model.layers.8.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.9.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.9.mlp.gate.weight to cuda:0\n",
      "loading model.layers.9.input_layernorm.weight to cuda:0\n",
      "loading model.layers.9.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.10.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.10.mlp.gate.weight to cuda:0\n",
      "loading model.layers.10.input_layernorm.weight to cuda:0\n",
      "loading model.layers.10.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.11.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.11.mlp.gate.weight to cuda:0\n",
      "loading model.layers.11.input_layernorm.weight to cuda:0\n",
      "loading model.layers.11.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.12.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.12.mlp.gate.weight to cuda:0\n",
      "loading model.layers.12.input_layernorm.weight to cuda:0\n",
      "loading model.layers.12.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.13.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.13.mlp.gate.weight to cuda:0\n",
      "loading model.layers.13.input_layernorm.weight to cuda:0\n",
      "loading model.layers.13.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.14.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.14.mlp.gate.weight to cuda:0\n",
      "loading model.layers.14.input_layernorm.weight to cuda:0\n",
      "loading model.layers.14.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.15.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.15.mlp.gate.weight to cuda:0\n",
      "loading model.layers.15.input_layernorm.weight to cuda:0\n",
      "loading model.layers.15.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.16.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.16.mlp.gate.weight to cuda:0\n",
      "loading model.layers.16.input_layernorm.weight to cuda:0\n",
      "loading model.layers.16.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.17.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.17.mlp.gate.weight to cuda:0\n",
      "loading model.layers.17.input_layernorm.weight to cuda:0\n",
      "loading model.layers.17.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.18.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.18.mlp.gate.weight to cuda:0\n",
      "loading model.layers.18.input_layernorm.weight to cuda:0\n",
      "loading model.layers.18.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.19.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.19.mlp.gate.weight to cuda:0\n",
      "loading model.layers.19.input_layernorm.weight to cuda:0\n",
      "loading model.layers.19.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.20.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.20.mlp.gate.weight to cuda:0\n",
      "loading model.layers.20.input_layernorm.weight to cuda:0\n",
      "loading model.layers.20.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.21.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.21.mlp.gate.weight to cuda:0\n",
      "loading model.layers.21.input_layernorm.weight to cuda:0\n",
      "loading model.layers.21.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.22.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.22.mlp.gate.weight to cuda:0\n",
      "loading model.layers.22.input_layernorm.weight to cuda:0\n",
      "loading model.layers.22.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.23.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.23.mlp.gate.weight to cuda:0\n",
      "loading model.layers.23.input_layernorm.weight to cuda:0\n",
      "loading model.layers.23.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.24.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.24.mlp.gate.weight to cuda:0\n",
      "loading model.layers.24.input_layernorm.weight to cuda:0\n",
      "loading model.layers.24.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.25.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.25.mlp.gate.weight to cuda:0\n",
      "loading model.layers.25.input_layernorm.weight to cuda:0\n",
      "loading model.layers.25.post_attention_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_a_layernorm.weight to cuda:0\n",
      "loading model.layers.26.self_attn.kv_b_proj.weight to cuda:0\n",
      "GO INTO AMXInt8!!\n",
      "loading model.layers.26.mlp.gate.weight to cuda:0\n",
      "loading model.layers.26.input_layernorm.weight to cuda:0\n",
      "loading model.layers.26.post_attention_layernorm.weight to cuda:0\n",
      "loading model.norm.weight to cuda:0\n",
      "[INFO|2025-11-09 09:31:03] llamafactory.model.model_utils.attention:143 >> Using vanilla attention implementation.\n",
      "lora_config:{'task_type': <TaskType.CAUSAL_LM: 'CAUSAL_LM'>, 'peft_type': <PeftType.LORA: 'LORA'>, 'auto_mapping': None, 'base_model_name_or_path': None, 'revision': None, 'inference_mode': True, 'r': 8, 'target_modules': {'shared_experts.up_proj', 'kv_a_proj_with_mqa', 'q_proj', 'mlp.gate_proj', 'o_proj', 'shared_experts.down_proj', 'kv_b_proj', 'mlp.down_proj', 'mlp.up_proj', 'shared_experts.gate_proj'}, 'exclude_modules': None, 'lora_alpha': 16, 'lora_dropout': 0.0, 'fan_in_fan_out': False, 'bias': 'none', 'use_rslora': False, 'modules_to_save': None, 'init_lora_weights': True, 'layers_to_transform': None, 'layers_pattern': None, 'rank_pattern': {}, 'alpha_pattern': {}, 'megatron_config': None, 'megatron_core': 'megatron.core', 'loftq_config': {}, 'eva_config': None, 'use_dora': False, 'layer_replication': None, 'runtime_config': LoraRuntimeConfig(ephemeral_gpu_offload=False), 'lora_bias': False, '_custom_modules': None}\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.mlp.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.mlp.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.mlp.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.mlp.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.mlp.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.mlp.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.0.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.1.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.10.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.11.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.12.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.13.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.14.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.15.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.16.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.17.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.18.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.19.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.2.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.20.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.21.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.22.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.23.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.24.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.25.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.26.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.3.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.4.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.5.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.6.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.7.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.8.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.down_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.gate_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.mlp.orig_module.shared_experts.up_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_a_proj_with_mqa.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.kv_b_proj.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.o_proj.orig_module.lora_B.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_A.weight -> model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_A.default.weight\n",
      "Loaded adapter weight: base_model.model.model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_B.weight -> model.orig_module.layers.9.self_attn.orig_module.q_proj.orig_module.lora_B.default.weight\n",
      "[INFO|2025-11-09 09:31:03] llamafactory.model.adapter:143 >> Loaded adapter(s): /mnt/data/lpl/test_adapter/Kllama_deepseekV2\n",
      "[INFO|2025-11-09 09:31:03] llamafactory.model.loader:143 >> all params: 276,772,352\n",
      "Welcome to the CLI application, use `clear` to remove the history, use `exit` to exit the application.\n",
      "\n",
      "User: \n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/data/lpl/anaconda3/envs/KNllama/bin/llamafactory-cli\", line 7, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/cli.py\", line 24, in main\n",
      "    launcher.launch()\n",
      "  File \"/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/launcher.py\", line 144, in launch\n",
      "    run_chat()\n",
      "  File \"/home/lpl/LLaMA-Factory-KT/notebook/LLaMA-Factory/src/llamafactory/chat/chat_model.py\", line 186, in run_chat\n",
      "    query = input(\"\\nUser: \")\n",
      "            ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli chat examples/inference/deepseek2_lora_sft_kt.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KNllama",
   "language": "python",
   "name": "knllama"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
