# kt-cli

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

## English

**kt-cli** is a unified command-line interface for KTransformers. It provides a user-friendly way to access all KTransformers functionality including model inference, fine-tuning, benchmarking, and more.

### Features

- ğŸš€ **Easy Model Serving**: Start inference servers with a single command
- ğŸ“¦ **Smart Installation**: Auto-detects environment and installs dependencies
- ğŸ” **Fuzzy Model Matching**: Find models by partial names
- ğŸŒ **Bilingual Support**: Full English and Chinese language support
- âš™ï¸ **Flexible Configuration**: Persistent settings with environment variable support
- ğŸ¥ **Environment Diagnostics**: Built-in health checks with `kt doctor`

### Installation

```bash
# Install from source
cd kt-cli
pip install -e .

# Or install from PyPI (coming soon)
pip install kt-cli
```

### First Run

On first run, kt-cli will prompt you to select your preferred language:

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ kt-cli â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Welcome to KTransformers CLI! / æ¬¢è¿ä½¿ç”¨ KTransformers CLI! â”‚
â”‚                                                             â”‚
â”‚ Let's set up your preferences.                              â”‚
â”‚ è®©æˆ‘ä»¬è®¾ç½®æ‚¨çš„åå¥½ã€‚                                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Select your preferred language / é€‰æ‹©æ‚¨çš„é¦–é€‰è¯­è¨€:

  [1] English
  [2] ä¸­æ–‡ (Chinese)
```

You can re-run this setup anytime with `kt config init`.

### Quick Start

```bash
# Check your environment
kt doctor

# Download a model
kt download deepseek-v3

# Start inference server
kt run deepseek-v3

# Fine-tune with LlamaFactory
kt sft train config.yaml
```

### Commands

#### `kt version`

Display version information.

```bash
kt version          # Basic info
kt version -v       # Detailed package versions
```

#### `kt install`

Install KTransformers and dependencies.

**Important**: It's recommended to run `kt install` inside a virtual environment.

```bash
# First, create and activate a virtual environment
conda create -n kt python=3.10 && conda activate kt
# or: python -m venv kt-env && source kt-env/bin/activate

# Install from PyPI (default)
kt install                    # Install inference components
kt install inference          # Install inference components
kt install sft                # Install fine-tuning components
kt install full               # Install all components

# Install from source
kt install --source /path/to/ktransformers    # Build from local source
kt install -s /path/to/repo -e                # Editable install (for development)
kt install -s . -e                            # Editable install from current dir

# Options
kt install -y                 # Skip confirmations
kt install -f                 # Force reinstall
kt install --skip-torch       # Skip PyTorch (if already installed)
kt install -b dev             # Use specific git branch (with --source)
```

#### `kt update`

Update KTransformers to the latest version.

**You must specify the update method explicitly.**

```bash
# Update from PyPI
kt update --pypi

# Update from source (git pull + rebuild)
kt update --source /path/to/ktransformers

# Options
kt update --pypi -y           # Skip confirmations
```

#### `kt run`

Start model inference server (SGLang + kt-kernel).

```bash
kt run deepseek-v3            # Start with default settings
kt run qwen3-30b -p 8080      # Custom port
kt run /path/to/model         # Use local model path

# Options
--host, -H          Server host (default: 0.0.0.0)
--port, -p          Server port (default: 30000)
--gpu-experts       GPU experts per layer (default: 1)
--cpu-threads       CPU inference threads (auto-detected)
--numa-nodes        NUMA node count (auto-detected)
--model-path        Custom model path
--weights-path      Custom quantized weights path
--quantize, -q      Quantize if weights not found
--dry-run           Show command without executing
```

#### `kt download`

Download model weights from HuggingFace.

```bash
kt download deepseek-v3       # Download by name
kt download --list            # List available models
kt download Qwen/Qwen3-30B    # Direct HuggingFace repo

# Options
--path, -p          Custom download path
--resume            Resume incomplete downloads (default: on)
```

#### `kt quant`

Quantize model weights for CPU inference.

```bash
kt quant deepseek-v3                  # Quantize to INT4 (default)
kt quant deepseek-v3 --method int8    # Quantize to INT8
kt quant /path/to/model -o /output    # Custom output path

# Options
--method, -m        Quantization method: int4, int8
--output, -o        Output path
--input-type        Input type: fp8, fp16, bf16
--cpu-threads       CPU threads for quantization
--no-merge          Don't merge safetensor files
```

#### `kt bench` / `kt microbench`

Run benchmarks.

```bash
kt bench                      # Run full benchmark suite
kt bench --type moe           # Benchmark specific component
kt microbench moe             # Micro-benchmark MoE layer

# Options
--type, -t          Benchmark type: inference, moe, mla, linear, attention, all
--model, -m         Model to benchmark
--output, -o        Output file (JSON)
--iterations, -n    Number of iterations
```

#### `kt config`

Manage configuration.

```bash
kt config init                # Run first-time setup wizard
kt config show                # Show all settings
kt config show server.port    # Show specific setting
kt config set server.port 8080
kt config get server.port
kt config reset               # Reset to defaults
kt config path                # Show config file path
```

#### `kt doctor`

Diagnose environment issues.

```bash
kt doctor                     # Run diagnostics
kt doctor -v                  # Verbose output
```

#### `kt sft`

Fine-tuning with LlamaFactory.

```bash
kt sft train config.yaml      # Train model
kt sft chat config.yaml       # Chat with model
kt sft export config.yaml     # Export model
kt sft eval config.yaml       # Evaluate model

# Options
--use-kt/--no-kt    Enable/disable KTransformers optimization
```

### Configuration

Configuration is stored in `~/.ktransformers/config.yaml`.

```yaml
general:
  language: auto              # auto, en, zh
  color: true
  verbose: false

paths:
  models: ~/.ktransformers/models
  cache: ~/.ktransformers/cache
  weights: ""                 # Custom weights path

server:
  host: 0.0.0.0
  port: 30000

inference:
  cpu_threads: 0              # 0 = auto-detect
  numa_nodes: 0               # 0 = auto-detect
  gpu_experts: 1
  attention_backend: triton
  max_total_tokens: 40000
  max_running_requests: 32

download:
  mirror: ""                  # HuggingFace mirror URL
  resume: true

advanced:
  env: {}                     # Environment variables
  sglang_args: []             # Extra SGLang arguments
  llamafactory_args: []       # Extra LlamaFactory arguments
```

### Environment Variables

- `KT_LANG`: Override language (en, zh)
- `KT_CONFIG`: Custom config file path

### Supported Models

| Model | Aliases | GPU VRAM | CPU RAM |
|-------|---------|----------|---------|
| DeepSeek-V3.2 | deepseek-v3.2, dsv3.2 | 27GB | 350GB |
| DeepSeek-V3 | deepseek-v3, dsv3 | 27GB | 350GB |
| DeepSeek-V2.5 | deepseek-v2.5, dsv2.5 | 16GB | 128GB |
| Qwen3-30B-A3B | qwen3-30b, qwen3 | 12GB | 64GB |
| Kimi-K2 | kimi-k2, kimi, k2 | 24GB | 256GB |
| Mixtral-8x7B | mixtral, mixtral-moe | 12GB | 48GB |
| Mixtral-8x22B | mixtral-8x22b | 24GB | 176GB |

---

## ä¸­æ–‡

**kt-cli** æ˜¯ KTransformers çš„ç»Ÿä¸€å‘½ä»¤è¡Œç•Œé¢ã€‚å®ƒæä¾›äº†ä¸€ç§ç”¨æˆ·å‹å¥½çš„æ–¹å¼æ¥è®¿é—® KTransformers çš„æ‰€æœ‰åŠŸèƒ½ï¼ŒåŒ…æ‹¬æ¨¡å‹æ¨ç†ã€å¾®è°ƒã€åŸºå‡†æµ‹è¯•ç­‰ã€‚

### ç‰¹æ€§

- ğŸš€ **ç®€å•çš„æ¨¡å‹æœåŠ¡**ï¼šä¸€æ¡å‘½ä»¤å¯åŠ¨æ¨ç†æœåŠ¡å™¨
- ğŸ“¦ **æ™ºèƒ½å®‰è£…**ï¼šè‡ªåŠ¨æ£€æµ‹ç¯å¢ƒå¹¶å®‰è£…ä¾èµ–
- ğŸ” **æ¨¡ç³Šæ¨¡å‹åŒ¹é…**ï¼šé€šè¿‡éƒ¨åˆ†åç§°æŸ¥æ‰¾æ¨¡å‹
- ğŸŒ **åŒè¯­æ”¯æŒ**ï¼šå®Œæ•´çš„ä¸­è‹±æ–‡è¯­è¨€æ”¯æŒ
- âš™ï¸ **çµæ´»é…ç½®**ï¼šæŒä¹…åŒ–è®¾ç½®ï¼Œæ”¯æŒç¯å¢ƒå˜é‡
- ğŸ¥ **ç¯å¢ƒè¯Šæ–­**ï¼šå†…ç½®å¥åº·æ£€æŸ¥ `kt doctor`

### å®‰è£…

```bash
# ä»æºç å®‰è£…
cd kt-cli
pip install -e .

# æˆ–ä» PyPI å®‰è£…ï¼ˆå³å°†æ¨å‡ºï¼‰
pip install kt-cli
```

### é¦–æ¬¡è¿è¡Œ

é¦–æ¬¡è¿è¡Œæ—¶ï¼Œkt-cli ä¼šæç¤ºæ‚¨é€‰æ‹©é¦–é€‰è¯­è¨€ï¼š

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ kt-cli â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Welcome to KTransformers CLI! / æ¬¢è¿ä½¿ç”¨ KTransformers CLI! â”‚
â”‚                                                             â”‚
â”‚ Let's set up your preferences.                              â”‚
â”‚ è®©æˆ‘ä»¬è®¾ç½®æ‚¨çš„åå¥½ã€‚                                        â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Select your preferred language / é€‰æ‹©æ‚¨çš„é¦–é€‰è¯­è¨€:

  [1] English
  [2] ä¸­æ–‡ (Chinese)
```

æ‚¨å¯ä»¥éšæ—¶ä½¿ç”¨ `kt config init` é‡æ–°è¿è¡Œæ­¤è®¾ç½®ã€‚

### å¿«é€Ÿå¼€å§‹

```bash
# æ£€æŸ¥ç¯å¢ƒ
kt doctor

# ä¸‹è½½æ¨¡å‹
kt download deepseek-v3

# å¯åŠ¨æ¨ç†æœåŠ¡å™¨
kt run deepseek-v3

# ä½¿ç”¨ LlamaFactory å¾®è°ƒ
kt sft train config.yaml
```

### å‘½ä»¤è¯´æ˜

#### `kt version`

æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯ã€‚

```bash
kt version          # åŸºæœ¬ä¿¡æ¯
kt version -v       # è¯¦ç»†çš„åŒ…ç‰ˆæœ¬
```

#### `kt install`

å®‰è£… KTransformers åŠå…¶ä¾èµ–ã€‚

**é‡è¦**ï¼šå»ºè®®åœ¨è™šæ‹Ÿç¯å¢ƒä¸­è¿è¡Œ `kt install`ã€‚

```bash
# é¦–å…ˆï¼Œåˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
conda create -n kt python=3.10 && conda activate kt
# æˆ–: python -m venv kt-env && source kt-env/bin/activate

# ä» PyPI å®‰è£…ï¼ˆé»˜è®¤ï¼‰
kt install                    # å®‰è£…æ¨ç†ç»„ä»¶
kt install inference          # å®‰è£…æ¨ç†ç»„ä»¶
kt install sft                # å®‰è£…å¾®è°ƒç»„ä»¶
kt install full               # å®‰è£…æ‰€æœ‰ç»„ä»¶

# ä»æºç å®‰è£…
kt install --source /path/to/ktransformers    # ä»æœ¬åœ°æºç ç¼–è¯‘
kt install -s /path/to/repo -e                # å¯ç¼–è¾‘å®‰è£…ï¼ˆç”¨äºå¼€å‘ï¼‰
kt install -s . -e                            # ä»å½“å‰ç›®å½•å¯ç¼–è¾‘å®‰è£…

# é€‰é¡¹
kt install -y                 # è·³è¿‡ç¡®è®¤
kt install -f                 # å¼ºåˆ¶é‡æ–°å®‰è£…
kt install --skip-torch       # è·³è¿‡ PyTorchï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
kt install -b dev             # ä½¿ç”¨æŒ‡å®š git åˆ†æ”¯ï¼ˆé…åˆ --sourceï¼‰
```

#### `kt update`

æ›´æ–° KTransformers åˆ°æœ€æ–°ç‰ˆæœ¬ã€‚

**å¿…é¡»æ˜¾å¼æŒ‡å®šæ›´æ–°æ–¹å¼ã€‚**

```bash
# ä» PyPI æ›´æ–°
kt update --pypi

# ä»æºç æ›´æ–°ï¼ˆgit pull + é‡æ–°ç¼–è¯‘ï¼‰
kt update --source /path/to/ktransformers

# é€‰é¡¹
kt update --pypi -y           # è·³è¿‡ç¡®è®¤
```

#### `kt run`

å¯åŠ¨æ¨¡å‹æ¨ç†æœåŠ¡å™¨ï¼ˆSGLang + kt-kernelï¼‰ã€‚

```bash
kt run deepseek-v3            # ä½¿ç”¨é»˜è®¤è®¾ç½®å¯åŠ¨
kt run qwen3-30b -p 8080      # è‡ªå®šä¹‰ç«¯å£
kt run /path/to/model         # ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„

# é€‰é¡¹
--host, -H          æœåŠ¡å™¨åœ°å€ï¼ˆé»˜è®¤ï¼š0.0.0.0ï¼‰
--port, -p          æœåŠ¡å™¨ç«¯å£ï¼ˆé»˜è®¤ï¼š30000ï¼‰
--gpu-experts       æ¯å±‚ GPU ä¸“å®¶æ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰
--cpu-threads       CPU æ¨ç†çº¿ç¨‹æ•°ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
--numa-nodes        NUMA èŠ‚ç‚¹æ•°ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼‰
--model-path        è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„
--weights-path      è‡ªå®šä¹‰é‡åŒ–æƒé‡è·¯å¾„
--quantize, -q      å¦‚æœæ‰¾ä¸åˆ°æƒé‡åˆ™è¿›è¡Œé‡åŒ–
--dry-run           æ˜¾ç¤ºå‘½ä»¤ä½†ä¸æ‰§è¡Œ
```

#### `kt download`

ä» HuggingFace ä¸‹è½½æ¨¡å‹æƒé‡ã€‚

```bash
kt download deepseek-v3       # æŒ‰åç§°ä¸‹è½½
kt download --list            # åˆ—å‡ºå¯ç”¨æ¨¡å‹
kt download Qwen/Qwen3-30B    # ç›´æ¥ä½¿ç”¨ HuggingFace ä»“åº“

# é€‰é¡¹
--path, -p          è‡ªå®šä¹‰ä¸‹è½½è·¯å¾„
--resume            æ–­ç‚¹ç»­ä¼ ï¼ˆé»˜è®¤å¼€å¯ï¼‰
```

#### `kt quant`

é‡åŒ–æ¨¡å‹æƒé‡ä»¥ç”¨äº CPU æ¨ç†ã€‚

```bash
kt quant deepseek-v3                  # é‡åŒ–ä¸º INT4ï¼ˆé»˜è®¤ï¼‰
kt quant deepseek-v3 --method int8    # é‡åŒ–ä¸º INT8
kt quant /path/to/model -o /output    # è‡ªå®šä¹‰è¾“å‡ºè·¯å¾„

# é€‰é¡¹
--method, -m        é‡åŒ–æ–¹æ³•ï¼šint4, int8
--output, -o        è¾“å‡ºè·¯å¾„
--input-type        è¾“å…¥ç±»å‹ï¼šfp8, fp16, bf16
--cpu-threads       é‡åŒ–ä½¿ç”¨çš„ CPU çº¿ç¨‹æ•°
--no-merge          ä¸åˆå¹¶ safetensor æ–‡ä»¶
```

#### `kt bench` / `kt microbench`

è¿è¡ŒåŸºå‡†æµ‹è¯•ã€‚

```bash
kt bench                      # è¿è¡Œå®Œæ•´åŸºå‡†æµ‹è¯•å¥—ä»¶
kt bench --type moe           # æµ‹è¯•ç‰¹å®šç»„ä»¶
kt microbench moe             # MoE å±‚å¾®åŸºå‡†æµ‹è¯•

# é€‰é¡¹
--type, -t          æµ‹è¯•ç±»å‹ï¼šinference, moe, mla, linear, attention, all
--model, -m         è¦æµ‹è¯•çš„æ¨¡å‹
--output, -o        è¾“å‡ºæ–‡ä»¶ï¼ˆJSONï¼‰
--iterations, -n    è¿­ä»£æ¬¡æ•°
```

#### `kt config`

ç®¡ç†é…ç½®ã€‚

```bash
kt config init                # è¿è¡Œé¦–æ¬¡è®¾ç½®å‘å¯¼
kt config show                # æ˜¾ç¤ºæ‰€æœ‰è®¾ç½®
kt config show server.port    # æ˜¾ç¤ºç‰¹å®šè®¾ç½®
kt config set server.port 8080
kt config get server.port
kt config reset               # é‡ç½®ä¸ºé»˜è®¤å€¼
kt config path                # æ˜¾ç¤ºé…ç½®æ–‡ä»¶è·¯å¾„
```

#### `kt doctor`

è¯Šæ–­ç¯å¢ƒé—®é¢˜ã€‚

```bash
kt doctor                     # è¿è¡Œè¯Šæ–­
kt doctor -v                  # è¯¦ç»†è¾“å‡º
```

#### `kt sft`

ä½¿ç”¨ LlamaFactory è¿›è¡Œå¾®è°ƒã€‚

```bash
kt sft train config.yaml      # è®­ç»ƒæ¨¡å‹
kt sft chat config.yaml       # ä¸æ¨¡å‹å¯¹è¯
kt sft export config.yaml     # å¯¼å‡ºæ¨¡å‹
kt sft eval config.yaml       # è¯„ä¼°æ¨¡å‹

# é€‰é¡¹
--use-kt/--no-kt    å¯ç”¨/ç¦ç”¨ KTransformers ä¼˜åŒ–
```

### é…ç½®

é…ç½®å­˜å‚¨åœ¨ `~/.ktransformers/config.yaml`ã€‚

```yaml
general:
  language: auto              # auto, en, zh
  color: true
  verbose: false

paths:
  models: ~/.ktransformers/models
  cache: ~/.ktransformers/cache
  weights: ""                 # è‡ªå®šä¹‰æƒé‡è·¯å¾„

server:
  host: 0.0.0.0
  port: 30000

inference:
  cpu_threads: 0              # 0 = è‡ªåŠ¨æ£€æµ‹
  numa_nodes: 0               # 0 = è‡ªåŠ¨æ£€æµ‹
  gpu_experts: 1
  attention_backend: triton
  max_total_tokens: 40000
  max_running_requests: 32

download:
  mirror: ""                  # HuggingFace é•œåƒåœ°å€
  resume: true

advanced:
  env: {}                     # ç¯å¢ƒå˜é‡
  sglang_args: []             # é¢å¤–çš„ SGLang å‚æ•°
  llamafactory_args: []       # é¢å¤–çš„ LlamaFactory å‚æ•°
```

### ç¯å¢ƒå˜é‡

- `KT_LANG`ï¼šè¦†ç›–è¯­è¨€è®¾ç½®ï¼ˆen, zhï¼‰
- `KT_CONFIG`ï¼šè‡ªå®šä¹‰é…ç½®æ–‡ä»¶è·¯å¾„

### æ”¯æŒçš„æ¨¡å‹

| æ¨¡å‹ | åˆ«å | GPU æ˜¾å­˜ | CPU å†…å­˜ |
|------|------|----------|----------|
| DeepSeek-V3.2 | deepseek-v3.2, dsv3.2 | 27GB | 350GB |
| DeepSeek-V3 | deepseek-v3, dsv3 | 27GB | 350GB |
| DeepSeek-V2.5 | deepseek-v2.5, dsv2.5 | 16GB | 128GB |
| Qwen3-30B-A3B | qwen3-30b, qwen3 | 12GB | 64GB |
| Kimi-K2 | kimi-k2, kimi, k2 | 24GB | 256GB |
| Mixtral-8x7B | mixtral, mixtral-moe | 12GB | 48GB |
| Mixtral-8x22B | mixtral-8x22b | 24GB | 176GB |

---

## License

Apache 2.0

## Contributing

Contributions are welcome! Please see the main KTransformers repository for contribution guidelines.
