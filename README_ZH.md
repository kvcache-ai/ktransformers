<div align="center">
  <!-- <h1>KTransformers</h1> -->
  <p align="center">

<picture>
    <img alt="KTransformers" src="https://github.com/user-attachments/assets/d5a2492f-a415-4456-af99-4ab102f13f8b" width=50%>

</picture>

</p>
  <h3>ä¸€ä¸ªç”¨äºä½“éªŒå°–ç«¯ LLM æ¨ç†ä¼˜åŒ–çš„çµæ´»æ¡†æ¶</h3>
  <strong><a href="#show-cases">ğŸŒŸ æ¡ˆä¾‹å±•ç¤º</a> | <a href="#quick-start">ğŸš€ å¿«é€Ÿå…¥é—¨</a> | <a href="#tutorial">ğŸ“ƒ æ•™ç¨‹</a> | <a href="https://github.com/kvcache-ai/ktransformers/discussions">ğŸ’¬ è®¨è®º</a> | <a href="#FAQ">ğŸ™‹ å¸¸è§é—®é¢˜</a> </strong>
</div>

<h2 id="intro">ğŸ‰ ä»‹ç»</h2>
KTransformersï¼ˆå‘éŸ³ä¸º Quick Transformersï¼‰æ—¨åœ¨é€šè¿‡å…ˆè¿›çš„å†…æ ¸ä¼˜åŒ–å’Œæ”¾ç½®/å¹¶è¡Œç­–ç•¥æ¥å¢å¼ºæ‚¨å¯¹ ğŸ¤— [Transformers](https://github.com/huggingface/transformers) çš„ä½“éªŒã€‚
<br/><br/>
KTransformers æ˜¯ä¸€ä¸ªä»¥ Python ä¸ºä¸­å¿ƒçš„çµæ´»æ¡†æ¶ï¼Œå…¶æ ¸å¿ƒæ˜¯å¯æ‰©å±•æ€§ã€‚é€šè¿‡ç”¨ä¸€è¡Œä»£ç å®ç°å¹¶æ³¨å…¥ä¼˜åŒ–æ¨¡å—ï¼Œç”¨æˆ·å¯ä»¥è·å¾—ä¸ Transformers å…¼å®¹çš„æ¥å£ã€ç¬¦åˆ OpenAI å’Œ Ollama çš„ RESTful APIï¼Œç”šè‡³æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç±»ä¼¼ ChatGPT çš„ Web ç•Œé¢ã€‚
<br/><br/>
æˆ‘ä»¬å¯¹ KTransformers çš„æ„¿æ™¯æ˜¯æˆä¸ºä¸€ä¸ªç”¨äºå®éªŒåˆ›æ–° LLM æ¨ç†ä¼˜åŒ–çš„çµæ´»å¹³å°ã€‚å¦‚æœæ‚¨éœ€è¦ä»»ä½•å…¶ä»–åŠŸèƒ½ï¼Œè¯·å‘Šè¯‰æˆ‘ä»¬ã€‚

<h2 id="Updates">ğŸ”¥ æ›´æ–°</h2>

* **2025 å¹´ 2 æœˆ 10 æ—¥**ï¼šæ”¯æŒ Deepseek-R1 å’Œ V3 åœ¨å•ä¸ªï¼ˆ24GB VRAMï¼‰/å¤š GPU å’Œ 382G DRAM ä¸Šè¿è¡Œï¼Œé€Ÿåº¦æå‡é«˜è¾¾ 3~28 å€ã€‚è¯¦ç»†æ•™ç¨‹è¯·å‚è§ [è¿™é‡Œ](./doc/en/DeepseekR1_V3_tutorial.md)ã€‚
* **2024 å¹´ 8 æœˆ 28 æ—¥**ï¼šæ”¯æŒ InternLM2.5-7B-Chat-1M æ¨¡å‹ä¸‹çš„ 1M ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨ 24GB çš„ VRAM å’Œ 150GB çš„ DRAMã€‚è¯¦ç»†æ•™ç¨‹è¯·å‚è§ [è¿™é‡Œ](./doc/en/long_context_tutorial.md)ã€‚
* **2024 å¹´ 8 æœˆ 28 æ—¥**ï¼šå°† DeepseekV2 æ‰€éœ€çš„ VRAM ä» 21G é™ä½åˆ° 11Gã€‚
* **2024 å¹´ 8 æœˆ 15 æ—¥**ï¼šæ›´æ–°äº†è¯¦ç»†çš„ [æ•™ç¨‹](doc/en/injection_tutorial.md)ï¼Œä»‹ç»æ³¨å…¥å’Œå¤š GPU çš„ä½¿ç”¨ã€‚
* **2024 å¹´ 8 æœˆ 14 æ—¥**ï¼šæ”¯æŒ llamfile ä½œä¸ºçº¿æ€§åç«¯ã€‚
* **2024 å¹´ 8 æœˆ 12 æ—¥**ï¼šæ”¯æŒå¤š GPUï¼›æ”¯æŒæ–°æ¨¡å‹ï¼šmixtral 8\*7B å’Œ 8\*22Bï¼›æ”¯æŒ q2kã€q3kã€q5k åœ¨ GPU ä¸Šçš„å»é‡åŒ–ã€‚
* **2024 å¹´ 8 æœˆ 9 æ—¥**ï¼šæ”¯æŒ Windowsã€‚

<h2 id="show-cases">ğŸŒŸ æ¡ˆä¾‹å±•ç¤º</h2>

<div>
<h3>åœ¨ä»… 24GB VRAM çš„æ¡Œé¢ä¸Šè¿è¡Œ GPT-4/o1 çº§åˆ«çš„æœ¬åœ° VSCode Copilot</h3>
</div>

https://github.com/user-attachments/assets/ebd70bfa-b2c1-4abb-ae3b-296ed38aa285

</p>

- **[NEW!!!] æœ¬åœ° 671B DeepSeek-Coder-V3/R1**ï¼šä½¿ç”¨å…¶ Q4_K_M ç‰ˆæœ¬ï¼Œä»…éœ€ 14GB VRAM å’Œ 382GB DRAM å³å¯è¿è¡Œï¼ˆæ•™ç¨‹è¯·å‚è§ [è¿™é‡Œ](./doc/en/DeepseekR1_V3_tutorial.md)ï¼‰ã€‚
	- é¢„å¡«å……é€Ÿåº¦ï¼ˆtokens/sï¼‰ï¼š
 		- KTransformersï¼š54.21ï¼ˆ32 æ ¸ï¼‰â†’ 74.362ï¼ˆåŒæ’æ§½ï¼Œ2Ã—32 æ ¸ï¼‰â†’ 255.26ï¼ˆä¼˜åŒ–çš„ AMX åŸº MoE å†…æ ¸ï¼Œä»… V0.3ï¼‰â†’ 286.55ï¼ˆé€‰æ‹©æ€§ä½¿ç”¨ 6 ä¸ªä¸“å®¶ï¼Œä»… V0.3ï¼‰
 		- ä¸ llama.cpp åœ¨ 2Ã—32 æ ¸ä¸‹ç›¸æ¯”ï¼Œè¾¾åˆ° **27.79Ã— é€Ÿåº¦æå‡**ã€‚
 	- è§£ç é€Ÿåº¦ï¼ˆtokens/sï¼‰ï¼š
 		- KTransformersï¼š8.73ï¼ˆ32 æ ¸ï¼‰â†’ 11.26ï¼ˆåŒæ’æ§½ï¼Œ2Ã—32 æ ¸ï¼‰â†’ 13.69ï¼ˆé€‰æ‹©æ€§ä½¿ç”¨ 6 ä¸ªä¸“å®¶ï¼Œä»… V0.3ï¼‰
 		- ä¸ llama.cpp åœ¨ 2Ã—32 æ ¸ä¸‹ç›¸æ¯”ï¼Œè¾¾åˆ° **3.03Ã— é€Ÿåº¦æå‡**ã€‚
	- å³å°†å¼€æºå‘å¸ƒï¼š
		- AMX ä¼˜åŒ–å’Œé€‰æ‹©æ€§ä¸“å®¶æ¿€æ´»å°†åœ¨ V0.3 ä¸­å¼€æºã€‚
		- ç›®å‰ä»…åœ¨é¢„è§ˆäºŒè¿›åˆ¶åˆ†å‘ä¸­å¯ç”¨ï¼Œå¯ä» [è¿™é‡Œ](./doc/en/DeepseekR1_V3_tutorial.md) ä¸‹è½½ã€‚

- **æœ¬åœ° 236B DeepSeek-Coder-V2**ï¼šä½¿ç”¨å…¶ Q4_K_M ç‰ˆæœ¬ï¼Œä»…éœ€ 21GB VRAM å’Œ 136GB DRAM å³å¯è¿è¡Œï¼Œç”šè‡³åœ¨ [BigCodeBench](https://huggingface.co/blog/leaderboard-bigcodebench) ä¸­å¾—åˆ†è¶…è¿‡ GPT4-0613ã€‚

<p align="center">
  <picture>
    <img alt="DeepSeek-Coder-V2 Score" src="https://github.com/user-attachments/assets/d052924e-8631-44de-aad2-97c54b965693" width=100%>
  </picture>
</p>

- **æ›´å¿«çš„é€Ÿåº¦**ï¼šé€šè¿‡ MoE å¸è½½å’Œæ³¨å…¥æ¥è‡ª [Llamafile](https://github.com/Mozilla-Ocho/llamafile/tree/main) å’Œ [Marlin](https://github.com/IST-DASLab/marlin) çš„é«˜çº§å†…æ ¸ï¼Œå®ç°äº† 2K æç¤ºé¢„å¡«å…… 126 tokens/s å’Œç”Ÿæˆ 13.6 tokens/s çš„é€Ÿåº¦ã€‚
- **VSCode é›†æˆ**ï¼šå°è£…æˆç¬¦åˆ OpenAI å’Œ Ollama çš„ APIï¼Œå¯æ— ç¼é›†æˆåˆ° [Tabby](https://github.com/TabbyML/tabby) å’Œå…¶ä»–å‰ç«¯çš„åç«¯ã€‚

<p align="center">

https://github.com/user-attachments/assets/4c6a8a38-05aa-497d-8eb1-3a5b3918429c

</p>

<h3>åœ¨ä»… 24GB VRAM çš„æ¡Œé¢ä¸Šè¿›è¡Œ 1M ä¸Šä¸‹æ–‡æœ¬åœ°æ¨ç†</h3>
<p align="center">

https://github.com/user-attachments/assets/a865e5e4-bca3-401e-94b8-af3c080e6c12

* **1M ä¸Šä¸‹æ–‡ InternLM 2.5 7B**ï¼šä»¥å…¨ bf16 ç²¾åº¦è¿è¡Œï¼Œä½¿ç”¨ 24GB VRAM å’Œ 150GB DRAMï¼Œå¯åœ¨æœ¬åœ°æ¡Œé¢è®¾ç½®ä¸­å®ç°ã€‚åœ¨ 1M "é’ˆåœ¨å¹²è‰å †ä¸­" æµ‹è¯•ä¸­è¾¾åˆ° 92.88% çš„æˆåŠŸç‡ï¼Œåœ¨ 128K NIAH æµ‹è¯•ä¸­è¾¾åˆ° 100%ã€‚

<p align="center">
  <picture>
    <img alt="Single Needle Retrieval 128K" src="./doc/assets/needle_128K.png" width=100%>
  </picture>
</p>

<p align="center">
  <picture>
    <img alt="Single Needle Retrieval 1000K" src="./doc/assets/needle_1M.png" width=100%>
  </picture>
</p>

* **å¢å¼ºçš„é€Ÿåº¦**ï¼šä½¿ç”¨ç¨€ç–æ³¨æ„åŠ›ï¼Œé€šè¿‡ llamafile å†…æ ¸å®ç° 1M ä¸Šä¸‹æ–‡ç”Ÿæˆ 16.91 tokens/s çš„é€Ÿåº¦ã€‚è¿™ç§æ–¹æ³•æ¯” llama.cpp çš„å…¨æ³¨æ„åŠ›æ–¹æ³•å¿« 10 å€ä»¥ä¸Šã€‚

* **çµæ´»çš„ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶**ï¼šæä¾›äº†ä¸€ä¸ªçµæ´»çš„å—ç¨€ç–æ³¨æ„åŠ›æ¡†æ¶ï¼Œç”¨äº CPU å¸è½½è§£ç ã€‚ä¸ SnapKVã€Quest å’Œ InfLLm å…¼å®¹ã€‚æ›´å¤šä¿¡æ¯è¯·å‚è§ [è¿™é‡Œ](./doc/en/long_context_introduction.md)ã€‚

<strong>æ›´å¤šé«˜çº§åŠŸèƒ½å³å°†æ¨å‡ºï¼Œæ•¬è¯·æœŸå¾…ï¼</strong>

<h2 id="quick-start">ğŸš€ å¿«é€Ÿå…¥é—¨</h2>

<h3>å‡†å¤‡å·¥ä½œ</h3>
ä¸€äº›å‡†å¤‡å·¥ä½œï¼š

- å¦‚æœæ‚¨è¿˜æ²¡æœ‰ CUDA 12.1 åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œå¯ä»¥ä» [è¿™é‡Œ](https://developer.nvidia.com/cuda-downloads) å®‰è£…ã€‚
  
  ```sh
  # Adding CUDA to PATH
  export PATH=/usr/local/cuda/bin:$PATH
  export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
  export CUDA_PATH=/usr/local/cuda
  ```

- Linux-x86_64 ç³»ç»Ÿï¼Œéœ€è¦å®‰è£… gccã€g++ å’Œ cmake
  
  ```sh
  sudo apt-get update
  sudo apt-get install gcc g++ cmake ninja-build
  ```

- æˆ‘ä»¬å»ºè®®ä½¿ç”¨ Conda åˆ›å»ºä¸€ä¸ª Python=3.11 çš„è™šæ‹Ÿç¯å¢ƒæ¥è¿è¡Œæˆ‘ä»¬çš„ç¨‹åºã€‚
  
  ```sh
  conda create --name ktransformers python=3.11
  conda activate ktransformers # æ‚¨å¯èƒ½éœ€è¦å…ˆè¿è¡Œ â€˜conda initâ€™ å¹¶é‡æ–°æ‰“å¼€ shell
  ```

- ç¡®ä¿å®‰è£…äº† PyTorchã€packagingã€ninja
  
  ```
  pip install torch packaging ninja cpufeature numpy
  ```

<h3>å®‰è£…</h3>

1. ä½¿ç”¨ Docker é•œåƒï¼Œè¯¦è§ [Docker æ–‡æ¡£](./doc/en/Docker.md) 

2. æ‚¨å¯ä»¥ä½¿ç”¨ Pypi å®‰è£…ï¼ˆé€‚ç”¨äº Linuxï¼‰ï¼š
   
   ```
   pip install ktransformers --no-build-isolation
   ```
   
   å¯¹äº Windowsï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé¢„ç¼–è¯‘çš„ whl åŒ… [ktransformers-0.2.0+cu125torch24avx2-cp312-cp312-win_amd64.whl](https://github.com/kvcache-ai/ktransformers/releases/download/v0.2.0/ktransformers-0.2.0+cu125torch24avx2-cp312-cp312-win_amd64.whl)ï¼Œéœ€è¦ cuda-12.5ã€torch-2.4ã€python-3.11ï¼Œæ›´å¤šé¢„ç¼–è¯‘åŒ…æ­£åœ¨åˆ¶ä½œä¸­ã€‚

3. æˆ–è€…æ‚¨å¯ä»¥ä¸‹è½½æºä»£ç å¹¶ç¼–è¯‘ï¼š
   
   - init source code 
     
     ```sh
     git clone https://github.com/kvcache-ai/ktransformers.git
     cd ktransformers
     git submodule init
     git submodule update
     ```

   - [å¯é€‰] å¦‚æœæ‚¨æƒ³è¿è¡Œç½‘ç«™ï¼Œè¯·åœ¨æ‰§è¡Œ```bash install.sh```ä¹‹å‰, è¿›è¡Œ [compile the website](./doc/en/api/server/website.md)

   - ç¼–è¯‘å¹¶å®‰è£…ï¼ˆé€‚ç”¨äº Linuxï¼‰
     
     ```
     bash install.sh
     ```

   - ç¼–è¯‘å¹¶å®‰è£…ï¼ˆé€‚ç”¨äº Windowsï¼‰
     
     ```
     install.bat
     ```
4. å¦‚æœæ‚¨æ˜¯å¼€å‘è€…ï¼Œå¯ä»¥ä½¿ç”¨ makefile æ¥ç¼–è¯‘å’Œæ ¼å¼åŒ–ä»£ç ã€‚makefile çš„è¯¦ç»†ç”¨æ³•è¯·å‚è§ [è¿™é‡Œ](./doc/en/makefile_usage.md) 

<h3>æœ¬åœ°èŠå¤©</h3>
æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªç®€å•çš„å‘½ä»¤è¡Œæœ¬åœ°èŠå¤© Python è„šæœ¬ï¼Œæ‚¨å¯ä»¥è¿è¡Œå®ƒè¿›è¡Œæµ‹è¯•ã€‚

> è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„æµ‹è¯•å·¥å…·ï¼Œä»…æ”¯æŒä¸€è½®èŠå¤©ï¼Œä¸è®°å¿†ä¸Šä¸€æ¬¡è¾“å…¥ã€‚å¦‚æœæ‚¨æƒ³ä½“éªŒæ¨¡å‹çš„å…¨éƒ¨åŠŸèƒ½ï¼Œå¯ä»¥å‰å¾€ RESTful API å’Œ Web UIã€‚è¿™é‡Œä»¥ DeepSeek-V2-Lite-Chat-GGUF æ¨¡å‹ä¸ºä¾‹ï¼Œä½†æˆ‘ä»¬ä¹Ÿæ”¯æŒå…¶ä»–æ¨¡å‹ï¼Œæ‚¨å¯ä»¥æ›¿æ¢ä¸ºæ‚¨æƒ³è¦æµ‹è¯•çš„ä»»ä½•æ¨¡å‹ã€‚

<h4>è¿è¡Œç¤ºä¾‹</h4>

```shell
# ä»å…‹éš†çš„ä»“åº“æ ¹ç›®å½•å¼€å§‹ï¼
# ä»å…‹éš†çš„ä»“åº“æ ¹ç›®å½•å¼€å§‹ï¼ï¼
# ä»å…‹éš†çš„ä»“åº“æ ¹ç›®å½•å¼€å§‹!!!

# ä» Hugging Face ä¸‹è½½ mzwing/DeepSeek-V2-Lite-Chat-GGUF
mkdir DeepSeek-V2-Lite-Chat-GGUF
cd DeepSeek-V2-Lite-Chat-GGUF

wget https://huggingface.co/mzwing/DeepSeek-V2-Lite-Chat-GGUF/resolve/main/DeepSeek-V2-Lite-Chat.Q4_K_M.gguf -O DeepSeek-V2-Lite-Chat.Q4_K_M.gguf

cd .. # è¿”å›ä»“åº“æ ¹ç›®å½•

# å¯åŠ¨æœ¬åœ°èŠå¤©
python -m ktransformers.local_chat --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF

# å¦‚æœé‡åˆ°æŠ¥é”™ â€œOSError: We couldn't connect to 'https://huggingface.co' to load this fileâ€, è¯·å°è¯•ï¼š
# GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite
# python  ktransformers.local_chat --model_path ./DeepSeek-V2-Lite --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF
```

å®ƒå…·æœ‰ä»¥ä¸‹å‚æ•°:

- `--model_path` (required): æ¨¡å‹åç§° (ä¾‹å¦‚ "deepseek-ai/DeepSeek-V2-Lite-Chat" å°†è‡ªåŠ¨ä» [Hugging Face](https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite) ä¸‹è½½é…ç½®)ã€‚æˆ–è€…ï¼Œå¦‚æœæ‚¨å·²ç»æœ‰æœ¬åœ°æ–‡ä»¶ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨è¯¥è·¯å¾„æ¥åˆå§‹åŒ–æ¨¡å‹ã€‚
  
  > Note: <strong>.safetensors</strong> æ–‡ä»¶ä¸æ˜¯å¿…éœ€çš„ã€‚æˆ‘ä»¬åªéœ€è¦é…ç½®æ–‡ä»¶æ¥æ„å»ºæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚

- `--gguf_path` (required): åŒ…å« GGUF æ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼Œå¯ä»¥ä» [Hugging Face](https://huggingface.co/mzwing/DeepSeek-V2-Lite-Chat-GGUF/tree/main) ä¸‹è½½ã€‚è¯·æ³¨æ„ï¼Œè¯¥ç›®å½•åº”ä»…åŒ…å«å½“å‰æ¨¡å‹çš„ GGUFï¼Œè¿™æ„å‘³ç€æ‚¨éœ€è¦ä¸ºæ¯ä¸ªæ¨¡å‹ä½¿ç”¨ä¸€ä¸ªå•ç‹¬çš„ç›®å½•ã€‚

- `--optimize_rule_path` (å¿…éœ€ï¼ŒQwen2Moe å’Œ DeepSeek-V2 é™¤å¤–): åŒ…å«ä¼˜åŒ–è§„åˆ™çš„ YAML æ–‡ä»¶è·¯å¾„ã€‚åœ¨ [ktransformers/optimize/optimize_rules](ktransformers/optimize/optimize_rules) ç›®å½•ä¸­æœ‰ä¸¤ä¸ªé¢„å†™çš„è§„åˆ™æ–‡ä»¶ï¼Œç”¨äºä¼˜åŒ– DeepSeek-V2 å’Œ Qwen2-57B-A14ï¼Œè¿™ä¸¤ä¸ªæ˜¯ SOTA MoE æ¨¡å‹ã€‚ 

- `--max_new_tokens`: Int (default=1000). è¦ç”Ÿæˆçš„æœ€å¤§ new tokensã€‚

- `--cpu_infer`: Int (default=10). ç”¨äºæ¨ç†çš„ CPU æ•°é‡ã€‚ç†æƒ³æƒ…å†µä¸‹åº”è®¾ç½®ä¸ºï¼ˆæ€»æ ¸å¿ƒæ•° - 2ï¼‰ã€‚

<h3 id="suggested-model"> å»ºè®®æ¨¡å‹</h3>

| Model Name                     | Model Size | VRAM  | Minimum DRAM    | Recommended DRAM  |
| ------------------------------ | ---------- | ----- | --------------- | ----------------- |
| DeepSeek-R1-q4_k_m		 | 377G       | 14G   | 382G            | 512G		    |
| DeepSeek-V3-q4_k_m		 | 377G       | 14G   | 382G            | 512G		    |
| DeepSeek-V2-q4_k_m             | 133G       | 11G   | 136G            | 192G              |
| DeepSeek-V2.5-q4_k_m           | 133G       | 11G   | 136G            | 192G              |
| DeepSeek-V2.5-IQ4_XS           | 117G       | 10G   | 107G            | 128G              |
| Qwen2-57B-A14B-Instruct-q4_k_m | 33G        | 8G    | 34G             | 64G               |
| DeepSeek-V2-Lite-q4_k_m        | 9.7G       | 3G    | 13G             | 16G               |
| Mixtral-8x7B-q4_k_m            | 25G        | 1.6G  | 51G             | 64G               |
| Mixtral-8x22B-q4_k_m           | 80G        | 4G    | 86.1G           | 96G               |
| InternLM2.5-7B-Chat-1M         | 15.5G      | 15.5G | 8G(32K context) | 150G (1M context) |


æ›´å¤šå³å°†æ¨å‡ºã€‚è¯·å‘Šè¯‰æˆ‘ä»¬æ‚¨æœ€æ„Ÿå…´è¶£çš„æ¨¡å‹ã€‚

è¯·æ³¨æ„ï¼Œåœ¨ä½¿ç”¨ [DeepSeek](https://huggingface.co/deepseek-ai/DeepSeek-V2/blob/main/LICENSE) å’Œ [QWen](https://huggingface.co/Qwen/Qwen2-72B-Instruct/blob/main/LICENSE) æ—¶ï¼Œéœ€è¦éµå®ˆç›¸åº”çš„æ¨¡å‹è®¸å¯è¯ã€‚

<details>
  <summary>ç‚¹å‡»æ˜¾ç¤ºå¦‚ä½•è¿è¡Œå…¶ä»–ç¤ºä¾‹</summary>

* Qwen2-57B

  ```sh
  pip install flash_attn # For Qwen2

  mkdir Qwen2-57B-GGUF && cd Qwen2-57B-GGUF

  wget https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct-GGUF/resolve/main/qwen2-57b-a14b-instruct-q4_k_m.gguf?download=true -O qwen2-57b-a14b-instruct-q4_k_m.gguf

  cd ..

  python -m ktransformers.local_chat --model_name Qwen/Qwen2-57B-A14B-Instruct --gguf_path ./Qwen2-57B-GGUF

  # å¦‚æœé‡åˆ°æŠ¥é”™ â€œOSError: We couldn't connect to 'https://huggingface.co' to load this fileâ€, è¯·å°è¯•ï¼š
  # GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct
  # python  ktransformers/local_chat.py --model_path ./Qwen2-57B-A14B-Instruct --gguf_path ./DeepSeek-V2-Lite-Chat-GGUF
  ```

* DeepseekV2
  
  ```sh
  mkdir DeepSeek-V2-Chat-0628-GGUF && cd DeepSeek-V2-Chat-0628-GGUF
  # Download weights
  wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00001-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00001-of-00004.gguf
  wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00002-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00002-of-00004.gguf
  wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00003-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00003-of-00004.gguf
  wget https://huggingface.co/bartowski/DeepSeek-V2-Chat-0628-GGUF/resolve/main/DeepSeek-V2-Chat-0628-Q4_K_M/DeepSeek-V2-Chat-0628-Q4_K_M-00004-of-00004.gguf -o DeepSeek-V2-Chat-0628-Q4_K_M-00004-of-00004.gguf

  cd ..

  python -m ktransformers.local_chat --model_name deepseek-ai/DeepSeek-V2-Chat-0628 --gguf_path ./DeepSeek-V2-Chat-0628-GGUF

  # å¦‚æœé‡åˆ°æŠ¥é”™ â€œOSError: We couldn't connect to 'https://huggingface.co' to load this fileâ€, è¯·å°è¯•ï¼š

  # GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628

  # python -m ktransformers.local_chat --model_path ./DeepSeek-V2-Chat-0628 --gguf_path ./DeepSeek-V2-Chat-0628-GGUF
  ```

| model name | weights download link |
|----------|----------|
| Qwen2-57B | [Qwen2-57B-A14B-gguf-Q4K-M](https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct-GGUF/tree/main) |
| DeepseekV2-coder |[DeepSeek-Coder-V2-Instruct-gguf-Q4K-M](https://huggingface.co/LoneStriker/DeepSeek-Coder-V2-Instruct-GGUF/tree/main) |
| DeepseekV2-chat |[DeepSeek-V2-Chat-gguf-Q4K-M](https://huggingface.co/bullerwins/DeepSeek-V2-Chat-0628-GGUF/tree/main) |
| DeepseekV2-lite | [DeepSeek-V2-Lite-Chat-GGUF-Q4K-M](https://huggingface.co/mzwing/DeepSeek-V2-Lite-Chat-GGUF/tree/main) |

</details>

<!-- pin block for jump -->
<span id='id_666'> 

<h3>RESTful API and Web UI</h3>


Start without website:

```sh
ktransformers --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path /path/to/DeepSeek-V2-Lite-Chat-GGUF --port 10002
```

Start with website:

```sh
ktransformers --model_path deepseek-ai/DeepSeek-V2-Lite-Chat --gguf_path /path/to/DeepSeek-V2-Lite-Chat-GGUF  --port 10002 --web True
```

æˆ–è€…ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨ transformers å¯åŠ¨æœåŠ¡ï¼Œmodel_path åº”è¯¥åŒ…å« safetensors æ–‡ä»¶ï¼š

```bash
ktransformers --type transformers --model_path /mnt/data/model/Qwen2-0.5B-Instruct --port 10002 --web True
```

é€šè¿‡ [http://localhost:10002/web/index.html#/chat](http://localhost:10002/web/index.html#/chat)  è®¿é—®ï¼š

<p align="center">
  <picture>
    <img alt="Web UI" src="https://github.com/user-attachments/assets/615dca9b-a08c-4183-bbd3-ad1362680faf" width=90%>
  </picture>
</p>

å…³äº RESTful API æœåŠ¡å™¨çš„æ›´å¤šä¿¡æ¯å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ° [è¿™é‡Œ](doc/en/api/server/server.md)ã€‚æ‚¨è¿˜å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ä¸ Tabby é›†æˆçš„ç¤ºä¾‹ [è¿™é‡Œ](doc/en/api/server/tabby.md)ã€‚

<h2 id="tutorial">ğŸ“ƒ ç®€è¦æ³¨å…¥æ•™ç¨‹</h2>
KTransformers çš„æ ¸å¿ƒæ˜¯ä¸€ä¸ªç”¨æˆ·å‹å¥½çš„ã€åŸºäºæ¨¡æ¿çš„æ³¨å…¥æ¡†æ¶ã€‚è¿™ä½¿å¾—ç ”ç©¶äººå‘˜å¯ä»¥è½»æ¾åœ°å°†åŸå§‹ torch æ¨¡å—æ›¿æ¢ä¸ºä¼˜åŒ–çš„å˜ä½“ã€‚å®ƒè¿˜ç®€åŒ–äº†å¤šç§ä¼˜åŒ–çš„ç»„åˆè¿‡ç¨‹ï¼Œå…è®¸æ¢ç´¢å®ƒä»¬çš„ååŒæ•ˆåº”ã€‚
</br>
<p align="center">
  <picture>
    <img alt="Inject-Struction" src="https://github.com/user-attachments/assets/6b4c1e54-9f6d-45c5-a3fc-8fa45e7d257e" width=65%>
  </picture>
</p>

é‰´äº vLLM å·²ç»æ˜¯ä¸€ä¸ªç”¨äºå¤§è§„æ¨¡éƒ¨ç½²ä¼˜åŒ–çš„ä¼˜ç§€æ¡†æ¶ï¼ŒKTransformers ç‰¹åˆ«å…³æ³¨å—èµ„æºé™åˆ¶çš„æœ¬åœ°éƒ¨ç½²ã€‚æˆ‘ä»¬ç‰¹åˆ«å…³æ³¨å¼‚æ„è®¡ç®—æ—¶æœºï¼Œä¾‹å¦‚é‡åŒ–æ¨¡å‹çš„ GPU/CPU å¸è½½ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ”¯æŒé«˜æ•ˆçš„ <a herf="https://github.com/Mozilla-Ocho/llamafile/tree/main">Llamafile</a> å’Œ<a herf="https://github.com/IST-DASLab/marlin">Marlin</a> å†…æ ¸ï¼Œåˆ†åˆ«ç”¨äº CPU å’Œ GPUã€‚ æ›´å¤šè¯¦ç»†ä¿¡æ¯å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ° <a herf="doc/en/operators/llamafile.md">è¿™é‡Œ</a>ã€‚


<h3>ç¤ºä¾‹ç”¨æ³•</h3>
è¦ä½¿ç”¨æä¾›çš„å†…æ ¸ï¼Œç”¨æˆ·åªéœ€åˆ›å»ºä¸€ä¸ªåŸºäº YAML çš„æ³¨å…¥æ¨¡æ¿ï¼Œå¹¶åœ¨ä½¿ç”¨ Transformers æ¨¡å‹ä¹‹å‰æ·»åŠ å¯¹ `optimize_and_load_gguf` çš„è°ƒç”¨ã€‚

```python
with torch.device("meta"):
    model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)
optimize_and_load_gguf(model, optimize_rule_path, gguf_path, config)
...
generated = prefill_and_generate(model, tokenizer, input_tensor.cuda(), max_new_tokens=1000)
```

åœ¨è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œé¦–å…ˆåœ¨ meta è®¾å¤‡ä¸Šåˆå§‹åŒ– AutoModelï¼Œä»¥é¿å…å ç”¨ä»»ä½•å†…å­˜èµ„æºã€‚ç„¶åï¼Œ`optimize_and_load_gguf` éå†æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—ï¼ŒåŒ¹é…æ‚¨çš„ YAML è§„åˆ™æ–‡ä»¶ä¸­æŒ‡å®šçš„è§„åˆ™ï¼Œå¹¶å°†å®ƒä»¬æ›¿æ¢ä¸ºæŒ‡å®šçš„é«˜çº§æ¨¡å—ã€‚

æ³¨å…¥åï¼ŒåŸå§‹çš„ `generate` æ¥å£ä»ç„¶å¯ç”¨ï¼Œä½†æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªå…¼å®¹çš„ `prefill_and_generate` æ–¹æ³•ï¼Œè¿™ä½¿å¾—å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä¾‹å¦‚ä½¿ç”¨ CUDAGraph æé«˜ç”Ÿæˆé€Ÿåº¦ã€‚

<h3>å¦‚ä½•è‡ªå®šä¹‰æ‚¨çš„æ¨¡å‹</h3>

ä¸€ä¸ªè¯¦ç»†çš„ä½¿ç”¨ DeepSeek-V2 ä½œä¸ºç¤ºä¾‹çš„æ³¨å…¥å’Œ multi-GPU æ•™ç¨‹åœ¨è¿™é‡Œç»™å‡º [è¿™é‡Œ](doc/en/injection_tutorial.md)ã€‚

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå°†æ‰€æœ‰åŸå§‹ Linear æ¨¡å—æ›¿æ¢ä¸º Marlin çš„ YAML æ¨¡æ¿ç¤ºä¾‹ï¼ŒMarlin æ˜¯ä¸€ä¸ªé«˜çº§çš„ 4 ä½é‡åŒ–å†…æ ¸ã€‚

```yaml
- match:
    name: "^model\\.layers\\..*$"  # æ­£åˆ™è¡¨è¾¾å¼ 
    class: torch.nn.Linear  # ä»…åŒ¹é…åŒæ—¶ç¬¦åˆåç§°å’Œç±»çš„æ¨¡å—
  replace:
    class: ktransformers.operators.linear.KTransformerLinear  # é‡åŒ–æ•°æ®ç±»å‹çš„ä¼˜åŒ–å†…æ ¸
    device: "cpu"   # åˆå§‹åŒ–æ—¶åŠ è½½è¯¥æ¨¡å—çš„ device
    kwargs:
      generate_device: "cuda"
      generate_linear_type: "QuantizedLinearMarlin"
```

YAML æ–‡ä»¶ä¸­çš„æ¯ä¸ªè§„åˆ™éƒ½æœ‰ä¸¤éƒ¨åˆ†ï¼š`match` å’Œ `replace`ã€‚`match` éƒ¨åˆ†æŒ‡å®šåº”æ›¿æ¢çš„æ¨¡å—ï¼Œ`replace` éƒ¨åˆ†æŒ‡å®šè¦æ³¨å…¥åˆ°æ¨¡å‹ä¸­çš„æ¨¡å—ä»¥åŠåˆå§‹åŒ–å…³é”®å­—ã€‚

æ‚¨å¯ä»¥åœ¨ [ktransformers/optimize/optimize_rules](ktransformers/optimize/optimize_rules) ç›®å½•ä¸­æ‰¾åˆ°ç”¨äºä¼˜åŒ– DeepSeek-V2 å’Œ Qwen2-57B-A14 çš„ç¤ºä¾‹è§„åˆ™æ¨¡æ¿ã€‚è¿™äº›æ¨¡æ¿ç”¨äºä¸º `local_chat.py` ç¤ºä¾‹æä¾›æ”¯æŒã€‚

If you are interested in our design principles and the implementation of the injection framework, please refer to the [design document](doc/en/deepseek-v2-injection.md).
å¦‚æœæ‚¨å¯¹æˆ‘ä»¬çš„è®¾è®¡åŸåˆ™å’Œæ³¨å…¥æ¡†æ¶çš„å®ç°æ„Ÿå…´è¶£ï¼Œè¯·å‚è€ƒ [è®¾è®¡æ–‡æ¡£](doc/en/deepseek-v2-injection.md)ã€‚

<h2 id="ack">è‡´è°¢å’Œè´¡çŒ®è€…</h2>

KTransformer çš„å¼€å‘åŸºäº Transformers æä¾›çš„çµæ´»å’Œå¤šåŠŸèƒ½æ¡†æ¶ã€‚æˆ‘ä»¬è¿˜å—ç›Šäº GGUF/GGMLã€Llamafile å’Œ Marlin ç­‰é«˜çº§å†…æ ¸ã€‚æˆ‘ä»¬è®¡åˆ’é€šè¿‡å‘ä¸Šæ¸¸è´¡çŒ®æˆ‘ä»¬çš„ä¿®æ”¹æ¥å›é¦ˆç¤¾åŒºã€‚

KTransformer ç”±æ¸…åå¤§å­¦ <a href="https://madsys.cs.tsinghua.edu.cn/">MADSys group</a> å°ç»„çš„æˆå‘˜ä»¥åŠ <a href="http://approaching.ai/">Approaching.AI</a> çš„æˆå‘˜ç§¯æç»´æŠ¤å’Œå¼€å‘ã€‚æˆ‘ä»¬æ¬¢è¿æ–°çš„è´¡çŒ®è€…åŠ å…¥æˆ‘ä»¬ï¼Œä½¿ KTransformer æ›´å¿«ã€æ›´æ˜“äºä½¿ç”¨ã€‚


<h2 id="ack">è®¨è®º</h2>

å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿éšæ—¶æå‡º issueã€‚æˆ–è€…ï¼Œæ‚¨å¯ä»¥åŠ å…¥æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤è¿›è¡Œè¿›ä¸€æ­¥è®¨è®ºã€‚äºŒç»´ç ï¼š [å¾®ä¿¡ç¾¤](WeChatGroup.png)

<h2 id="FAQ">ğŸ™‹ å¸¸è§é—®é¢˜</h2>

ä¸€äº›å¸¸è§é—®é¢˜çš„ç­”æ¡ˆå¯ä»¥åœ¨ [FAQ](doc/en/FAQ.md) ä¸­æ‰¾åˆ°ã€‚ 
